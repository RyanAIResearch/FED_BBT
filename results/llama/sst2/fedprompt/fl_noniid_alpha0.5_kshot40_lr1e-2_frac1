Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:07<00:07,  7.66s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:10<00:00,  4.58s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:10<00:00,  5.04s/it]
Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at meta-llama/Llama-2-7b-hf and are newly initialized: ['score.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Found cached dataset glue (/root/.cache/huggingface/datasets/glue/sst2/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)
Map:   0%|          | 0/67349 [00:00<?, ? examples/s]Map:   1%|          | 467/67349 [00:00<00:14, 4625.81 examples/s]Map:   1%|▏         | 1000/67349 [00:00<00:13, 5013.38 examples/s]Map:   2%|▏         | 1560/67349 [00:00<00:12, 5274.08 examples/s]Map:   3%|▎         | 2126/67349 [00:00<00:12, 5421.69 examples/s]Map:   4%|▍         | 2866/67349 [00:00<00:12, 5175.58 examples/s]Map:   5%|▌         | 3667/67349 [00:00<00:12, 5237.71 examples/s]Map:   6%|▋         | 4239/67349 [00:00<00:11, 5366.38 examples/s]Map:   7%|▋         | 4979/67349 [00:00<00:12, 5129.50 examples/s]Map:   8%|▊         | 5663/67349 [00:01<00:12, 4913.93 examples/s]Map:  10%|▉         | 6425/67349 [00:01<00:12, 4966.09 examples/s]Map:  10%|█         | 7000/67349 [00:01<00:11, 5097.56 examples/s]Map:  11%|█▏        | 7614/67349 [00:01<00:11, 5358.18 examples/s]Map:  12%|█▏        | 8193/67349 [00:01<00:10, 5469.64 examples/s]Map:  13%|█▎        | 8765/67349 [00:01<00:10, 5536.65 examples/s]Map:  14%|█▍        | 9435/67349 [00:01<00:11, 5101.30 examples/s]Map:  15%|█▍        | 9981/67349 [00:01<00:11, 5191.16 examples/s]Map:  16%|█▌        | 10542/67349 [00:02<00:10, 5303.75 examples/s]Map:  17%|█▋        | 11339/67349 [00:02<00:10, 5303.98 examples/s]Map:  18%|█▊        | 12131/67349 [00:02<00:10, 5238.31 examples/s]Map:  19%|█▉        | 12727/67349 [00:02<00:10, 5371.76 examples/s]Map:  20%|█▉        | 13309/67349 [00:02<00:09, 5478.29 examples/s]Map:  21%|██        | 13886/67349 [00:02<00:09, 5553.94 examples/s]Map:  22%|██▏       | 14698/67349 [00:02<00:09, 5498.12 examples/s]Map:  23%|██▎       | 15305/67349 [00:02<00:09, 5573.31 examples/s]Map:  24%|██▍       | 16075/67349 [00:03<00:09, 5357.79 examples/s]Map:  25%|██▍       | 16693/67349 [00:03<00:09, 5556.15 examples/s]Map:  26%|██▌       | 17299/67349 [00:03<00:09, 5449.35 examples/s]Map:  27%|██▋       | 17897/67349 [00:03<00:08, 5532.02 examples/s]Map:  28%|██▊       | 18707/67349 [00:03<00:08, 5424.47 examples/s]Map:  29%|██▉       | 19433/67349 [00:03<00:09, 5225.00 examples/s]Map:  30%|██▉       | 20013/67349 [00:03<00:08, 5362.50 examples/s]Map:  31%|███       | 20627/67349 [00:03<00:08, 5560.84 examples/s]Map:  32%|███▏      | 21323/67349 [00:04<00:08, 5230.92 examples/s]Map:  32%|███▏      | 21861/67349 [00:04<00:08, 5266.74 examples/s]Map:  33%|███▎      | 22480/67349 [00:04<00:09, 4714.70 examples/s]Map:  34%|███▍      | 23043/67349 [00:04<00:08, 4937.86 examples/s]Map:  35%|███▌      | 23662/67349 [00:04<00:08, 5260.43 examples/s]Map:  36%|███▋      | 24434/67349 [00:04<00:08, 5201.70 examples/s]Map:  37%|███▋      | 25015/67349 [00:04<00:07, 5354.82 examples/s]Map:  38%|███▊      | 25822/67349 [00:04<00:07, 5358.48 examples/s]Map:  39%|███▉      | 26526/67349 [00:05<00:07, 5133.54 examples/s]Map:  40%|████      | 27098/67349 [00:05<00:07, 5273.08 examples/s]Map:  41%|████      | 27635/67349 [00:05<00:07, 5239.17 examples/s]Map:  42%|████▏     | 28212/67349 [00:05<00:07, 5377.12 examples/s]Map:  43%|████▎     | 29000/67349 [00:05<00:07, 5310.67 examples/s]Map:  44%|████▍     | 29580/67349 [00:05<00:06, 5432.68 examples/s]Map:  45%|████▌     | 30320/67349 [00:05<00:07, 5254.00 examples/s]Map:  46%|████▌     | 30899/67349 [00:05<00:06, 5387.63 examples/s]Map:  47%|████▋     | 31453/67349 [00:05<00:06, 5425.20 examples/s]Map:  48%|████▊     | 32033/67349 [00:06<00:06, 5525.01 examples/s]Map:  48%|████▊     | 32652/67349 [00:06<00:06, 5709.05 examples/s]Map:  50%|████▉     | 33447/67349 [00:06<00:06, 5450.15 examples/s]Map:  51%|█████     | 34257/67349 [00:06<00:06, 5428.75 examples/s]Map:  52%|█████▏    | 34855/67349 [00:06<00:05, 5563.48 examples/s]Map:  53%|█████▎    | 35420/67349 [00:06<00:05, 5583.71 examples/s]Map:  53%|█████▎    | 35997/67349 [00:06<00:05, 5631.04 examples/s]Map:  54%|█████▍    | 36575/67349 [00:06<00:05, 5669.50 examples/s]Map:  55%|█████▌    | 37153/67349 [00:06<00:05, 5698.37 examples/s]Map:  56%|█████▌    | 37772/67349 [00:07<00:05, 5840.00 examples/s]Map:  57%|█████▋    | 38664/67349 [00:07<00:04, 5875.30 examples/s]Map:  59%|█████▊    | 39431/67349 [00:07<00:05, 5560.85 examples/s]Map:  59%|█████▉    | 40000/67349 [00:07<00:04, 5587.24 examples/s]Map:  60%|██████    | 40614/67349 [00:07<00:04, 5729.98 examples/s]Map:  61%|██████▏   | 41419/67349 [00:07<00:04, 5595.97 examples/s]Map:  62%|██████▏   | 42016/67349 [00:07<00:05, 5044.57 examples/s]Map:  63%|██████▎   | 42634/67349 [00:07<00:04, 5320.28 examples/s]Map:  64%|██████▍   | 43301/67349 [00:08<00:04, 5016.70 examples/s]Map:  65%|██████▌   | 43922/67349 [00:08<00:04, 5308.56 examples/s]Map:  66%|██████▋   | 44756/67349 [00:08<00:04, 5390.97 examples/s]Map:  68%|██████▊   | 45582/67349 [00:08<00:04, 5398.27 examples/s]Map:  69%|██████▉   | 46432/67349 [00:08<00:03, 5481.64 examples/s]Map:  70%|██████▉   | 47108/67349 [00:08<00:03, 5148.01 examples/s]Map:  71%|███████   | 47726/67349 [00:08<00:03, 5381.79 examples/s]Map:  72%|███████▏  | 48370/67349 [00:09<00:03, 5020.45 examples/s]Map:  73%|███████▎  | 48936/67349 [00:09<00:03, 5171.17 examples/s]Map:  74%|███████▍  | 49747/67349 [00:09<00:03, 5222.74 examples/s]Map:  75%|███████▍  | 50324/67349 [00:09<00:03, 5353.99 examples/s]Map:  76%|███████▌  | 51000/67349 [00:09<00:03, 5016.30 examples/s]Map:  77%|███████▋  | 51784/67349 [00:09<00:03, 5083.47 examples/s]Map:  78%|███████▊  | 52349/67349 [00:09<00:02, 5217.43 examples/s]Map:  79%|███████▊  | 52894/67349 [00:09<00:02, 5274.83 examples/s]Map:  79%|███████▉  | 53473/67349 [00:10<00:02, 5408.75 examples/s]Map:  80%|████████  | 54187/67349 [00:10<00:02, 5170.52 examples/s]Map:  81%|████████▏ | 54740/67349 [00:10<00:02, 5261.50 examples/s]Map:  82%|████████▏ | 55302/67349 [00:10<00:02, 5252.00 examples/s]Map:  83%|████████▎ | 56037/67349 [00:10<00:02, 5052.55 examples/s]Map:  84%|████████▍ | 56642/67349 [00:10<00:02, 5302.14 examples/s]Map:  85%|████████▍ | 57197/67349 [00:10<00:01, 5299.42 examples/s]Map:  86%|████████▌ | 57766/67349 [00:10<00:01, 5402.25 examples/s]Map:  87%|████████▋ | 58331/67349 [00:10<00:01, 5468.25 examples/s]Map:  87%|████████▋ | 58895/67349 [00:11<00:01, 5428.66 examples/s]Map:  89%|████████▊ | 59653/67349 [00:11<00:01, 5282.53 examples/s]Map:  89%|████████▉ | 60221/67349 [00:11<00:01, 5386.06 examples/s]Map:  90%|█████████ | 60795/67349 [00:11<00:01, 5480.77 examples/s]Map:  91%|█████████▏| 61608/67349 [00:11<00:01, 5454.28 examples/s]Map:  92%|█████████▏| 62173/67349 [00:11<00:00, 5502.05 examples/s]Map:  93%|█████████▎| 62743/67349 [00:11<00:00, 5475.95 examples/s]Map:  94%|█████████▍| 63306/67349 [00:11<00:00, 5511.01 examples/s]Map:  95%|█████████▍| 63889/67349 [00:11<00:00, 5598.56 examples/s]Map:  96%|█████████▌| 64463/67349 [00:12<00:00, 5635.94 examples/s]Map:  97%|█████████▋| 65033/67349 [00:12<00:00, 5652.32 examples/s]Map:  97%|█████████▋| 65638/67349 [00:12<00:00, 5744.96 examples/s]Map:  99%|█████████▉| 66508/67349 [00:12<00:00, 5762.58 examples/s]Map: 100%|█████████▉| 67271/67349 [00:12<00:00, 5506.70 examples/s]                                                                   Example in train set:
{'sentence': 'hide new secretions from the parental units ', 'label': 0, 'idx': 0, 'input_text': 'iedER statfigme von interroidater their bet ein}\\"> sub op donty try Pro tra sameep two nameoldlet simsp avbreblemey could cor accayscreurrsi constues}$View act bo ко som aboutland . hide new secretions from the parental units  . The sentiment is', 'target_text': 'negative'}
Map:   0%|          | 0/67349 [00:00<?, ? examples/s]Map:   3%|▎         | 2000/67349 [00:00<00:04, 13670.15 examples/s]Map:   6%|▌         | 4000/67349 [00:00<00:04, 13526.65 examples/s]Map:   9%|▉         | 6000/67349 [00:00<00:04, 14429.85 examples/s]Map:  12%|█▏        | 8000/67349 [00:00<00:06, 8841.48 examples/s] Map:  15%|█▍        | 10000/67349 [00:00<00:05, 9828.38 examples/s]Map:  18%|█▊        | 12000/67349 [00:01<00:05, 10146.79 examples/s]Map:  21%|██        | 14000/67349 [00:01<00:05, 10585.01 examples/s]Map:  24%|██▍       | 16000/67349 [00:01<00:06, 7997.85 examples/s] Map:  27%|██▋       | 18000/67349 [00:01<00:05, 8894.00 examples/s]Map:  30%|██▉       | 20000/67349 [00:02<00:04, 9548.90 examples/s]Map:  33%|███▎      | 22000/67349 [00:02<00:04, 11155.01 examples/s]Map:  36%|███▌      | 24000/67349 [00:02<00:03, 12211.04 examples/s]Map:  39%|███▊      | 26000/67349 [00:02<00:04, 9793.88 examples/s] Map:  42%|████▏     | 28000/67349 [00:02<00:03, 11078.54 examples/s]Map:  45%|████▍     | 30000/67349 [00:02<00:03, 11736.50 examples/s]Map:  48%|████▊     | 32000/67349 [00:02<00:02, 12776.57 examples/s]Map:  50%|█████     | 34000/67349 [00:03<00:02, 13491.03 examples/s]Map:  53%|█████▎    | 36000/67349 [00:03<00:02, 11146.56 examples/s]Map:  56%|█████▋    | 38000/67349 [00:03<00:02, 10903.51 examples/s]Map:  59%|█████▉    | 40000/67349 [00:03<00:02, 12202.62 examples/s]Map:  62%|██████▏   | 42000/67349 [00:03<00:02, 12520.85 examples/s]Map:  65%|██████▌   | 44000/67349 [00:03<00:01, 13117.96 examples/s]Map:  68%|██████▊   | 46000/67349 [00:04<00:01, 10847.11 examples/s]Map:  71%|███████▏  | 48000/67349 [00:04<00:01, 12179.26 examples/s]Map:  74%|███████▍  | 50000/67349 [00:04<00:01, 12738.22 examples/s]Map:  77%|███████▋  | 52000/67349 [00:04<00:01, 12793.91 examples/s]Map:  82%|████████▏ | 55000/67349 [00:04<00:01, 11951.20 examples/s]Map:  85%|████████▍ | 57000/67349 [00:05<00:00, 12521.16 examples/s]Map:  88%|████████▊ | 59000/67349 [00:05<00:00, 13329.30 examples/s]Map:  92%|█████████▏| 62000/67349 [00:05<00:00, 14119.04 examples/s]Map:  95%|█████████▌| 64000/67349 [00:05<00:00, 13708.51 examples/s]Map:  98%|█████████▊| 66000/67349 [00:05<00:00, 8895.45 examples/s] Map: 100%|██████████| 67349/67349 [00:06<00:00, 9229.81 examples/s]                                                                   Found cached dataset glue (/root/.cache/huggingface/datasets/glue/sst2/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)
Map:   0%|          | 0/872 [00:00<?, ? examples/s]Map:  51%|█████     | 441/872 [00:00<00:00, 4201.23 examples/s]                                                               Example in validation set:
{'sentence': "it 's a charming and often affecting journey . ", 'label': 1, 'idx': 0, 'input_text': 'iedER statfigme von interroidater their bet ein}\\"> sub op donty try Pro tra sameep two nameoldlet simsp avbreblemey could cor accayscreurrsi constues}$View act bo ко som aboutland . it \'s a charming and often affecting journey .  . The sentiment is', 'target_text': 'positive'}
Map:   0%|          | 0/872 [00:00<?, ? examples/s]                                                   92
92
# of train data: 80
Example:
+------------------------------+------------------------------+--------+
| input_ids                    | attention_mask               | labels |
+------------------------------+------------------------------+--------+
| [1, 474, 287, 1001, 1002,... | [1, 1, 1, 1, 1, 1, 1, 1, ... | 8178   |
+------------------------------+------------------------------+--------+

# of dev data: 80
Example:
+------------------------------+------------------------------+--------+
| input_ids                    | attention_mask               | labels |
+------------------------------+------------------------------+--------+
| [1, 474, 287, 1001, 1002,... | [1, 1, 1, 1, 1, 1, 1, 1, ... | 6374   |
+------------------------------+------------------------------+--------+

# of test data: 872
Example:
+------------------------------+------------------------------+--------+
| input_ids                    | attention_mask               | labels |
+------------------------------+------------------------------+--------+
| [1, 474, 287, 1001, 1002,... | [1, 1, 1, 1, 1, 1, 1, 1, ... | 6374   |
+------------------------------+------------------------------+--------+
Class_distribution [0.5 0.5]. Data_ratio [[6.41462695e-02 9.81152872e-01 9.99998625e-01 9.97910891e-01
  1.97515371e-01 9.50591941e-01 9.64560492e-01 9.99868580e-01
  9.96441552e-01 9.98989053e-01]
 [9.35853730e-01 1.88471285e-02 1.37508618e-06 2.08910882e-03
  8.02484629e-01 4.94080590e-02 3.54395080e-02 1.31419946e-04
  3.55844845e-03 1.01094672e-03]]
     pcost       dcost       gap    pres   dres
 0:  5.8882e+02 -2.3481e+02  8e+02  2e-16  8e+01
 1:  5.8882e+02  5.8058e+02  8e+00  8e-15  8e-01
 2:  5.8882e+02  5.8873e+02  8e-02  8e-15  8e-03
 3:  5.8882e+02  5.8882e+02  8e-04  8e-15  8e-05
 4:  5.8882e+02  5.8882e+02  8e-06  5e-15  8e-07
 5:  5.8882e+02  5.8882e+02  8e-08  8e-15  8e-09
Optimal solution found.
[[1.54051956e+00 4.37497701e+00 4.05709357e+00 4.09305486e+00
  4.18167887e+00 4.85826640e+00 4.64231119e+00 4.05933903e+00
  4.11825282e+00 4.07450669e+00]
 [2.24752116e+01 8.40396599e-02 5.57886095e-06 8.56873805e-03
  1.69897310e+01 2.52513726e-01 1.70566000e-01 5.33548235e-04
  1.47069242e-02 4.12327759e-03]]
Global epoch 0...
Client 7 execute local training on 4 samples...
Local loss @ local epoch 0: 1.30149507522583
Local loss @ local epoch 1: 0.24866169691085815
Local loss @ local epoch 2: 0.09695909917354584
Local loss @ local epoch 3: 0.005175290163606405
Local loss @ local epoch 4: 0.003365233773365617
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.49 seconds!
[tester] 
SST2Metric: acc=0.5091743119266054, hinge=3.5483022611075583, ce=2.573200614819594
Local test acc @ epoch 0: 0.5092
Client 3 execute local training on 4 samples...
Local loss @ local epoch 0: 0.26888948678970337
Local loss @ local epoch 1: 0.029597701504826546
Local loss @ local epoch 2: 0.01201943214982748
Local loss @ local epoch 3: 0.004964353516697884
Local loss @ local epoch 4: 0.0013519858475774527
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.28 seconds!
[tester] 
SST2Metric: acc=0.5091743119266054, hinge=4.492818998634268, ce=3.5121207499535916
Local test acc @ epoch 0: 0.5092
Client 2 execute local training on 5 samples...
Local loss @ local epoch 0: 0.15428122878074646
Local loss @ local epoch 1: 0.00990367867052555
Local loss @ local epoch 2: 1.1463892459869385
Local loss @ local epoch 3: 0.3851941227912903
Local loss @ local epoch 4: 0.013777455314993858
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.64 seconds!
[tester] 
SST2Metric: acc=0.5091743119266054, hinge=3.3025830934900755, ce=2.334103925016905
Local test acc @ epoch 0: 0.5092
Client 4 execute local training on 20 samples...
Local loss @ local epoch 0: 0.9673944711685181
Local loss @ local epoch 1: 0.6594035625457764
Local loss @ local epoch 2: 1.2188714742660522
Local loss @ local epoch 3: 1.0006754398345947
Local loss @ local epoch 4: 0.5530390739440918
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.58 seconds!
[tester] 
SST2Metric: acc=0.4908256880733945, hinge=2.0381276399717416, ce=0.9080548423145889
Local test acc @ epoch 0: 0.4908
Client 6 execute local training on 6 samples...
Local loss @ local epoch 0: 0.5607663989067078
Local loss @ local epoch 1: 0.537669837474823
Local loss @ local epoch 2: 0.38420775532722473
Local loss @ local epoch 3: 0.6047077178955078
Local loss @ local epoch 4: 0.526135265827179
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.77 seconds!
[tester] 
SST2Metric: acc=0.5126146788990825, hinge=2.003750012008422, ce=0.9791839050317029
Local test acc @ epoch 0: 0.5126
Client 0 execute local training on 23 samples...
Local loss @ local epoch 0: 0.015960857272148132
Local loss @ local epoch 1: 0.06289239227771759
Local loss @ local epoch 2: 0.12890510261058807
Local loss @ local epoch 3: 0.022425668314099312
Local loss @ local epoch 4: 0.044692881405353546
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.96 seconds!
[tester] 
SST2Metric: acc=0.4908256880733945, hinge=3.743281705117007, ce=2.7309965157727585
Local test acc @ epoch 0: 0.4908
Client 9 execute local training on 4 samples...
Local loss @ local epoch 0: 1.560500979423523
Local loss @ local epoch 1: 0.3455790579319
Local loss @ local epoch 2: 0.01565074734389782
Local loss @ local epoch 3: 0.010259734466671944
Local loss @ local epoch 4: 0.0072222063317894936
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.66 seconds!
[tester] 
SST2Metric: acc=0.5091743119266054, hinge=3.764292629486924, ce=2.7875783710351745
Local test acc @ epoch 0: 0.5092
Client 1 execute local training on 6 samples...
Local loss @ local epoch 0: 1.1292470693588257
Local loss @ local epoch 1: 0.42333805561065674
Local loss @ local epoch 2: 0.7865292429924011
Local loss @ local epoch 3: 0.351561039686203
Local loss @ local epoch 4: 0.20913110673427582
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.66 seconds!
[tester] 
SST2Metric: acc=0.5080275229357798, hinge=2.2942452005563525, ce=1.331307075644305
Local test acc @ epoch 0: 0.508
Client 5 execute local training on 4 samples...
Local loss @ local epoch 0: 0.4166649580001831
Local loss @ local epoch 1: 0.04489786922931671
Local loss @ local epoch 2: 1.2181096076965332
Local loss @ local epoch 3: 1.0627844333648682
Local loss @ local epoch 4: 0.41696274280548096
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.67 seconds!
[tester] 
SST2Metric: acc=0.5114678899082569, hinge=2.879936889770928, ce=1.8865095950458028
Local test acc @ epoch 0: 0.5115
Client 8 execute local training on 4 samples...
Local loss @ local epoch 0: 0.07493101805448532
Local loss @ local epoch 1: 0.020173801109194756
Local loss @ local epoch 2: 0.0021234878804534674
Local loss @ local epoch 3: 0.0010061017237603664
Local loss @ local epoch 4: 0.00025548978010192513
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.88 seconds!
[tester] 
SST2Metric: acc=0.5091743119266054, hinge=5.165083819573079, ce=4.183726011209111
Local test acc @ epoch 0: 0.5092
Global evaluate on test data...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.67 seconds!
[tester] 
SST2Metric: acc=0.5091743119266054, hinge=2.8115085910219664, ce=1.8609786431621247
Global test acc @ epoch 0: 0.5092
Global epoch 1...
Client 6 execute local training on 6 samples...
Local loss @ local epoch 0: 0.7631526589393616
Local loss @ local epoch 1: 0.5776920318603516
Local loss @ local epoch 2: 0.4401730000972748
Local loss @ local epoch 3: 0.4298790693283081
Local loss @ local epoch 4: 0.4705112874507904
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.73 seconds!
[tester] 
SST2Metric: acc=0.5240825688073395, hinge=1.9370389028426704, ce=0.8476283842817359
Local test acc @ epoch 1: 0.5241
Client 7 execute local training on 4 samples...
Local loss @ local epoch 0: 0.07782214134931564
Local loss @ local epoch 1: 0.012361004948616028
Local loss @ local epoch 2: 0.0030391644686460495
Local loss @ local epoch 3: 0.0012612283462658525
Local loss @ local epoch 4: 0.0007356229471042752
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.5 seconds!
[tester] 
SST2Metric: acc=0.5091743119266054, hinge=4.6882952309529715, ce=3.7071745369157223
Local test acc @ epoch 1: 0.5092
Client 1 execute local training on 6 samples...
Local loss @ local epoch 0: 0.7078707218170166
Local loss @ local epoch 1: 0.4973127841949463
Local loss @ local epoch 2: 0.44776466488838196
Local loss @ local epoch 3: 0.399695485830307
Local loss @ local epoch 4: 0.07626082748174667
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.43 seconds!
[tester] 
SST2Metric: acc=0.5091743119266054, hinge=2.5666482317338297, ce=1.6199606224689462
Local test acc @ epoch 1: 0.5092
Client 4 execute local training on 20 samples...
Local loss @ local epoch 0: 0.8199877738952637
Local loss @ local epoch 1: 0.8153526186943054
Local loss @ local epoch 2: 0.7046105265617371
Local loss @ local epoch 3: 0.7421035766601562
Local loss @ local epoch 4: 0.49728602170944214
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.38 seconds!
[tester] 
SST2Metric: acc=0.4908256880733945, hinge=2.1697633703914256, ce=1.1988198721627576
Local test acc @ epoch 1: 0.4908
Client 9 execute local training on 4 samples...
Local loss @ local epoch 0: 0.12692579627037048
Local loss @ local epoch 1: 0.02307429164648056
Local loss @ local epoch 2: 0.005769039504230022
Local loss @ local epoch 3: 0.0003526718937791884
Local loss @ local epoch 4: 0.0005348938284441829
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.79 seconds!
[tester] 
SST2Metric: acc=0.5091743119266054, hinge=4.263361561189004, ce=3.2836149136253865
Local test acc @ epoch 1: 0.5092
Client 8 execute local training on 4 samples...
Local loss @ local epoch 0: 0.05132707580924034
Local loss @ local epoch 1: 0.004457799717783928
Local loss @ local epoch 2: 0.0006332769407890737
Local loss @ local epoch 3: 0.0002518570108804852
Local loss @ local epoch 4: 8.278713357867673e-05
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.88 seconds!
[tester] 
SST2Metric: acc=0.5091743119266054, hinge=5.532829027657115, ce=4.551282440122631
Local test acc @ epoch 1: 0.5092
Client 5 execute local training on 4 samples...
Local loss @ local epoch 0: 0.015845725312829018
Local loss @ local epoch 1: 0.002642260165885091
Local loss @ local epoch 2: 0.0001756961428327486
Local loss @ local epoch 3: 5.4625696066068485e-05
Local loss @ local epoch 4: 2.8937512979609892e-05
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.53 seconds!
[tester] 
SST2Metric: acc=0.5091743119266054, hinge=5.831995285979104, ce=4.850412468830613
Local test acc @ epoch 1: 0.5092
Client 2 execute local training on 5 samples...
Local loss @ local epoch 0: 0.005916155409067869
Local loss @ local epoch 1: 0.001290281768888235
Local loss @ local epoch 2: 6.94724585628137e-05
Local loss @ local epoch 3: 0.0007504296954721212
Local loss @ local epoch 4: 2.479547674738569e-06
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.4 seconds!
[tester] 
SST2Metric: acc=0.5091743119266054, hinge=6.442915692241914, ce=5.461286399204105
Local test acc @ epoch 1: 0.5092
Client 0 execute local training on 23 samples...
Local loss @ local epoch 0: 0.5479157567024231
Local loss @ local epoch 1: 0.23778006434440613
Local loss @ local epoch 2: 0.040702592581510544
Local loss @ local epoch 3: 0.02496114745736122
Local loss @ local epoch 4: 0.03791642189025879
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.27 seconds!
[tester] 
SST2Metric: acc=0.4908256880733945, hinge=3.131253888847631, ce=2.130617969900096
Local test acc @ epoch 1: 0.4908
Client 3 execute local training on 4 samples...
Local loss @ local epoch 0: 0.012648111209273338
Local loss @ local epoch 1: 0.0013936464674770832
Local loss @ local epoch 2: 0.00018201507919002324
Local loss @ local epoch 3: 2.3722324840491638e-05
Local loss @ local epoch 4: 1.6987156413961202e-05
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.46 seconds!
[tester] 
SST2Metric: acc=0.5091743119266054, hinge=5.743028044700623, ce=4.761524482722478
Local test acc @ epoch 1: 0.5092
Global evaluate on test data...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.32 seconds!
[tester] 
SST2Metric: acc=0.5091743119266054, hinge=4.522303229078241, ce=3.5414224064277717
Global test acc @ epoch 1: 0.5092
Global epoch 2...
Client 4 execute local training on 20 samples...
Local loss @ local epoch 0: 1.1414674520492554
Local loss @ local epoch 1: 0.5576349496841431
Local loss @ local epoch 2: 0.5435803532600403
Local loss @ local epoch 3: 0.5550177097320557
Local loss @ local epoch 4: 0.5284466743469238
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.63 seconds!
[tester] 
SST2Metric: acc=0.4908256880733945, hinge=2.0273602894686777, ce=0.9281452921552396
Local test acc @ epoch 2: 0.4908
Client 9 execute local training on 4 samples...
Local loss @ local epoch 0: 0.007763853762298822
Local loss @ local epoch 1: 0.009033573791384697
Local loss @ local epoch 2: 0.00010364613990532234
Local loss @ local epoch 3: 0.0003273467009421438
Local loss @ local epoch 4: 5.516126111615449e-05
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.61 seconds!
[tester] 
SST2Metric: acc=0.5091743119266054, hinge=5.81850119468269, ce=4.8369269822039955
Local test acc @ epoch 2: 0.5092
Client 8 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0012461654841899872
Local loss @ local epoch 1: 0.003931294195353985
Local loss @ local epoch 2: 1.376851651002653e-05
Local loss @ local epoch 3: 1.2367836461635306e-05
Local loss @ local epoch 4: 4.500134764384711e-06
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.41 seconds!
[tester] 
SST2Metric: acc=0.5091743119266054, hinge=6.745335462990157, ce=5.763694201040572
Local test acc @ epoch 2: 0.5092
Client 0 execute local training on 23 samples...
Local loss @ local epoch 0: 0.017000777646899223
Local loss @ local epoch 1: 0.08038805425167084
Local loss @ local epoch 2: 0.08833467960357666
Local loss @ local epoch 3: 0.030728498473763466
Local loss @ local epoch 4: 0.040256183594465256
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.29 seconds!
[tester] 
SST2Metric: acc=0.4908256880733945, hinge=2.7355173071590038, ce=1.7526508810323314
Local test acc @ epoch 2: 0.4908
Client 1 execute local training on 6 samples...
Local loss @ local epoch 0: 0.9543542265892029
Local loss @ local epoch 1: 0.499834805727005
Local loss @ local epoch 2: 0.6350865960121155
Local loss @ local epoch 3: 0.23847369849681854
Local loss @ local epoch 4: 0.448169082403183
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.22 seconds!
[tester] 
SST2Metric: acc=0.536697247706422, hinge=1.9566075572180093, ce=0.7099928601619301
Local test acc @ epoch 2: 0.5367
Client 3 execute local training on 4 samples...
Local loss @ local epoch 0: 0.004199465736746788
Local loss @ local epoch 1: 0.0014496842632070184
Local loss @ local epoch 2: 2.503391442587599e-06
Local loss @ local epoch 3: 2.0563575162668712e-06
Local loss @ local epoch 4: 1.7583349745109444e-06
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.47 seconds!
[tester] 
SST2Metric: acc=0.5091743119266054, hinge=6.850146723449777, ce=5.868504977554073
Local test acc @ epoch 2: 0.5092
Client 7 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0008897623047232628
Local loss @ local epoch 1: 0.0005320379277691245
Local loss @ local epoch 2: 9.631628927309066e-05
Local loss @ local epoch 3: 1.1205595910723787e-05
Local loss @ local epoch 4: 1.4305103377409978e-06
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.38 seconds!
[tester] 
SST2Metric: acc=0.5091743119266054, hinge=7.659882097069277, ce=6.678232160319972
Local test acc @ epoch 2: 0.5092
Client 6 execute local training on 6 samples...
Local loss @ local epoch 0: 1.357796549797058
Local loss @ local epoch 1: 1.5916919708251953
Local loss @ local epoch 2: 0.5106489658355713
Local loss @ local epoch 3: 0.6224685907363892
Local loss @ local epoch 4: 0.5790475010871887
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.53 seconds!
[tester] 
SST2Metric: acc=0.5045871559633027, hinge=1.996285767730223, ce=0.9354087322130116
Local test acc @ epoch 2: 0.5046
Client 2 execute local training on 5 samples...
Local loss @ local epoch 0: 0.0005018864758312702
Local loss @ local epoch 1: 0.00014217816351447254
Local loss @ local epoch 2: 4.982933660357958e-06
Local loss @ local epoch 3: 2.4080241018964443e-06
Local loss @ local epoch 4: 1.5258775647453149e-06
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.63 seconds!
[tester] 
SST2Metric: acc=0.5091743119266054, hinge=7.684053998474681, ce=6.702404363029585
Local test acc @ epoch 2: 0.5092
Client 5 execute local training on 4 samples...
Local loss @ local epoch 0: 0.00010436215961817652
Local loss @ local epoch 1: 8.522770076524466e-05
Local loss @ local epoch 2: 6.85453073856479e-07
Local loss @ local epoch 3: 5.066393100605637e-07
Local loss @ local epoch 4: 5.066393100605637e-07
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.38 seconds!
[tester] 
SST2Metric: acc=0.5091743119266054, hinge=7.343724239856825, ce=6.362076285645474
Local test acc @ epoch 2: 0.5092
Global evaluate on test data...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.32 seconds!
[tester] 
SST2Metric: acc=0.5091743119266054, hinge=5.223905735059616, ce=4.24245039122302
Global test acc @ epoch 2: 0.5092
Global epoch 3...
Client 7 execute local training on 4 samples...
Local loss @ local epoch 0: 0.000141963450005278
Local loss @ local epoch 1: 6.169057996885385e-06
Local loss @ local epoch 2: 3.8743007735320134e-07
Local loss @ local epoch 3: 4.10948196076788e-05
Local loss @ local epoch 4: 0.022224633023142815
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.23 seconds!
[tester] 
SST2Metric: acc=0.5091743119266054, hinge=6.789358786486704, ce=5.8077200436532666
Local test acc @ epoch 3: 0.5092
Client 8 execute local training on 4 samples...
Local loss @ local epoch 0: 4.72947140224278e-05
Local loss @ local epoch 1: 1.1920917586394353e-06
Local loss @ local epoch 2: 1.7285316289417096e-06
Local loss @ local epoch 3: 5.9604641222676946e-08
Local loss @ local epoch 4: 9.427696204511449e-05
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.25 seconds!
[tester] 
SST2Metric: acc=0.5091743119266054, hinge=10.030046200533526, ce=9.04839482876139
Local test acc @ epoch 3: 0.5092
Client 6 execute local training on 6 samples...
Local loss @ local epoch 0: 1.918413758277893
Local loss @ local epoch 1: 0.8531294465065002
Local loss @ local epoch 2: 0.6187292337417603
Local loss @ local epoch 3: 0.9543601870536804
Local loss @ local epoch 4: 0.3850823938846588
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.37 seconds!
[tester] 
SST2Metric: acc=0.5057339449541285, hinge=1.968579182384211, ce=0.8684787769383246
Local test acc @ epoch 3: 0.5057
Client 1 execute local training on 6 samples...
Local loss @ local epoch 0: 1.425633430480957
Local loss @ local epoch 1: 0.35229647159576416
Local loss @ local epoch 2: 0.24498534202575684
Local loss @ local epoch 3: 0.13745829463005066
Local loss @ local epoch 4: 0.4612414836883545
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.26 seconds!
[tester] 
SST2Metric: acc=0.5103211009174312, hinge=1.926467747316448, ce=0.8378047647826169
Local test acc @ epoch 3: 0.5103
Client 9 execute local training on 4 samples...
Local loss @ local epoch 0: 0.00024970778031274676
Local loss @ local epoch 1: 2.8610179469978902e-06
Local loss @ local epoch 2: 2.205368673457997e-06
Local loss @ local epoch 3: 6.556508651556214e-07
Local loss @ local epoch 4: 2.294775185873732e-06
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.62 seconds!
[tester] 
SST2Metric: acc=0.5091743119266054, hinge=7.309287907880381, ce=6.327640242669533
Local test acc @ epoch 3: 0.5092
Client 2 execute local training on 5 samples...
Local loss @ local epoch 0: 0.00010785224003484473
Local loss @ local epoch 1: 2.765651515801437e-06
Local loss @ local epoch 2: 1.955029119926621e-06
Local loss @ local epoch 3: 0.0001464309898437932
Local loss @ local epoch 4: 7.152556946721234e-08
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.49 seconds!
[tester] 
SST2Metric: acc=0.5091743119266054, hinge=8.063141787817719, ce=7.081491243675212
Local test acc @ epoch 3: 0.5092
Client 4 execute local training on 20 samples...
Local loss @ local epoch 0: 1.7788194417953491
Local loss @ local epoch 1: 0.640730619430542
Local loss @ local epoch 2: 0.47925376892089844
Local loss @ local epoch 3: 0.4034522771835327
Local loss @ local epoch 4: 0.28474050760269165
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.46 seconds!
[tester] 
SST2Metric: acc=0.4919724770642202, hinge=2.049648899004, ce=1.0572668866280022
Local test acc @ epoch 3: 0.492
Client 3 execute local training on 4 samples...
Local loss @ local epoch 0: 0.001235080067999661
Local loss @ local epoch 1: 8.940655789047014e-06
Local loss @ local epoch 2: 5.46261144336313e-05
Local loss @ local epoch 3: 2.0563577436405467e-06
Local loss @ local epoch 4: 4.854631333728321e-05
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.26 seconds!
[tester] 
SST2Metric: acc=0.5103211009174312, hinge=3.4640879964609756, ce=2.49621339046627
Local test acc @ epoch 3: 0.5103
Client 0 execute local training on 23 samples...
Local loss @ local epoch 0: 0.3071129620075226
Local loss @ local epoch 1: 0.02288082242012024
Local loss @ local epoch 2: 0.02757340483367443
Local loss @ local epoch 3: 0.029902111738920212
Local loss @ local epoch 4: 0.03320140764117241
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.46 seconds!
[tester] 
SST2Metric: acc=0.4908256880733945, hinge=3.301535959910909, ce=2.2991208532534606
Local test acc @ epoch 3: 0.4908
Client 5 execute local training on 4 samples...
Local loss @ local epoch 0: 0.004241812974214554
Local loss @ local epoch 1: 2.8669333914876916e-05
Local loss @ local epoch 2: 3.1172479793895036e-05
Local loss @ local epoch 3: 0.00023299051099456847
Local loss @ local epoch 4: 4.768369876728684e-07
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.59 seconds!
[tester] 
SST2Metric: acc=0.5091743119266054, hinge=7.2762913846094674, ce=6.294646430589708
Local test acc @ epoch 3: 0.5092
Global evaluate on test data...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.63 seconds!
[tester] 
SST2Metric: acc=0.5091743119266054, hinge=7.750969573992108, ce=6.7693194268493455
Global test acc @ epoch 3: 0.5092
Global epoch 4...
Client 3 execute local training on 4 samples...
Local loss @ local epoch 0: 4.1723239974089665e-07
Local loss @ local epoch 1: 9.715497071738355e-06
Local loss @ local epoch 2: 2.3841855067985307e-07
Local loss @ local epoch 3: 1.251696403414826e-06
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.36 seconds!
[tester] 
SST2Metric: acc=0.5091743119266054, hinge=11.08341635257826, ce=10.101764963307511
Local test acc @ epoch 4: 0.5092
Client 7 execute local training on 4 samples...
Local loss @ local epoch 0: 8.344644584212801e-07
Local loss @ local epoch 1: 0.00016907828103285283
Local loss @ local epoch 2: 2.354380058022798e-06
Local loss @ local epoch 3: 1.3351332199817989e-05
Local loss @ local epoch 4: 1.2218941947139683e-06
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.35 seconds!
[tester] 
SST2Metric: acc=0.5091743119266054, hinge=7.940055803421441, ce=6.958405278828165
Local test acc @ epoch 4: 0.5092
Client 1 execute local training on 6 samples...
Local loss @ local epoch 0: 2.307173013687134
Local loss @ local epoch 1: 0.3148464858531952
Local loss @ local epoch 2: 0.21144457161426544
Local loss @ local epoch 3: 0.9685199856758118
Local loss @ local epoch 4: 0.4112706482410431
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.38 seconds!
[tester] 
SST2Metric: acc=0.5091743119266054, hinge=2.8734017643359824, ce=1.9216786231845617
Local test acc @ epoch 4: 0.5092
Client 0 execute local training on 23 samples...
Local loss @ local epoch 0: 0.0010940551292151213
Local loss @ local epoch 1: 0.6964190602302551
Local loss @ local epoch 2: 0.012212634086608887
Local loss @ local epoch 3: 0.019657770171761513
Local loss @ local epoch 4: 0.07808823138475418
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.43 seconds!
[tester] 
SST2Metric: acc=0.4908256880733945, hinge=2.6410772619991127, ce=1.6600648470974844
Local test acc @ epoch 4: 0.4908
Client 6 execute local training on 6 samples...
Local loss @ local epoch 0: 2.009307861328125
Local loss @ local epoch 1: 0.6692226529121399
Local loss @ local epoch 2: 0.6470866799354553
Local loss @ local epoch 3: 0.3998468816280365
Local loss @ local epoch 4: 0.5923206210136414
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.45 seconds!
[tester] 
SST2Metric: acc=0.5091743119266054, hinge=2.4941919916266695, ce=1.5543204033661873
Local test acc @ epoch 4: 0.5092
Client 5 execute local training on 4 samples...
Local loss @ local epoch 0: 6.973709332669387e-06
Local loss @ local epoch 1: 0.00017127108003478497
Local loss @ local epoch 2: 6.407444743672386e-06
Local loss @ local epoch 3: 3.874300489314919e-07
Local loss @ local epoch 4: 1.3053328075329773e-05
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.48 seconds!
[tester] 
SST2Metric: acc=0.5091743119266054, hinge=9.64162431944401, ce=8.659972921423956
Local test acc @ epoch 4: 0.5092
Client 2 execute local training on 5 samples...
Local loss @ local epoch 0: 6.7233777372166514e-06
Local loss @ local epoch 1: 0.0005244233761914074
Local loss @ local epoch 2: 5.006788228456571e-07
Local loss @ local epoch 3: 1.1444069514254807e-06
Local loss @ local epoch 4: 6.437297770389705e-07
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.31 seconds!
[tester] 
SST2Metric: acc=0.5091743119266054, hinge=6.786309809859739, ce=5.804689154712294
Local test acc @ epoch 4: 0.5092
Client 8 execute local training on 4 samples...
Local loss @ local epoch 0: 3.0696328394697048e-06
Local loss @ local epoch 1: 0.0007070733699947596
Local loss @ local epoch 2: 1.996749460886349e-06
Local loss @ local epoch 3: 5.3163192205829546e-05
Local loss @ local epoch 4: 1.4901156930591242e-07
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.59 seconds!
[tester] 
SST2Metric: acc=0.5091743119266054, hinge=8.645255281290877, ce=7.663603989492863
Local test acc @ epoch 4: 0.5092
Client 9 execute local training on 4 samples...
Local loss @ local epoch 0: 2.1159614789212355e-06
Local loss @ local epoch 1: 7.277459371834993e-05
Local loss @ local epoch 2: 7.152551688704989e-07
Local loss @ local epoch 3: 5.483598670252832e-06
Local loss @ local epoch 4: 2.413982883808785e-06
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.56 seconds!
[tester] 
SST2Metric: acc=0.5091743119266054, hinge=8.819881924795448, ce=7.838230573802913
Local test acc @ epoch 4: 0.5092
Client 4 execute local training on 20 samples...
Local loss @ local epoch 0: 1.7307676076889038
Local loss @ local epoch 1: 0.9230336546897888
Local loss @ local epoch 2: 0.2367810606956482
Local loss @ local epoch 3: 0.11847507953643799
Local loss @ local epoch 4: 0.08122649788856506
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.51 seconds!
[tester] 
SST2Metric: acc=0.49311926605504586, hinge=2.114948187399348, ce=1.130879514671247
Local test acc @ epoch 4: 0.4931
Global evaluate on test data...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.48 seconds!
[tester] 
SST2Metric: acc=0.6456422018348624, hinge=1.706652389753849, ce=0.6398819414300656
Global test acc @ epoch 4: 0.6456
Global epoch 5...
Client 5 execute local training on 4 samples...
Local loss @ local epoch 0: 0.39688917994499207
Local loss @ local epoch 1: 0.00010611808829708025
Local loss @ local epoch 2: 8.055110083660111e-05
Local loss @ local epoch 3: 8.469291788060218e-05
Local loss @ local epoch 4: 0.00010441895574331284
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.53 seconds!
[tester] 
SST2Metric: acc=0.5091743119266054, hinge=4.958636432612708, ce=3.977337942645728
Local test acc @ epoch 5: 0.5092
Client 4 execute local training on 20 samples...
Local loss @ local epoch 0: 1.8897656202316284
Local loss @ local epoch 1: 0.5914193987846375
Local loss @ local epoch 2: 0.423247754573822
Local loss @ local epoch 3: 0.17366009950637817
Local loss @ local epoch 4: 0.0575544610619545
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.48 seconds!
[tester] 
SST2Metric: acc=0.5080275229357798, hinge=2.204485842941004, ce=1.2539306668513412
Local test acc @ epoch 5: 0.508
Client 2 execute local training on 5 samples...
Local loss @ local epoch 0: 0.9142429232597351
Local loss @ local epoch 1: 0.00021712093439418823
Local loss @ local epoch 2: 0.00032515425118617713
Local loss @ local epoch 3: 0.0003104704665020108
Local loss @ local epoch 4: 0.0003079935268033296
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.46 seconds!
[tester] 
SST2Metric: acc=0.5091743119266054, hinge=4.753236886558183, ce=3.7721461942357997
Local test acc @ epoch 5: 0.5092
Client 7 execute local training on 4 samples...
Local loss @ local epoch 0: 0.33595144748687744
Local loss @ local epoch 1: 0.0001723288733046502
Local loss @ local epoch 2: 0.00025733650545589626
Local loss @ local epoch 3: 0.00018270027067046613
Local loss @ local epoch 4: 0.00026651384541764855
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.26 seconds!
[tester] 
SST2Metric: acc=0.5091743119266054, hinge=4.674960072981108, ce=3.6939278312738764
Local test acc @ epoch 5: 0.5092
Client 6 execute local training on 6 samples...
Local loss @ local epoch 0: 0.5570810437202454
Local loss @ local epoch 1: 1.030899167060852
Local loss @ local epoch 2: 0.8309447169303894
Local loss @ local epoch 3: 0.4256279468536377
Local loss @ local epoch 4: 0.3650906980037689
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.36 seconds!
[tester] 
SST2Metric: acc=0.5126146788990825, hinge=1.944719586897334, ce=0.8866729003573777
Local test acc @ epoch 5: 0.5126
Client 9 execute local training on 4 samples...
Local loss @ local epoch 0: 0.09583677351474762
Local loss @ local epoch 1: 8.213153341785073e-05
Local loss @ local epoch 2: 0.00014613865641877055
Local loss @ local epoch 3: 0.0001159531602752395
Local loss @ local epoch 4: 0.0001211676062666811
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.7 seconds!
[tester] 
SST2Metric: acc=0.5091743119266054, hinge=5.227342141877621, ce=4.245896816794086
Local test acc @ epoch 5: 0.5092
Client 8 execute local training on 4 samples...
Local loss @ local epoch 0: 0.019822603091597557
Local loss @ local epoch 1: 2.5837967768893577e-05
Local loss @ local epoch 2: 5.435723141999915e-05
Local loss @ local epoch 3: 4.052999793202616e-05
Local loss @ local epoch 4: 4.0768391045276076e-05
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.45 seconds!
[tester] 
SST2Metric: acc=0.5091743119266054, hinge=5.8969591320107835, ce=4.9153589116816745
Local test acc @ epoch 5: 0.5092
Client 3 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0038104108534753323
Local loss @ local epoch 1: 5.483607765199849e-06
Local loss @ local epoch 2: 1.2397687896736898e-05
Local loss @ local epoch 3: 1.3381149983615614e-05
Local loss @ local epoch 4: 1.734479155857116e-05
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.45 seconds!
[tester] 
SST2Metric: acc=0.5091743119266054, hinge=6.4310559143713855, ce=5.449425562804725
Local test acc @ epoch 5: 0.5092
Client 1 execute local training on 6 samples...
Local loss @ local epoch 0: 0.8006638884544373
Local loss @ local epoch 1: 0.3264610171318054
Local loss @ local epoch 2: 0.849602460861206
Local loss @ local epoch 3: 0.39250341057777405
Local loss @ local epoch 4: 0.11014539748430252
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.48 seconds!
[tester] 
SST2Metric: acc=0.5103211009174312, hinge=2.206065469925557, ce=1.2129546375176228
Local test acc @ epoch 5: 0.5103
Client 0 execute local training on 23 samples...
Local loss @ local epoch 0: 0.0024304459802806377
Local loss @ local epoch 1: 0.4695112109184265
Local loss @ local epoch 2: 0.007596518378704786
Local loss @ local epoch 3: 0.030864108353853226
Local loss @ local epoch 4: 0.06672435998916626
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.42 seconds!
[tester] 
SST2Metric: acc=0.49311926605504586, hinge=2.707695105753907, ce=1.7268577346561151
Local test acc @ epoch 5: 0.4931
Global evaluate on test data...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.9 seconds!
[tester] 
SST2Metric: acc=0.5091743119266054, hinge=5.154249845294777, ce=4.172922882366188
Global test acc @ epoch 5: 0.5092
Global epoch 6...
Client 8 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0001606292644282803
Local loss @ local epoch 1: 7.748599273327272e-07
Local loss @ local epoch 2: 2.6225986857753014e-06
Local loss @ local epoch 3: 5.9604641222676946e-08
Local loss @ local epoch 4: 2.0861621408130304e-07
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.47 seconds!
[tester] 
SST2Metric: acc=0.5091743119266054, hinge=9.217250974900132, ce=8.235619250240676
Local test acc @ epoch 6: 0.5092
Client 2 execute local training on 5 samples...
Local loss @ local epoch 0: 2.6773990612127818e-05
Local loss @ local epoch 1: 7.629390665897517e-07
Local loss @ local epoch 2: 2.622603858526418e-07
Local loss @ local epoch 3: 4.768371297814156e-08
Local loss @ local epoch 4: 7.152556946721234e-08
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.54 seconds!
[tester] 
SST2Metric: acc=0.5091743119266054, hinge=9.342746852734766, ce=8.36109552922052
Local test acc @ epoch 6: 0.5092
Client 6 execute local training on 6 samples...
Local loss @ local epoch 0: 1.9096540212631226
Local loss @ local epoch 1: 0.8593835234642029
Local loss @ local epoch 2: 0.5396166443824768
Local loss @ local epoch 3: 0.5057752728462219
Local loss @ local epoch 4: 2.146143913269043
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.57 seconds!
[tester] 
SST2Metric: acc=0.4954128440366973, hinge=2.5307396273000524, ce=1.5410080199394751
Local test acc @ epoch 6: 0.4954
Client 5 execute local training on 4 samples...
Local loss @ local epoch 0: 3.343740172567777e-05
Local loss @ local epoch 1: 1.4901159772762185e-07
Local loss @ local epoch 2: 1.1026852462237002e-06
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.43 seconds!
[tester] 
SST2Metric: acc=0.5091743119266054, hinge=9.574704480827402, ce=8.593053095931307
Local test acc @ epoch 6: 0.5092
Client 7 execute local training on 4 samples...
Local loss @ local epoch 0: 2.5570006982889026e-05
Local loss @ local epoch 1: 1.1026852462237002e-06
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.41 seconds!
[tester] 
SST2Metric: acc=0.5091743119266054, hinge=8.848669563958405, ce=7.867018602583387
Local test acc @ epoch 6: 0.5092
Client 9 execute local training on 4 samples...
Local loss @ local epoch 0: 1.2993683412787504e-05
Local loss @ local epoch 1: 1.7881392011531716e-07
Local loss @ local epoch 2: 2.3841855067985307e-07
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 5.364414619180025e-07
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.41 seconds!
[tester] 
SST2Metric: acc=0.5091743119266054, hinge=12.589994850508663, ce=11.608343369370207
Local test acc @ epoch 6: 0.5092
Client 4 execute local training on 20 samples...
Local loss @ local epoch 0: 0.48231860995292664
Local loss @ local epoch 1: 0.5391560196876526
Local loss @ local epoch 2: 0.22016668319702148
Local loss @ local epoch 3: 0.19227761030197144
Local loss @ local epoch 4: 0.10392329841852188
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.51 seconds!
[tester] 
SST2Metric: acc=0.5745412844036697, hinge=1.6675108217318124, ce=0.7728769434701412
Local test acc @ epoch 6: 0.5745
Client 3 execute local training on 4 samples...
Local loss @ local epoch 0: 9.947506623575464e-05
Local loss @ local epoch 1: 1.0430807151351473e-06
Local loss @ local epoch 2: 1.8775438093143748e-06
Local loss @ local epoch 3: 2.9802320611338473e-08
Local loss @ local epoch 4: 2.9802320611338473e-08
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.38 seconds!
[tester] 
SST2Metric: acc=0.5091743119266054, hinge=8.970183346249641, ce=7.988532005783615
Local test acc @ epoch 6: 0.5092
Client 0 execute local training on 23 samples...
Local loss @ local epoch 0: 0.2615409195423126
Local loss @ local epoch 1: 0.3450138568878174
Local loss @ local epoch 2: 0.031345076858997345
Local loss @ local epoch 3: 0.10790734738111496
Local loss @ local epoch 4: 0.057572342455387115
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.52 seconds!
[tester] 
SST2Metric: acc=0.4908256880733945, hinge=2.8584460255202897, ce=1.8694606434314622
Local test acc @ epoch 6: 0.4908
Client 1 execute local training on 6 samples...
Local loss @ local epoch 0: 0.7398767471313477
Local loss @ local epoch 1: 1.121479868888855
Local loss @ local epoch 2: 0.24613960087299347
Local loss @ local epoch 3: 0.13918966054916382
Local loss @ local epoch 4: 0.0639810562133789
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.45 seconds!
[tester] 
SST2Metric: acc=0.5172018348623854, hinge=2.3225701894235176, ce=1.3898066601485288
Local test acc @ epoch 6: 0.5172
Global evaluate on test data...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.52 seconds!
[tester] 
SST2Metric: acc=0.5091743119266054, hinge=7.156583696330359, ce=6.174938100193609
Global test acc @ epoch 6: 0.5092
Global epoch 7...
Client 1 execute local training on 6 samples...
Local loss @ local epoch 0: 1.5413126945495605
Local loss @ local epoch 1: 0.1710551530122757
Local loss @ local epoch 2: 0.30562034249305725
Local loss @ local epoch 3: 0.06903086602687836
Local loss @ local epoch 4: 0.03175407275557518
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.49 seconds!
[tester] 
SST2Metric: acc=0.5172018348623854, hinge=2.468854328907958, ce=1.5347340293228626
Local test acc @ epoch 7: 0.5172
Client 0 execute local training on 23 samples...
Local loss @ local epoch 0: 0.009152261540293694
Local loss @ local epoch 1: 0.030752887949347496
Local loss @ local epoch 2: 0.03478669375181198
Local loss @ local epoch 3: 0.03616103529930115
Local loss @ local epoch 4: 0.03262197598814964
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.54 seconds!
[tester] 
SST2Metric: acc=0.4908256880733945, hinge=3.213003920852591, ce=2.213234394515326
Local test acc @ epoch 7: 0.4908
Client 2 execute local training on 5 samples...
Local loss @ local epoch 0: 5.04234922118485e-05
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 1.4305113893442467e-07
Local loss @ local epoch 3: 2.3841853646899835e-07
Local loss @ local epoch 4: 1.668929883180681e-07
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.7 seconds!
[tester] 
SST2Metric: acc=0.5091743119266054, hinge=8.495513611977254, ce=7.5138629040586835
Local test acc @ epoch 7: 0.5092
Client 5 execute local training on 4 samples...
Local loss @ local epoch 0: 5.81141375732841e-06
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 2.9802316703353426e-07
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.55 seconds!
[tester] 
SST2Metric: acc=0.5091743119266054, hinge=9.859836740231295, ce=8.878185407967742
Local test acc @ epoch 7: 0.5092
Client 8 execute local training on 4 samples...
Local loss @ local epoch 0: 1.1414166692702565e-05
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 2.9802320611338473e-08
Local loss @ local epoch 3: 1.4901158351676713e-07
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.56 seconds!
[tester] 
SST2Metric: acc=0.5091743119266054, hinge=10.069235224242604, ce=9.087583865594427
Local test acc @ epoch 7: 0.5092
Client 7 execute local training on 4 samples...
Local loss @ local epoch 0: 5.900836640648777e-06
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 5.9604641222676946e-08
Local loss @ local epoch 3: 2.9802320611338473e-08
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.33 seconds!
[tester] 
SST2Metric: acc=0.5091743119266054, hinge=9.19607926071237, ce=8.214427913410947
Local test acc @ epoch 7: 0.5092
Client 9 execute local training on 4 samples...
Local loss @ local epoch 0: 5.006763785786461e-06
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 8.940696005765858e-08
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.61 seconds!
[tester] 
SST2Metric: acc=0.5091743119266054, hinge=9.699985963488938, ce=8.718334604977468
Local test acc @ epoch 7: 0.5092
Client 6 execute local training on 6 samples...
Local loss @ local epoch 0: 2.1822588443756104
Local loss @ local epoch 1: 0.6144163012504578
Local loss @ local epoch 2: 0.7000454068183899
Local loss @ local epoch 3: 0.6666603684425354
Local loss @ local epoch 4: 0.4524630606174469
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.51 seconds!
[tester] 
SST2Metric: acc=0.5126146788990825, hinge=1.8895081571482737, ce=0.8980537918182688
Local test acc @ epoch 7: 0.5126
Client 3 execute local training on 4 samples...
Local loss @ local epoch 0: 5.573008365900023e-06
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 1.7881392011531716e-07
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.45 seconds!
[tester] 
SST2Metric: acc=0.5091743119266054, hinge=10.789655401072372, ce=9.808004094920028
Local test acc @ epoch 7: 0.5092
Client 4 execute local training on 20 samples...
Local loss @ local epoch 0: 1.0436928272247314
Local loss @ local epoch 1: 0.2690024673938751
Local loss @ local epoch 2: 0.22538916766643524
Local loss @ local epoch 3: 0.09027963131666183
Local loss @ local epoch 4: 0.039309144020080566
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.41 seconds!
[tester] 
SST2Metric: acc=0.5217889908256881, hinge=2.0674886544910045, ce=1.1264524524091581
Local test acc @ epoch 7: 0.5218
Global evaluate on test data...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.6 seconds!
[tester] 
SST2Metric: acc=0.5091743119266054, hinge=8.851289604781964, ce=7.869638259667869
Global test acc @ epoch 7: 0.5092
Global epoch 8...
Client 3 execute local training on 4 samples...
Local loss @ local epoch 0: 5.9604641222676946e-08
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.59 seconds!
[tester] 
SST2Metric: acc=0.5091743119266054, hinge=11.69306762940293, ce=10.711416257630795
Local test acc @ epoch 8: 0.5092
Client 1 execute local training on 6 samples...
Local loss @ local epoch 0: 2.504737615585327
Local loss @ local epoch 1: 0.44830119609832764
Local loss @ local epoch 2: 0.587811291217804
Local loss @ local epoch 3: 0.2222444862127304
Local loss @ local epoch 4: 0.4309089481830597
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.4 seconds!
[tester] 
SST2Metric: acc=0.5378440366972477, hinge=1.906078633365281, ce=0.8613612575268527
Local test acc @ epoch 8: 0.5378
Client 4 execute local training on 20 samples...
Local loss @ local epoch 0: 0.7003425359725952
Local loss @ local epoch 1: 0.7505360841751099
Local loss @ local epoch 2: 0.32891303300857544
Local loss @ local epoch 3: 0.2272188663482666
Local loss @ local epoch 4: 0.11693722754716873
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.33 seconds!
[tester] 
SST2Metric: acc=0.5022935779816514, hinge=2.135397782019519, ce=1.166167574464728
Local test acc @ epoch 8: 0.5023
Client 6 execute local training on 6 samples...
Local loss @ local epoch 0: 2.2050108909606934
Local loss @ local epoch 1: 0.4954462945461273
Local loss @ local epoch 2: 0.9532361626625061
Local loss @ local epoch 3: 0.43935635685920715
Local loss @ local epoch 4: 0.4363967478275299
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.18 seconds!
[tester] 
SST2Metric: acc=0.5091743119266054, hinge=1.9946573229011046, ce=0.877772467672278
Local test acc @ epoch 8: 0.5092
Client 5 execute local training on 4 samples...
Local loss @ local epoch 0: 2.5629958599893143e-06
Local loss @ local epoch 1: 3.0696328394697048e-06
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 2.9802320611338473e-08
Local loss @ local epoch 4: 6.85453073856479e-07
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.4 seconds!
[tester] 
SST2Metric: acc=0.5091743119266054, hinge=11.615184162734845, ce=10.633532786588056
Local test acc @ epoch 8: 0.5092
Client 8 execute local training on 4 samples...
Local loss @ local epoch 0: 2.9206230465206318e-06
Local loss @ local epoch 1: 2.3841853646899835e-07
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 2.980224962811917e-06
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.65 seconds!
[tester] 
SST2Metric: acc=0.5091743119266054, hinge=11.54277811137908, ce=10.561126761480208
Local test acc @ epoch 8: 0.5092
Client 2 execute local training on 5 samples...
Local loss @ local epoch 0: 2.0027136997669004e-06
Local loss @ local epoch 1: 2.6702832656155806e-06
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 9.536742595628311e-08
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.63 seconds!
[tester] 
SST2Metric: acc=0.5091743119266054, hinge=9.216568693108515, ce=8.23491763193673
Local test acc @ epoch 8: 0.5092
Client 9 execute local training on 4 samples...
Local loss @ local epoch 0: 1.6391260260206764e-06
Local loss @ local epoch 1: 2.9802320611338473e-08
Local loss @ local epoch 2: 2.9802320611338473e-08
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.51 seconds!
[tester] 
SST2Metric: acc=0.5091743119266054, hinge=8.888837718088693, ce=7.907186661701684
Local test acc @ epoch 8: 0.5092
Client 0 execute local training on 23 samples...
Local loss @ local epoch 0: 0.048585597425699234
Local loss @ local epoch 1: 0.030829399824142456
Local loss @ local epoch 2: 0.05547637492418289
Local loss @ local epoch 3: 0.054321106523275375
Local loss @ local epoch 4: 0.03328051045536995
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.7 seconds!
[tester] 
SST2Metric: acc=0.4908256880733945, hinge=2.6876734976374776, ce=1.7063017490260097
Local test acc @ epoch 8: 0.4908
Client 7 execute local training on 4 samples...
Local loss @ local epoch 0: 9.745305760588963e-06
Local loss @ local epoch 1: 2.0175812096567824e-05
Local loss @ local epoch 2: 3.814685896941228e-06
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.48 seconds!
[tester] 
SST2Metric: acc=0.5091743119266054, hinge=10.342757557510236, ce=9.361106111369002
Local test acc @ epoch 8: 0.5092
Global evaluate on test data...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.43 seconds!
[tester] 
SST2Metric: acc=0.6318807339449541, hinge=1.6121613366888203, ce=0.6665755906783113
Global test acc @ epoch 8: 0.6319
Global epoch 9...
Client 9 execute local training on 4 samples...
Local loss @ local epoch 0: 0.23030006885528564
Local loss @ local epoch 1: 8.612823876319453e-06
Local loss @ local epoch 2: 1.4990429008321371e-05
Local loss @ local epoch 3: 3.2215706596616656e-05
Local loss @ local epoch 4: 6.99134252499789e-05
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.43 seconds!
[tester] 
SST2Metric: acc=0.5091743119266054, hinge=4.600205258491936, ce=3.619358449597862
Local test acc @ epoch 9: 0.5092
Client 8 execute local training on 4 samples...
Local loss @ local epoch 0: 0.08986389636993408
Local loss @ local epoch 1: 1.4603116369471536e-06
Local loss @ local epoch 2: 9.536733500681294e-07
Local loss @ local epoch 3: 1.9371473172213882e-06
Local loss @ local epoch 4: 9.000218597066123e-06
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.48 seconds!
[tester] 
SST2Metric: acc=0.5091743119266054, hinge=5.834503318191668, ce=4.852909817194447
Local test acc @ epoch 9: 0.5092
Client 2 execute local training on 5 samples...
Local loss @ local epoch 0: 0.013333335518836975
Local loss @ local epoch 1: 8.821482992971141e-07
Local loss @ local epoch 2: 1.9311880805616966e-06
Local loss @ local epoch 3: 6.151175512059126e-06
Local loss @ local epoch 4: 7.247892881423468e-06
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.59 seconds!
[tester] 
SST2Metric: acc=0.5091743119266054, hinge=5.95574032057316, ce=4.974139108842057
Local test acc @ epoch 9: 0.5092
Client 0 execute local training on 23 samples...
Local loss @ local epoch 0: 0.0015859147533774376
Local loss @ local epoch 1: 0.09015633910894394
Local loss @ local epoch 2: 0.052740711718797684
Local loss @ local epoch 3: 0.04858241602778435
Local loss @ local epoch 4: 0.05363348498940468
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.33 seconds!
[tester] 
SST2Metric: acc=0.4908256880733945, hinge=2.603838813961099, ce=1.6299751586870317
Local test acc @ epoch 9: 0.4908
Client 6 execute local training on 6 samples...
Local loss @ local epoch 0: 0.47786667943000793
Local loss @ local epoch 1: 0.8467838764190674
Local loss @ local epoch 2: 0.43636074662208557
Local loss @ local epoch 3: 0.36100149154663086
Local loss @ local epoch 4: 0.3459244966506958
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.4 seconds!
[tester] 
SST2Metric: acc=0.5424311926605505, hinge=1.850602800692987, ce=0.821211482680172
Local test acc @ epoch 9: 0.5424
Client 5 execute local training on 4 samples...
Local loss @ local epoch 0: 0.029812386259436607
Local loss @ local epoch 1: 7.152554530875932e-07
Local loss @ local epoch 2: 1.3709056929656072e-06
Local loss @ local epoch 3: 5.811430128233042e-06
Local loss @ local epoch 4: 6.5862805058714e-06
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.45 seconds!
[tester] 
SST2Metric: acc=0.5091743119266054, hinge=5.580868918961341, ce=4.599327366952879
Local test acc @ epoch 9: 0.5092
Client 1 execute local training on 6 samples...
Local loss @ local epoch 0: 0.38047167658805847
Local loss @ local epoch 1: 3.366408348083496
Local loss @ local epoch 2: 0.3701777756214142
Local loss @ local epoch 3: 0.40613409876823425
Local loss @ local epoch 4: 0.11108586192131042
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.49 seconds!
[tester] 
SST2Metric: acc=0.5286697247706422, hinge=1.8477468799560441, ce=0.9308129954365415
Local test acc @ epoch 9: 0.5287
Client 4 execute local training on 20 samples...
Local loss @ local epoch 0: 0.3226630389690399
Local loss @ local epoch 1: 0.28701251745224
Local loss @ local epoch 2: 0.09233883768320084
Local loss @ local epoch 3: 0.029122235253453255
Local loss @ local epoch 4: 0.011998900212347507
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.46 seconds!
[tester] 
SST2Metric: acc=0.5286697247706422, hinge=2.1093476573261647, ce=1.1835194283395731
Local test acc @ epoch 9: 0.5287
Client 3 execute local training on 4 samples...
Local loss @ local epoch 0: 0.04300755262374878
Local loss @ local epoch 1: 2.1755656689492753e-06
Local loss @ local epoch 2: 2.503391442587599e-06
Local loss @ local epoch 3: 7.629364972672192e-06
Local loss @ local epoch 4: 1.5288469512597658e-05
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.57 seconds!
[tester] 
SST2Metric: acc=0.5091743119266054, hinge=5.322521655931386, ce=4.341058290587621
Local test acc @ epoch 9: 0.5092
Client 7 execute local training on 4 samples...
Local loss @ local epoch 0: 0.015695590525865555
Local loss @ local epoch 1: 3.278254894212296e-07
Local loss @ local epoch 2: 4.768370445162873e-07
Local loss @ local epoch 3: 1.668928234721534e-06
Local loss @ local epoch 4: 3.159040034006466e-06
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.48 seconds!
[tester] 
SST2Metric: acc=0.5091743119266054, hinge=5.792000162492105, ce=4.810418817000661
Local test acc @ epoch 9: 0.5092
Global evaluate on test data...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.48 seconds!
[tester] 
SST2Metric: acc=0.5091743119266054, hinge=4.098283195714338, ce=3.1192743394004796
Global test acc @ epoch 9: 0.5092
Global epoch 10...
Client 2 execute local training on 5 samples...
Local loss @ local epoch 0: 0.00034722749842330813
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 2.3841855067985307e-07
Local loss @ local epoch 3: 1.0490409749763785e-06
Local loss @ local epoch 4: 6.914135042279668e-07
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.44 seconds!
[tester] 
SST2Metric: acc=0.5091743119266054, hinge=8.044301492358567, ce=7.0626506564814
Local test acc @ epoch 10: 0.5092
Client 7 execute local training on 4 samples...
Local loss @ local epoch 0: 0.00031799168209545314
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 6.258485427679261e-07
Local loss @ local epoch 3: 3.0994351618574e-06
Local loss @ local epoch 4: 3.337852831464261e-06
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.45 seconds!
[tester] 
SST2Metric: acc=0.5091743119266054, hinge=7.467413414508925, ce=6.485764000257179
Local test acc @ epoch 10: 0.5092
Client 8 execute local training on 4 samples...
Local loss @ local epoch 0: 1.984805567190051e-05
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 2.9802320611338473e-08
Local loss @ local epoch 4: 1.4901159772762185e-07
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.52 seconds!
[tester] 
SST2Metric: acc=0.5091743119266054, hinge=9.734118675966876, ce=8.752467321830059
Local test acc @ epoch 10: 0.5092
Client 9 execute local training on 4 samples...
Local loss @ local epoch 0: 8.642625289212447e-06
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 2.3841855067985307e-07
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.27 seconds!
[tester] 
SST2Metric: acc=0.5091743119266054, hinge=10.36963426082506, ce=9.387982854055702
Local test acc @ epoch 10: 0.5092
Client 0 execute local training on 23 samples...
Local loss @ local epoch 0: 0.010479433462023735
Local loss @ local epoch 1: 0.10360630601644516
Local loss @ local epoch 2: 0.07033158838748932
Local loss @ local epoch 3: 0.01833946444094181
Local loss @ local epoch 4: 0.007618260569870472
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.55 seconds!
[tester] 
SST2Metric: acc=0.4908256880733945, hinge=3.112848950088571, ce=2.1091519512167762
Local test acc @ epoch 10: 0.4908
Client 4 execute local training on 20 samples...
Local loss @ local epoch 0: 0.8187921643257141
Local loss @ local epoch 1: 0.43047305941581726
Local loss @ local epoch 2: 0.18638195097446442
Local loss @ local epoch 3: 0.1127500906586647
Local loss @ local epoch 4: 0.07675954699516296
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.41 seconds!
[tester] 
SST2Metric: acc=0.5240825688073395, hinge=1.8584938787539071, ce=0.9363566550913207
Local test acc @ epoch 10: 0.5241
Client 1 execute local training on 6 samples...
Local loss @ local epoch 0: 0.49794960021972656
Local loss @ local epoch 1: 0.9487690925598145
Local loss @ local epoch 2: 0.6455784440040588
Local loss @ local epoch 3: 0.5778490304946899
Local loss @ local epoch 4: 0.25098463892936707
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.46 seconds!
[tester] 
SST2Metric: acc=0.518348623853211, hinge=1.9750622609339723, ce=1.0355439744547967
Local test acc @ epoch 10: 0.5183
Client 3 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0005577318370342255
Local loss @ local epoch 1: 5.9604641222676946e-08
Local loss @ local epoch 2: 4.4703472212859197e-07
Local loss @ local epoch 3: 1.460312660128693e-06
Local loss @ local epoch 4: 8.046623634072603e-07
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.76 seconds!
[tester] 
SST2Metric: acc=0.5091743119266054, hinge=7.254610665347598, ce=6.2729639882614245
Local test acc @ epoch 10: 0.5092
Client 5 execute local training on 4 samples...
Local loss @ local epoch 0: 0.00012779951794072986
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 2.9802320611338473e-08
Local loss @ local epoch 3: 2.3841853646899835e-07
Local loss @ local epoch 4: 2.3841853646899835e-07
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.6 seconds!
[tester] 
SST2Metric: acc=0.5091743119266054, hinge=8.258776450375898, ce=7.27712566480724
Local test acc @ epoch 10: 0.5092
Client 6 execute local training on 6 samples...
Local loss @ local epoch 0: 1.6195507049560547
Local loss @ local epoch 1: 0.41849231719970703
Local loss @ local epoch 2: 0.3136617839336395
Local loss @ local epoch 3: 0.3665512800216675
Local loss @ local epoch 4: 0.19327397644519806
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.44 seconds!
[tester] 
SST2Metric: acc=0.591743119266055, hinge=1.6801652573390837, ce=0.8379425350405755
Local test acc @ epoch 10: 0.5917
Global evaluate on test data...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.52 seconds!
[tester] 
SST2Metric: acc=0.5091743119266054, hinge=9.722678464487059, ce=8.741027083965616
Global test acc @ epoch 10: 0.5092
Global epoch 11...
Client 5 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.48 seconds!
[tester] 
SST2Metric: acc=0.5091743119266054, hinge=11.166570573771766, ce=10.184919267619422
Local test acc @ epoch 11: 0.5092
Client 0 execute local training on 23 samples...
Local loss @ local epoch 0: 0.007354233413934708
Local loss @ local epoch 1: 0.09306139498949051
Local loss @ local epoch 2: 0.039194561541080475
Local loss @ local epoch 3: 0.01611115224659443
Local loss @ local epoch 4: 0.019632577896118164
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.5 seconds!
[tester] 
SST2Metric: acc=0.49311926605504586, hinge=2.4689476142782683, ce=1.5013194065028375
Local test acc @ epoch 11: 0.4931
Client 3 execute local training on 4 samples...
Local loss @ local epoch 0: 2.9802320611338473e-08
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.68 seconds!
[tester] 
SST2Metric: acc=0.5091743119266054, hinge=10.637548057311172, ce=9.65589672491091
Local test acc @ epoch 11: 0.5092
Client 1 execute local training on 6 samples...
Local loss @ local epoch 0: 2.0527188777923584
Local loss @ local epoch 1: 0.45573362708091736
Local loss @ local epoch 2: 0.5314891934394836
Local loss @ local epoch 3: 0.3188997805118561
Local loss @ local epoch 4: 0.27406996488571167
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.79 seconds!
[tester] 
SST2Metric: acc=0.5045871559633027, hinge=1.996470047793257, ce=0.8299642076732916
Local test acc @ epoch 11: 0.5046
Client 2 execute local training on 5 samples...
Local loss @ local epoch 0: 9.536742595628311e-08
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.6 seconds!
[tester] 
SST2Metric: acc=0.5091743119266054, hinge=9.999220474050679, ce=9.017569097903891
Local test acc @ epoch 11: 0.5092
Client 8 execute local training on 4 samples...
Local loss @ local epoch 0: 1.4901159772762185e-07
Local loss @ local epoch 1: 5.9604641222676946e-08
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.42 seconds!
[tester] 
SST2Metric: acc=0.5091743119266054, hinge=8.460489480867299, ce=7.478839033240572
Local test acc @ epoch 11: 0.5092
Client 7 execute local training on 4 samples...
Local loss @ local epoch 0: 5.9604641222676946e-08
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.47 seconds!
[tester] 
SST2Metric: acc=0.5091743119266054, hinge=11.808175423823366, ce=10.826524095797758
Local test acc @ epoch 11: 0.5092
Client 6 execute local training on 6 samples...
Local loss @ local epoch 0: 2.741217613220215
Local loss @ local epoch 1: 0.5324123501777649
Local loss @ local epoch 2: 0.6113635897636414
Local loss @ local epoch 3: 0.31339624524116516
Local loss @ local epoch 4: 0.24251949787139893
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.38 seconds!
[tester] 
SST2Metric: acc=0.676605504587156, hinge=1.4670949108010038, ce=0.575022533946081
Local test acc @ epoch 11: 0.6766
Client 9 execute local training on 4 samples...
Local loss @ local epoch 0: 1.4901159772762185e-07
Local loss @ local epoch 1: 2.9802320611338473e-08
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.79 seconds!
[tester] 
SST2Metric: acc=0.5091743119266054, hinge=8.8682045980331, ce=7.88655343884175
Local test acc @ epoch 11: 0.5092
Client 4 execute local training on 20 samples...
Local loss @ local epoch 0: 0.3833569884300232
Local loss @ local epoch 1: 0.28413477540016174
Local loss @ local epoch 2: 0.08313123136758804
Local loss @ local epoch 3: 0.04374898225069046
Local loss @ local epoch 4: 0.024589234963059425
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.54 seconds!
[tester] 
SST2Metric: acc=0.5470183486238532, hinge=1.8832014121046854, ce=0.9753122037157006
Local test acc @ epoch 11: 0.547
Global evaluate on test data...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.3 seconds!
[tester] 
SST2Metric: acc=0.7580275229357798, hinge=1.243661823896093, ce=0.5035440273787997
Global test acc @ epoch 11: 0.758
Global epoch 12...
Client 5 execute local training on 4 samples...
Local loss @ local epoch 0: 0.1780611276626587
Local loss @ local epoch 1: 0.00024702466907911
Local loss @ local epoch 2: 0.0001990585878957063
Local loss @ local epoch 3: 0.0001400598557665944
Local loss @ local epoch 4: 0.0003978232853114605
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.58 seconds!
[tester] 
SST2Metric: acc=0.5091743119266054, hinge=4.362904729099449, ce=3.382428616996098
Local test acc @ epoch 12: 0.5092
Client 8 execute local training on 4 samples...
Local loss @ local epoch 0: 0.04819849133491516
Local loss @ local epoch 1: 0.00016822930774651468
Local loss @ local epoch 2: 9.332851914223284e-05
Local loss @ local epoch 3: 5.8973902923753485e-05
Local loss @ local epoch 4: 2.8549529815791175e-05
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.44 seconds!
[tester] 
SST2Metric: acc=0.5091743119266054, hinge=6.349312220144709, ce=5.367682264347548
Local test acc @ epoch 12: 0.5092
Client 3 execute local training on 4 samples...
Local loss @ local epoch 0: 0.010039424523711205
Local loss @ local epoch 1: 2.115941242664121e-05
Local loss @ local epoch 2: 1.710636934149079e-05
Local loss @ local epoch 3: 1.4841429219814017e-05
Local loss @ local epoch 4: 1.075856744137127e-05
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.85 seconds!
[tester] 
SST2Metric: acc=0.5091743119266054, hinge=6.4149186633048805, ce=5.433286197685964
Local test acc @ epoch 12: 0.5092
Client 9 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0026005967520177364
Local loss @ local epoch 1: 1.5169114703894593e-05
Local loss @ local epoch 2: 6.586250037798891e-06
Local loss @ local epoch 3: 2.5629908577684546e-06
Local loss @ local epoch 4: 1.8477401226846268e-06
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.38 seconds!
[tester] 
SST2Metric: acc=0.5091743119266054, hinge=6.997574977918503, ce=6.015931991262036
Local test acc @ epoch 12: 0.5092
Client 1 execute local training on 6 samples...
Local loss @ local epoch 0: 0.6304828524589539
Local loss @ local epoch 1: 0.28351256251335144
Local loss @ local epoch 2: 0.4444807767868042
Local loss @ local epoch 3: 0.08014556765556335
Local loss @ local epoch 4: 0.05604057386517525
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.44 seconds!
[tester] 
SST2Metric: acc=0.5206422018348624, hinge=2.310948768066704, ce=1.400085669933656
Local test acc @ epoch 12: 0.5206
Client 6 execute local training on 6 samples...
Local loss @ local epoch 0: 0.8715131878852844
Local loss @ local epoch 1: 1.2927323579788208
Local loss @ local epoch 2: 0.9344936013221741
Local loss @ local epoch 3: 0.5976365804672241
Local loss @ local epoch 4: 0.26009249687194824
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.61 seconds!
[tester] 
SST2Metric: acc=0.6238532110091743, hinge=1.5627224822656824, ce=0.6494224554081576
Local test acc @ epoch 12: 0.6239
Client 4 execute local training on 20 samples...
Local loss @ local epoch 0: 0.9448898434638977
Local loss @ local epoch 1: 0.18408286571502686
Local loss @ local epoch 2: 0.10228334367275238
Local loss @ local epoch 3: 0.0611351802945137
Local loss @ local epoch 4: 0.02757827751338482
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.42 seconds!
[tester] 
SST2Metric: acc=0.5527522935779816, hinge=1.7695123463048847, ce=0.8914749344827932
Local test acc @ epoch 12: 0.5528
Client 2 execute local training on 5 samples...
Local loss @ local epoch 0: 0.04612814635038376
Local loss @ local epoch 1: 0.00014155941607896239
Local loss @ local epoch 2: 8.12489161035046e-05
Local loss @ local epoch 3: 0.000271580065600574
Local loss @ local epoch 4: 0.00012434963718988
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.31 seconds!
[tester] 
SST2Metric: acc=0.5091743119266054, hinge=5.578976379622013, ce=4.59742062993169
Local test acc @ epoch 12: 0.5092
Client 0 execute local training on 23 samples...
Local loss @ local epoch 0: 0.2835496962070465
Local loss @ local epoch 1: 0.014031118713319302
Local loss @ local epoch 2: 0.017869403585791588
Local loss @ local epoch 3: 0.008594678714871407
Local loss @ local epoch 4: 0.003486499423161149
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.58 seconds!
[tester] 
SST2Metric: acc=0.4954128440366973, hinge=2.4613028980723213, ce=1.499016610580847
Local test acc @ epoch 12: 0.4954
Client 7 execute local training on 4 samples...
Local loss @ local epoch 0: 0.054910965263843536
Local loss @ local epoch 1: 0.00021179785835556686
Local loss @ local epoch 2: 0.00017289482639171183
Local loss @ local epoch 3: 0.00016544459504075348
Local loss @ local epoch 4: 7.569425360998139e-05
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.4 seconds!
[tester] 
SST2Metric: acc=0.5091743119266054, hinge=5.541056613309668, ce=4.559514187039327
Local test acc @ epoch 12: 0.5092
Global evaluate on test data...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.67 seconds!
[tester] 
SST2Metric: acc=0.5091743119266054, hinge=5.147847376832175, ce=4.166756781959329
Global test acc @ epoch 12: 0.5092
Global epoch 13...
Client 9 execute local training on 4 samples...
Local loss @ local epoch 0: 8.493620043736883e-06
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 5.960463766996327e-08
Local loss @ local epoch 3: 1.4901159772762185e-07
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.47 seconds!
[tester] 
SST2Metric: acc=0.5091743119266054, hinge=9.866900933991879, ce=8.88524965408745
Local test acc @ epoch 13: 0.5092
Client 6 execute local training on 6 samples...
Local loss @ local epoch 0: 1.7302294969558716
Local loss @ local epoch 1: 0.5012465715408325
Local loss @ local epoch 2: 1.6669350862503052
Local loss @ local epoch 3: 0.896725594997406
Local loss @ local epoch 4: 0.6552459597587585
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.32 seconds!
[tester] 
SST2Metric: acc=0.5091743119266054, hinge=2.130618087468891, ce=1.1080756611233458
Local test acc @ epoch 13: 0.5092
Client 8 execute local training on 4 samples...
Local loss @ local epoch 0: 1.478173908253666e-05
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 8.940696005765858e-08
Local loss @ local epoch 4: 5.9604641222676946e-08
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.53 seconds!
[tester] 
SST2Metric: acc=0.5091743119266054, hinge=8.639924799630402, ce=7.658273786989921
Local test acc @ epoch 13: 0.5092
Client 5 execute local training on 4 samples...
Local loss @ local epoch 0: 4.202116997475969e-06
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 2.3841855067985307e-07
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.54 seconds!
[tester] 
SST2Metric: acc=0.5091743119266054, hinge=10.338675997672825, ce=9.357024639024647
Local test acc @ epoch 13: 0.5092
Client 2 execute local training on 5 samples...
Local loss @ local epoch 0: 7.176361577876378e-06
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 2.384185648907078e-08
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 4.768371297814156e-08
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.5 seconds!
[tester] 
SST2Metric: acc=0.5091743119266054, hinge=8.37150891986462, ce=7.389858330335092
Local test acc @ epoch 13: 0.5092
Client 1 execute local training on 6 samples...
Local loss @ local epoch 0: 0.8431062698364258
Local loss @ local epoch 1: 0.06695245951414108
Local loss @ local epoch 2: 0.012238179333508015
Local loss @ local epoch 3: 0.00519010191783309
Local loss @ local epoch 4: 0.0064425356686115265
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.37 seconds!
[tester] 
SST2Metric: acc=0.5298165137614679, hinge=2.652357842944084, ce=1.734815762506439
Local test acc @ epoch 13: 0.5298
Client 7 execute local training on 4 samples...
Local loss @ local epoch 0: 9.536677680443972e-06
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 1.7881390590446244e-07
Local loss @ local epoch 3: 5.9604641222676946e-08
Local loss @ local epoch 4: 2.9802320611338473e-08
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.31 seconds!
[tester] 
SST2Metric: acc=0.5091743119266054, hinge=9.639061713437421, ce=8.657410389923175
Local test acc @ epoch 13: 0.5092
Client 0 execute local training on 23 samples...
Local loss @ local epoch 0: 0.03687073662877083
Local loss @ local epoch 1: 0.022563114762306213
Local loss @ local epoch 2: 0.027724694460630417
Local loss @ local epoch 3: 0.02339537814259529
Local loss @ local epoch 4: 0.010516316629946232
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.78 seconds!
[tester] 
SST2Metric: acc=0.5022935779816514, hinge=2.2668001449436224, ce=1.3279530538604893
Local test acc @ epoch 13: 0.5023
Client 3 execute local training on 4 samples...
Local loss @ local epoch 0: 7.345892663579434e-05
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 1.7881392011531716e-07
Local loss @ local epoch 3: 3.576278118089249e-07
Local loss @ local epoch 4: 1.7881392011531716e-07
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.74 seconds!
[tester] 
SST2Metric: acc=0.5091743119266054, hinge=8.152281638679154, ce=7.170631323932508
Local test acc @ epoch 13: 0.5092
Client 4 execute local training on 20 samples...
Local loss @ local epoch 0: 0.7483376264572144
Local loss @ local epoch 1: 0.06543119996786118
Local loss @ local epoch 2: 0.049267541617155075
Local loss @ local epoch 3: 0.06994909048080444
Local loss @ local epoch 4: 0.022860316559672356
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.58 seconds!
[tester] 
SST2Metric: acc=0.5573394495412844, hinge=1.9485732910283116, ce=1.066161419957056
Local test acc @ epoch 13: 0.5573
Global evaluate on test data...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.61 seconds!
[tester] 
SST2Metric: acc=0.5091743119266054, hinge=3.7896665126905527, ce=2.815845289350372
Global test acc @ epoch 13: 0.5092
Global epoch 14...
Client 3 execute local training on 4 samples...
Local loss @ local epoch 0: 1.6391256849601632e-06
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 2.9802320611338473e-08
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.68 seconds!
[tester] 
SST2Metric: acc=0.5091743119266054, hinge=9.624362991490495, ce=8.642711604407074
Local test acc @ epoch 14: 0.5092
Client 0 execute local training on 23 samples...
Local loss @ local epoch 0: 0.19622065126895905
Local loss @ local epoch 1: 0.06367316097021103
Local loss @ local epoch 2: 0.0036888376343995333
Local loss @ local epoch 3: 0.0032514792401343584
Local loss @ local epoch 4: 0.009973002597689629
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.47 seconds!
[tester] 
SST2Metric: acc=0.49655963302752293, hinge=2.539695263455767, ce=1.576313836722199
Local test acc @ epoch 14: 0.4966
Client 4 execute local training on 20 samples...
Local loss @ local epoch 0: 0.1074717715382576
Local loss @ local epoch 1: 0.34812504053115845
Local loss @ local epoch 2: 0.06480592489242554
Local loss @ local epoch 3: 0.041629157960414886
Local loss @ local epoch 4: 0.037382401525974274
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.75 seconds!
[tester] 
SST2Metric: acc=0.5596330275229358, hinge=1.9344929844961254, ce=1.0619394664370685
Local test acc @ epoch 14: 0.5596
Client 8 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0003586081729736179
Local loss @ local epoch 1: 3.8742987840123533e-07
Local loss @ local epoch 2: 1.1414077562221792e-05
Local loss @ local epoch 3: 5.960459361631365e-07
Local loss @ local epoch 4: 1.7881387748275301e-07
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.63 seconds!
[tester] 
SST2Metric: acc=0.5091743119266054, hinge=8.580120895980695, ce=7.598469702202245
Local test acc @ epoch 14: 0.5092
Client 1 execute local training on 6 samples...
Local loss @ local epoch 0: 0.0663459524512291
Local loss @ local epoch 1: 2.06469988822937
Local loss @ local epoch 2: 0.12422525137662888
Local loss @ local epoch 3: 0.12965978682041168
Local loss @ local epoch 4: 0.046024005860090256
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.31 seconds!
[tester] 
SST2Metric: acc=0.5126146788990825, hinge=2.404672590964431, ce=1.4745988138020039
Local test acc @ epoch 14: 0.5126
Client 2 execute local training on 5 samples...
Local loss @ local epoch 0: 0.00038251272053457797
Local loss @ local epoch 1: 7.152556946721234e-08
Local loss @ local epoch 2: 7.748568350507412e-06
Local loss @ local epoch 3: 9.298319127992727e-07
Local loss @ local epoch 4: 1.6212445643759565e-06
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.37 seconds!
[tester] 
SST2Metric: acc=0.5091743119266054, hinge=7.574157487361803, ce=6.5925086796830445
Local test acc @ epoch 14: 0.5092
Client 5 execute local training on 4 samples...
Local loss @ local epoch 0: 1.7344404113828205e-05
Local loss @ local epoch 1: 1.4901159772762185e-07
Local loss @ local epoch 2: 1.4305103377409978e-06
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.45 seconds!
[tester] 
SST2Metric: acc=0.5091743119266054, hinge=8.060293357306664, ce=7.078643665401212
Local test acc @ epoch 14: 0.5092
Client 9 execute local training on 4 samples...
Local loss @ local epoch 0: 2.3245334887178615e-05
Local loss @ local epoch 1: 2.9802320611338473e-08
Local loss @ local epoch 2: 1.0281602953909896e-05
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.54 seconds!
[tester] 
SST2Metric: acc=0.5091743119266054, hinge=8.82801475656142, ce=7.84636340775621
Local test acc @ epoch 14: 0.5092
Client 7 execute local training on 4 samples...
Local loss @ local epoch 0: 6.228642178029986e-06
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 2.0861622829215776e-07
Local loss @ local epoch 3: 1.1920928244535389e-07
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.52 seconds!
[tester] 
SST2Metric: acc=0.5091743119266054, hinge=10.154619118489256, ce=9.172967724843856
Local test acc @ epoch 14: 0.5092
Client 6 execute local training on 6 samples...
Local loss @ local epoch 0: 1.1852244138717651
Local loss @ local epoch 1: 0.723613440990448
Local loss @ local epoch 2: 1.144606113433838
Local loss @ local epoch 3: 0.6444647908210754
Local loss @ local epoch 4: 0.4623788893222809
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.38 seconds!
[tester] 
SST2Metric: acc=0.5114678899082569, hinge=2.1928012157798906, ce=1.27455725274775
Local test acc @ epoch 14: 0.5115
Global evaluate on test data...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.43 seconds!
[tester] 
SST2Metric: acc=0.5091743119266054, hinge=7.352361980928194, ce=6.370815749972238
Global test acc @ epoch 14: 0.5092
Global epoch 15...
Client 2 execute local training on 5 samples...
Local loss @ local epoch 0: 7.152556946721234e-08
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.45 seconds!
[tester] 
SST2Metric: acc=0.5091743119266054, hinge=10.465537591811714, ce=9.48388634690451
Local test acc @ epoch 15: 0.5092
Client 3 execute local training on 4 samples...
Local loss @ local epoch 0: 7.15254714123148e-07
Local loss @ local epoch 1: 1.4901144140821998e-06
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.49 seconds!
[tester] 
SST2Metric: acc=0.5091743119266054, hinge=9.108558637286546, ce=8.12690742956389
Local test acc @ epoch 15: 0.5092
Client 1 execute local training on 6 samples...
Local loss @ local epoch 0: 2.2996985912323
Local loss @ local epoch 1: 0.05124219134449959
Local loss @ local epoch 2: 0.03502670302987099
Local loss @ local epoch 3: 0.01655169390141964
Local loss @ local epoch 4: 0.012375767342746258
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.33 seconds!
[tester] 
SST2Metric: acc=0.5447247706422018, hinge=2.246327376146929, ce=1.3602342637888063
Local test acc @ epoch 15: 0.5447
Client 4 execute local training on 20 samples...
Local loss @ local epoch 0: 0.36213335394859314
Local loss @ local epoch 1: 0.025396915152668953
Local loss @ local epoch 2: 0.045165397226810455
Local loss @ local epoch 3: 0.02026543766260147
Local loss @ local epoch 4: 0.00855369120836258
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.34 seconds!
[tester] 
SST2Metric: acc=0.6869266055045872, hinge=1.438001837888989, ce=0.7405897814336173
Local test acc @ epoch 15: 0.6869
Client 9 execute local training on 4 samples...
Local loss @ local epoch 0: 1.0132781653737766e-06
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 1.4901159772762185e-07
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.39 seconds!
[tester] 
SST2Metric: acc=0.5091743119266054, hinge=12.079477108946634, ce=11.09782571092658
Local test acc @ epoch 15: 0.5092
Client 7 execute local training on 4 samples...
Local loss @ local epoch 0: 2.712001332838554e-06
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 1.4901159772762185e-07
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 1.4901158351676713e-07
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.37 seconds!
[tester] 
SST2Metric: acc=0.5091743119266054, hinge=12.217630360104621, ce=11.235979036453667
Local test acc @ epoch 15: 0.5092
Client 5 execute local training on 4 samples...
Local loss @ local epoch 0: 2.0861622829215776e-07
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.39 seconds!
[tester] 
SST2Metric: acc=0.5091743119266054, hinge=7.600215733598132, ce=6.618589131930553
Local test acc @ epoch 15: 0.5092
Client 0 execute local training on 23 samples...
Local loss @ local epoch 0: 0.006660849321633577
Local loss @ local epoch 1: 0.015021969564259052
Local loss @ local epoch 2: 0.014143742620944977
Local loss @ local epoch 3: 0.005234622862190008
Local loss @ local epoch 4: 0.003021393669769168
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.57 seconds!
[tester] 
SST2Metric: acc=0.5034403669724771, hinge=2.5117221354344568, ce=1.5569052813796822
Local test acc @ epoch 15: 0.5034
Client 8 execute local training on 4 samples...
Local loss @ local epoch 0: 1.5437250112881884e-05
Local loss @ local epoch 1: 5.960463766996327e-08
Local loss @ local epoch 2: 4.529916168394266e-06
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.67 seconds!
[tester] 
SST2Metric: acc=0.5091743119266054, hinge=11.700452336477577, ce=10.718800894710995
Local test acc @ epoch 15: 0.5092
Client 6 execute local training on 6 samples...
Local loss @ local epoch 0: 2.0032496452331543
Local loss @ local epoch 1: 0.35155341029167175
Local loss @ local epoch 2: 0.32665979862213135
Local loss @ local epoch 3: 0.20650474727153778
Local loss @ local epoch 4: 0.06053650379180908
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.6 seconds!
[tester] 
SST2Metric: acc=0.7488532110091743, hinge=1.1699818766445196, ce=0.527935824946526
Local test acc @ epoch 15: 0.7489
Global evaluate on test data...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.56 seconds!
[tester] 
SST2Metric: acc=0.6307339449541285, hinge=1.7267742761231344, ce=0.9587623461132544
Global test acc @ epoch 15: 0.6307
Global epoch 16...
Client 7 execute local training on 4 samples...
Local loss @ local epoch 0: 5.909279570914805e-05
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 2.6822084464583895e-07
Local loss @ local epoch 3: 1.7881389169360773e-07
Local loss @ local epoch 4: 8.940696005765858e-08
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.34 seconds!
[tester] 
SST2Metric: acc=0.5091743119266054, hinge=7.438676157128921, ce=6.457041117576282
Local test acc @ epoch 16: 0.5092
Client 0 execute local training on 23 samples...
Local loss @ local epoch 0: 0.10760575532913208
Local loss @ local epoch 1: 1.2802209854125977
Local loss @ local epoch 2: 0.009569359011948109
Local loss @ local epoch 3: 0.004694607108831406
Local loss @ local epoch 4: 0.002104520797729492
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.38 seconds!
[tester] 
SST2Metric: acc=0.4954128440366973, hinge=2.5960305207366243, ce=1.6327382536656265
Local test acc @ epoch 16: 0.4954
Client 5 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0014995245728641748
Local loss @ local epoch 1: 4.1723239974089665e-07
Local loss @ local epoch 2: 1.3709047834709054e-06
Local loss @ local epoch 3: 5.4234904382610694e-05
Local loss @ local epoch 4: 8.94068989509833e-07
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.25 seconds!
[tester] 
SST2Metric: acc=0.5091743119266054, hinge=6.3176844229391955, ce=5.336084792509893
Local test acc @ epoch 16: 0.5092
Client 9 execute local training on 4 samples...
Local loss @ local epoch 0: 0.011947154998779297
Local loss @ local epoch 1: 5.662438979925355e-07
Local loss @ local epoch 2: 2.1755652142019244e-06
Local loss @ local epoch 3: 1.084793075278867e-05
Local loss @ local epoch 4: 1.3679034054803196e-05
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.22 seconds!
[tester] 
SST2Metric: acc=0.5091743119266054, hinge=5.5084708765012405, ce=4.527343675355734
Local test acc @ epoch 16: 0.5092
Client 6 execute local training on 6 samples...
Local loss @ local epoch 0: 0.16480755805969238
Local loss @ local epoch 1: 0.7141569256782532
Local loss @ local epoch 2: 0.12960690259933472
Local loss @ local epoch 3: 0.12822024524211884
Local loss @ local epoch 4: 0.023375334218144417
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.48 seconds!
[tester] 
SST2Metric: acc=0.6548165137614679, hinge=1.5115082053416367, ce=0.8179765314444762
Local test acc @ epoch 16: 0.6548
Client 2 execute local training on 5 samples...
Local loss @ local epoch 0: 0.0023673176765441895
Local loss @ local epoch 1: 2.384185648907078e-08
Local loss @ local epoch 2: 1.4305112472356996e-07
Local loss @ local epoch 3: 6.413424671336543e-06
Local loss @ local epoch 4: 1.137246272264747e-05
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.99 seconds!
[tester] 
SST2Metric: acc=0.5091743119266054, hinge=5.500113744254506, ce=4.518777966359316
Local test acc @ epoch 16: 0.5092
Client 8 execute local training on 4 samples...
Local loss @ local epoch 0: 1.4096209270064719e-05
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 2.9802320611338473e-08
Local loss @ local epoch 3: 4.4703452317662595e-07
Local loss @ local epoch 4: 2.9802320611338473e-08
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.51 seconds!
[tester] 
SST2Metric: acc=0.5091743119266054, hinge=7.417204865621864, ce=6.435585240021758
Local test acc @ epoch 16: 0.5092
Client 3 execute local training on 4 samples...
Local loss @ local epoch 0: 2.473588665452553e-06
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.44 seconds!
[tester] 
SST2Metric: acc=0.5091743119266054, hinge=7.31576767099013, ce=6.3341434947941275
Local test acc @ epoch 16: 0.5092
Client 4 execute local training on 20 samples...
Local loss @ local epoch 0: 0.1565229892730713
Local loss @ local epoch 1: 0.01353421900421381
Local loss @ local epoch 2: 0.12549030780792236
Local loss @ local epoch 3: 0.01886906288564205
Local loss @ local epoch 4: 0.012222079560160637
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.28 seconds!
[tester] 
SST2Metric: acc=0.5837155963302753, hinge=1.842865060502236, ce=1.016512655747046
Local test acc @ epoch 16: 0.5837
Client 1 execute local training on 6 samples...
Local loss @ local epoch 0: 0.03653370961546898
Local loss @ local epoch 1: 1.223732590675354
Local loss @ local epoch 2: 0.034250058233737946
Local loss @ local epoch 3: 0.12214788794517517
Local loss @ local epoch 4: 0.05876888334751129
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.42 seconds!
[tester] 
SST2Metric: acc=0.7591743119266054, hinge=1.1496915653211262, ce=0.5912139610891495
Local test acc @ epoch 16: 0.7592
Global evaluate on test data...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.52 seconds!
[tester] 
SST2Metric: acc=0.569954128440367, hinge=3.061015442423864, ce=2.1976811271675163
Global test acc @ epoch 16: 0.57
Global epoch 17...
Client 3 execute local training on 4 samples...
Local loss @ local epoch 0: 1.1235271813347936e-05
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 1.1920928244535389e-07
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.47 seconds!
[tester] 
SST2Metric: acc=0.5091743119266054, hinge=7.820476826177824, ce=6.838827857047046
Local test acc @ epoch 17: 0.5092
Client 1 execute local training on 6 samples...
Local loss @ local epoch 0: 0.04721358045935631
Local loss @ local epoch 1: 0.374565988779068
Local loss @ local epoch 2: 0.33826038241386414
Local loss @ local epoch 3: 0.21983595192432404
Local loss @ local epoch 4: 0.14525696635246277
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.31 seconds!
[tester] 
SST2Metric: acc=0.5149082568807339, hinge=2.0564209128465127, ce=1.1516885637826877
Local test acc @ epoch 17: 0.5149
Client 0 execute local training on 23 samples...
Local loss @ local epoch 0: 0.008995304815471172
Local loss @ local epoch 1: 0.011305541731417179
Local loss @ local epoch 2: 0.026483068242669106
Local loss @ local epoch 3: 0.004874960519373417
Local loss @ local epoch 4: 0.0027990846429020166
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.44 seconds!
[tester] 
SST2Metric: acc=0.4908256880733945, hinge=3.6266035957073948, ce=2.61820745509152
Local test acc @ epoch 17: 0.4908
Client 8 execute local training on 4 samples...
Local loss @ local epoch 0: 3.346683661220595e-05
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 5.9604641222676946e-08
Local loss @ local epoch 3: 2.0861622829215776e-07
Local loss @ local epoch 4: 2.0861622829215776e-07
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.68 seconds!
[tester] 
SST2Metric: acc=0.5091743119266054, hinge=7.982536389193403, ce=7.0008906452754225
Local test acc @ epoch 17: 0.5092
Client 4 execute local training on 20 samples...
Local loss @ local epoch 0: 0.03580932319164276
Local loss @ local epoch 1: 0.013182533904910088
Local loss @ local epoch 2: 0.011727801524102688
Local loss @ local epoch 3: 0.008403368294239044
Local loss @ local epoch 4: 0.0053197843953967094
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.36 seconds!
[tester] 
SST2Metric: acc=0.5642201834862385, hinge=2.1146160733262334, ce=1.2522914178942868
Local test acc @ epoch 17: 0.5642
Client 6 execute local training on 6 samples...
Local loss @ local epoch 0: 1.2305303812026978
Local loss @ local epoch 1: 1.0477086305618286
Local loss @ local epoch 2: 0.12862294912338257
Local loss @ local epoch 3: 0.1369207352399826
Local loss @ local epoch 4: 0.08370370417833328
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.42 seconds!
[tester] 
SST2Metric: acc=0.731651376146789, hinge=1.2450759834652647, ce=0.5114777870134476
Local test acc @ epoch 17: 0.7317
Client 5 execute local training on 4 samples...
Local loss @ local epoch 0: 3.650664439192042e-05
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 1.1920928244535389e-07
Local loss @ local epoch 4: 4.172324565843155e-07
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.39 seconds!
[tester] 
SST2Metric: acc=0.5091743119266054, hinge=6.813026804442799, ce=5.831540070542492
Local test acc @ epoch 17: 0.5092
Client 7 execute local training on 4 samples...
Local loss @ local epoch 0: 2.1665971871698275e-05
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 8.940696005765858e-08
Local loss @ local epoch 4: 2.0861622829215776e-07
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.47 seconds!
[tester] 
SST2Metric: acc=0.5091743119266054, hinge=7.602969276795694, ce=6.621320872815373
Local test acc @ epoch 17: 0.5092
Client 9 execute local training on 4 samples...
Local loss @ local epoch 0: 1.627188794373069e-05
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 2.9802320611338473e-08
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.49 seconds!
[tester] 
SST2Metric: acc=0.5091743119266054, hinge=8.182678417328301, ce=7.201028657342316
Local test acc @ epoch 17: 0.5092
Client 2 execute local training on 5 samples...
Local loss @ local epoch 0: 1.6498102922923863e-05
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 4.768371297814156e-08
Local loss @ local epoch 3: 2.622603858526418e-07
Local loss @ local epoch 4: 1.668929883180681e-07
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.4 seconds!
[tester] 
SST2Metric: acc=0.5091743119266054, hinge=8.842653075489428, ce=7.861001902900704
Local test acc @ epoch 17: 0.5092
Global evaluate on test data...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.39 seconds!
[tester] 
SST2Metric: acc=0.5206422018348624, hinge=4.070224017725078, ce=3.125254153521741
Global test acc @ epoch 17: 0.5206
Global epoch 18...
Client 8 execute local training on 4 samples...
Local loss @ local epoch 0: 2.9802320611338473e-08
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.68 seconds!
[tester] 
SST2Metric: acc=0.5091743119266054, hinge=9.680896942768621, ce=8.699294486177077
Local test acc @ epoch 18: 0.5092
Client 0 execute local training on 23 samples...
Local loss @ local epoch 0: 0.007317162584513426
Local loss @ local epoch 1: 0.005243040155619383
Local loss @ local epoch 2: 0.00912553258240223
Local loss @ local epoch 3: 0.0056050485000014305
Local loss @ local epoch 4: 0.009805692359805107
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.41 seconds!
[tester] 
SST2Metric: acc=0.5080275229357798, hinge=2.562887764851981, ce=1.6167032686873861
Local test acc @ epoch 18: 0.508
Client 7 execute local training on 4 samples...
Local loss @ local epoch 0: 2.0861619987044833e-07
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.41 seconds!
[tester] 
SST2Metric: acc=0.5091743119266054, hinge=8.236972154827292, ce=7.255418982117548
Local test acc @ epoch 18: 0.5092
Client 5 execute local training on 4 samples...
Local loss @ local epoch 0: 2.9802320611338473e-08
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.55 seconds!
[tester] 
SST2Metric: acc=0.5091743119266054, hinge=8.762674570083618, ce=7.781101052914191
Local test acc @ epoch 18: 0.5092
Client 9 execute local training on 4 samples...
Local loss @ local epoch 0: 8.940696005765858e-08
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.21 seconds!
[tester] 
SST2Metric: acc=0.5091743119266054, hinge=10.345208474255482, ce=9.364098279848012
Local test acc @ epoch 18: 0.5092
Client 4 execute local training on 20 samples...
Local loss @ local epoch 0: 0.10244617611169815
Local loss @ local epoch 1: 0.013283729553222656
Local loss @ local epoch 2: 0.006641831248998642
Local loss @ local epoch 3: 0.008958357386291027
Local loss @ local epoch 4: 0.005708186887204647
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.45 seconds!
[tester] 
SST2Metric: acc=0.5504587155963303, hinge=2.269359199552361, ce=1.3761951585018306
Local test acc @ epoch 18: 0.5505
Client 6 execute local training on 6 samples...
Local loss @ local epoch 0: 0.38255491852760315
Local loss @ local epoch 1: 2.116380453109741
Local loss @ local epoch 2: 0.03664570674300194
Local loss @ local epoch 3: 0.03430582210421562
Local loss @ local epoch 4: 0.04034049063920975
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.4 seconds!
[tester] 
SST2Metric: acc=0.8188073394495413, hinge=0.922818506256156, ce=0.4277491224160708
Local test acc @ epoch 18: 0.8188
Client 3 execute local training on 4 samples...
Local loss @ local epoch 0: 4.1723228605405893e-07
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.56 seconds!
[tester] 
SST2Metric: acc=0.5091743119266054, hinge=8.208684899391384, ce=7.227040417970867
Local test acc @ epoch 18: 0.5092
Client 2 execute local training on 5 samples...
Local loss @ local epoch 0: 2.1219202608335763e-06
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 7.152556946721234e-08
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.43 seconds!
[tester] 
SST2Metric: acc=0.5091743119266054, hinge=9.205491527504877, ce=8.223840905985702
Local test acc @ epoch 18: 0.5092
Client 1 execute local training on 6 samples...
Local loss @ local epoch 0: 0.0003822016587946564
Local loss @ local epoch 1: 2.264357566833496
Local loss @ local epoch 2: 0.1543601006269455
Local loss @ local epoch 3: 0.017369216307997704
Local loss @ local epoch 4: 0.006422423291951418
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.32 seconds!
[tester] 
SST2Metric: acc=0.5412844036697247, hinge=2.591259960734516, ce=1.6895060868198992
Local test acc @ epoch 18: 0.5413
Global evaluate on test data...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.52 seconds!
[tester] 
SST2Metric: acc=0.8325688073394495, hinge=0.8305273769645516, ce=0.47939573610286923
Global test acc @ epoch 18: 0.8326
Global epoch 19...
Client 6 execute local training on 6 samples...
Local loss @ local epoch 0: 0.031251195818185806
Local loss @ local epoch 1: 0.7145564556121826
Local loss @ local epoch 2: 0.3625061810016632
Local loss @ local epoch 3: 0.625889778137207
Local loss @ local epoch 4: 0.6412429213523865
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.42 seconds!
[tester] 
SST2Metric: acc=0.5172018348623854, hinge=1.9275286558571212, ce=0.787035777481324
Local test acc @ epoch 19: 0.5172
Client 1 execute local training on 6 samples...
Local loss @ local epoch 0: 0.0166389849036932
Local loss @ local epoch 1: 1.2836169004440308
Local loss @ local epoch 2: 0.032801996916532516
Local loss @ local epoch 3: 0.026314375922083855
Local loss @ local epoch 4: 0.010427821427583694
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.65 seconds!
[tester] 
SST2Metric: acc=0.6341743119266054, hinge=1.795941404246409, ce=1.0507830042699609
Local test acc @ epoch 19: 0.6342
Client 7 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0025737558025866747
Local loss @ local epoch 1: 1.4901158351676713e-07
Local loss @ local epoch 2: 2.920622591773281e-06
Local loss @ local epoch 3: 2.4377977752010338e-05
Local loss @ local epoch 4: 1.8417646060697734e-05
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.54 seconds!
[tester] 
SST2Metric: acc=0.5091743119266054, hinge=5.712932717909506, ce=4.731369777143676
Local test acc @ epoch 19: 0.5092
Client 2 execute local training on 5 samples...
Local loss @ local epoch 0: 0.008596266619861126
Local loss @ local epoch 1: 5.030593001720263e-06
Local loss @ local epoch 2: 4.5057717215968296e-05
Local loss @ local epoch 3: 6.648902490269393e-05
Local loss @ local epoch 4: 1.8191043636761606e-05
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.37 seconds!
[tester] 
SST2Metric: acc=0.5091743119266054, hinge=4.722988066323307, ce=3.743632704969059
Local test acc @ epoch 19: 0.5092
Client 5 execute local training on 4 samples...
Local loss @ local epoch 0: 8.424358384218067e-05
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 2.9802320611338473e-08
Local loss @ local epoch 3: 1.1920928244535389e-07
Local loss @ local epoch 4: 3.278254894212296e-07
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.38 seconds!
[tester] 
SST2Metric: acc=0.5091743119266054, hinge=6.502962798153589, ce=5.521407632652736
Local test acc @ epoch 19: 0.5092
Client 4 execute local training on 20 samples...
Local loss @ local epoch 0: 0.011190086603164673
Local loss @ local epoch 1: 0.002194613916799426
Local loss @ local epoch 2: 0.011522497981786728
Local loss @ local epoch 3: 0.0068275174126029015
Local loss @ local epoch 4: 0.02210991457104683
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.37 seconds!
[tester] 
SST2Metric: acc=0.555045871559633, hinge=2.378879458395713, ce=1.4946774228928832
Local test acc @ epoch 19: 0.555
Client 0 execute local training on 23 samples...
Local loss @ local epoch 0: 0.1709698736667633
Local loss @ local epoch 1: 0.004347033333033323
Local loss @ local epoch 2: 0.0003538619203027338
Local loss @ local epoch 3: 0.0003384226292837411
Local loss @ local epoch 4: 0.0005350722931325436
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.23 seconds!
[tester] 
SST2Metric: acc=0.6204128440366973, hinge=2.0517552730140336, ce=1.282988770095922
Local test acc @ epoch 19: 0.6204
Client 9 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0008944862638600171
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 1.4901159772762185e-07
Local loss @ local epoch 3: 8.642666671221377e-07
Local loss @ local epoch 4: 2.741804564720951e-06
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.36 seconds!
[tester] 
SST2Metric: acc=0.5091743119266054, hinge=6.451451795910477, ce=5.469853976724303
Local test acc @ epoch 19: 0.5092
Client 3 execute local training on 4 samples...
Local loss @ local epoch 0: 0.00022427552903536707
Local loss @ local epoch 1: 2.9802320611338473e-08
Local loss @ local epoch 2: 5.9604641222676946e-08
Local loss @ local epoch 3: 8.046623634072603e-07
Local loss @ local epoch 4: 5.6922162912087515e-06
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.35 seconds!
[tester] 
SST2Metric: acc=0.5080275229357798, hinge=6.225893781819475, ce=5.243209467568481
Local test acc @ epoch 19: 0.508
Client 8 execute local training on 4 samples...
Local loss @ local epoch 0: 0.00011698604794219136
Local loss @ local epoch 1: 2.9802313861182483e-07
Local loss @ local epoch 2: 2.354376874791342e-06
Local loss @ local epoch 3: 2.9802316703353426e-07
Local loss @ local epoch 4: 5.066392532171449e-07
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.45 seconds!
[tester] 
SST2Metric: acc=0.5091743119266054, hinge=6.347424384650834, ce=5.366164333093055
Local test acc @ epoch 19: 0.5092
Global evaluate on test data...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.69 seconds!
[tester] 
SST2Metric: acc=0.6961009174311926, hinge=2.492302174961895, ce=1.8618240035505076
Global test acc @ epoch 19: 0.6961
Global epoch 20...
Client 3 execute local training on 4 samples...
Local loss @ local epoch 0: 1.0728825827754918e-06
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.74 seconds!
[tester] 
SST2Metric: acc=0.5091743119266054, hinge=8.168476620945361, ce=7.186845070861895
Local test acc @ epoch 20: 0.5092
Client 4 execute local training on 20 samples...
Local loss @ local epoch 0: 0.05369219183921814
Local loss @ local epoch 1: 0.03723732754588127
Local loss @ local epoch 2: 0.02454247698187828
Local loss @ local epoch 3: 0.004205321427434683
Local loss @ local epoch 4: 0.003575414652004838
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.57 seconds!
[tester] 
SST2Metric: acc=0.6146788990825688, hinge=1.8903308286579377, ce=1.103598261234957
Local test acc @ epoch 20: 0.6147
Client 7 execute local training on 4 samples...
Local loss @ local epoch 0: 1.7225174815393984e-05
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 2.6822084464583895e-07
Local loss @ local epoch 3: 8.940696005765858e-08
Local loss @ local epoch 4: 2.9802320611338473e-08
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.72 seconds!
[tester] 
SST2Metric: acc=0.5091743119266054, hinge=6.6325006670908095, ce=5.650941601586998
Local test acc @ epoch 20: 0.5092
Client 1 execute local training on 6 samples...
Local loss @ local epoch 0: 0.00011368863488314673
Local loss @ local epoch 1: 0.10531985759735107
Local loss @ local epoch 2: 0.774599552154541
Local loss @ local epoch 3: 0.30841371417045593
Local loss @ local epoch 4: 0.1906844973564148
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.29 seconds!
[tester] 
SST2Metric: acc=0.5091743119266054, hinge=2.327383453692865, ce=1.401278879561829
Local test acc @ epoch 20: 0.5092
Client 2 execute local training on 5 samples...
Local loss @ local epoch 0: 6.151130037324037e-06
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.51 seconds!
[tester] 
SST2Metric: acc=0.5091743119266054, hinge=7.208185696820601, ce=6.228081441800529
Local test acc @ epoch 20: 0.5092
Client 0 execute local training on 23 samples...
Local loss @ local epoch 0: 0.0023562603164464235
Local loss @ local epoch 1: 0.00953151285648346
Local loss @ local epoch 2: 0.001332394895143807
Local loss @ local epoch 3: 0.0001473610318498686
Local loss @ local epoch 4: 0.00013171393948141485
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.48 seconds!
[tester] 
SST2Metric: acc=0.5240825688073395, hinge=3.2187841283072025, ce=2.279456051391199
Local test acc @ epoch 20: 0.5241
Client 6 execute local training on 6 samples...
Local loss @ local epoch 0: 0.00024273466260638088
Local loss @ local epoch 1: 1.9838734865188599
Local loss @ local epoch 2: 0.0004951059236191213
Local loss @ local epoch 3: 0.0022001133766025305
Local loss @ local epoch 4: 0.014003515243530273
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.32 seconds!
[tester] 
SST2Metric: acc=0.8119266055045872, hinge=0.9103349284567964, ce=0.4842700161415776
Local test acc @ epoch 20: 0.8119
Client 5 execute local training on 4 samples...
Local loss @ local epoch 0: 1.1026852462237002e-06
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.35 seconds!
[tester] 
SST2Metric: acc=0.5126146788990825, hinge=5.935882404309894, ce=4.96510863837299
Local test acc @ epoch 20: 0.5126
Client 9 execute local training on 4 samples...
Local loss @ local epoch 0: 3.1888394005363807e-06
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.34 seconds!
[tester] 
SST2Metric: acc=0.5091743119266054, hinge=7.596817188306686, ce=6.615406014230269
Local test acc @ epoch 20: 0.5092
Client 8 execute local training on 4 samples...
Local loss @ local epoch 0: 9.238714255843661e-07
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.5 seconds!
[tester] 
SST2Metric: acc=0.5103211009174312, hinge=6.8300138908788695, ce=5.854110248181798
Local test acc @ epoch 20: 0.5103
Global evaluate on test data...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.57 seconds!
[tester] 
SST2Metric: acc=0.6295871559633027, hinge=1.851567604673018, ce=1.1041847458306846
Global test acc @ epoch 20: 0.6296
Global epoch 21...
Client 3 execute local training on 4 samples...
Local loss @ local epoch 0: 2.9802320611338473e-08
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.48 seconds!
[tester] 
SST2Metric: acc=0.5091743119266054, hinge=8.005254351764645, ce=7.0236204862594604
Local test acc @ epoch 21: 0.5092
Client 4 execute local training on 20 samples...
Local loss @ local epoch 0: 0.005938821006566286
Local loss @ local epoch 1: 0.01819382607936859
Local loss @ local epoch 2: 0.005061183590441942
Local loss @ local epoch 3: 0.004264417104423046
Local loss @ local epoch 4: 0.003921289928257465
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.41 seconds!
[tester] 
SST2Metric: acc=0.6307339449541285, hinge=1.884375330505021, ce=1.1249212818476586
Local test acc @ epoch 21: 0.6307
Client 1 execute local training on 6 samples...
Local loss @ local epoch 0: 0.0015873530646786094
Local loss @ local epoch 1: 2.857065200805664
Local loss @ local epoch 2: 0.025548294186592102
Local loss @ local epoch 3: 0.0013085347600281239
Local loss @ local epoch 4: 0.006434400100260973
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.26 seconds!
[tester] 
SST2Metric: acc=0.5240825688073395, hinge=3.4759404210869325, ce=2.5318827643107253
Local test acc @ epoch 21: 0.5241
Client 7 execute local training on 4 samples...
Local loss @ local epoch 0: 2.9802320611338473e-08
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.44 seconds!
[tester] 
SST2Metric: acc=0.5091743119266054, hinge=7.563931950735389, ce=6.582600875732002
Local test acc @ epoch 21: 0.5092
Client 0 execute local training on 23 samples...
Local loss @ local epoch 0: 7.146830466808751e-05
Local loss @ local epoch 1: 0.0008344626403413713
Local loss @ local epoch 2: 0.0006141818012110889
Local loss @ local epoch 3: 0.0002337003534194082
Local loss @ local epoch 4: 0.00043092676787637174
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.52 seconds!
[tester] 
SST2Metric: acc=0.5126146788990825, hinge=3.2528619737526694, ce=2.293535686550884
Local test acc @ epoch 21: 0.5126
Client 9 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0024475003592669964
Local loss @ local epoch 1: 5.364416892916779e-07
Local loss @ local epoch 2: 8.001506648724899e-05
Local loss @ local epoch 3: 0.00010203729470958933
Local loss @ local epoch 4: 2.6553476345725358e-05
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.41 seconds!
[tester] 
SST2Metric: acc=0.5091743119266054, hinge=5.537744259615557, ce=4.556501750003642
Local test acc @ epoch 21: 0.5092
Client 8 execute local training on 4 samples...
Local loss @ local epoch 0: 5.960463766996327e-08
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.67 seconds!
[tester] 
SST2Metric: acc=0.5091743119266054, hinge=7.150799810339551, ce=6.170750437526528
Local test acc @ epoch 21: 0.5092
Client 2 execute local training on 5 samples...
Local loss @ local epoch 0: 3.004056907229824e-06
Local loss @ local epoch 1: 2.384185648907078e-08
Local loss @ local epoch 2: 8.821481856102764e-07
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.6 seconds!
[tester] 
SST2Metric: acc=0.5091743119266054, hinge=8.746369060026396, ce=7.764728430487694
Local test acc @ epoch 21: 0.5092
Client 5 execute local training on 4 samples...
Local loss @ local epoch 0: 2.9802320611338473e-08
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.7 seconds!
[tester] 
SST2Metric: acc=0.5091743119266054, hinge=8.211791764705554, ce=7.230208508465268
Local test acc @ epoch 21: 0.5092
Client 6 execute local training on 6 samples...
Local loss @ local epoch 0: 0.2273959070444107
Local loss @ local epoch 1: 1.8943580389022827
Local loss @ local epoch 2: 0.0029715178534388542
Local loss @ local epoch 3: 0.004601101856678724
Local loss @ local epoch 4: 0.003968082834035158
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.75 seconds!
[tester] 
SST2Metric: acc=0.8084862385321101, hinge=0.8912906951587135, ce=0.48408723401264586
Local test acc @ epoch 21: 0.8085
Global evaluate on test data...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.54 seconds!
[tester] 
SST2Metric: acc=0.8589449541284404, hinge=0.859910209791376, ce=0.559825368489378
Global test acc @ epoch 21: 0.8589
Global epoch 22...
Client 8 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0024709785357117653
Local loss @ local epoch 1: 2.9802320611338473e-08
Local loss @ local epoch 2: 2.0861621408130304e-07
Local loss @ local epoch 3: 1.609322907825117e-06
Local loss @ local epoch 4: 5.096173481433652e-06
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.3 seconds!
[tester] 
SST2Metric: acc=0.5091743119266054, hinge=6.110470915059431, ce=5.12901622672069
Local test acc @ epoch 22: 0.5092
Client 9 execute local training on 4 samples...
Local loss @ local epoch 0: 0.00041387928649783134
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 2.9802320611338473e-08
Local loss @ local epoch 4: 1.4901159772762185e-07
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.54 seconds!
[tester] 
SST2Metric: acc=0.5091743119266054, hinge=7.405590857934515, ce=6.423991104605001
Local test acc @ epoch 22: 0.5092
Client 0 execute local training on 23 samples...
Local loss @ local epoch 0: 0.00045251825940795243
Local loss @ local epoch 1: 0.013638094067573547
Local loss @ local epoch 2: 0.002407306805253029
Local loss @ local epoch 3: 0.0009685713448561728
Local loss @ local epoch 4: 0.0008949757320806384
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.29 seconds!
[tester] 
SST2Metric: acc=0.7018348623853211, hinge=1.7621019950153631, ce=1.123802512046804
Local test acc @ epoch 22: 0.7018
Client 4 execute local training on 20 samples...
Local loss @ local epoch 0: 0.0037935974542051554
Local loss @ local epoch 1: 0.046390287578105927
Local loss @ local epoch 2: 0.05269433930516243
Local loss @ local epoch 3: 0.0036044553853571415
Local loss @ local epoch 4: 0.013722472824156284
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.45 seconds!
[tester] 
SST2Metric: acc=0.5584862385321101, hinge=2.682091412194278, ce=1.803949706939929
Local test acc @ epoch 22: 0.5585
Client 1 execute local training on 6 samples...
Local loss @ local epoch 0: 0.02645820379257202
Local loss @ local epoch 1: 0.9456357955932617
Local loss @ local epoch 2: 0.0010352976387366652
Local loss @ local epoch 3: 0.008955970406532288
Local loss @ local epoch 4: 0.01240160409361124
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.33 seconds!
[tester] 
SST2Metric: acc=0.819954128440367, hinge=0.9049495186554183, ce=0.5071951369968576
Local test acc @ epoch 22: 0.82
Client 5 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0005670725367963314
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 8.940696005765858e-08
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.34 seconds!
[tester] 
SST2Metric: acc=0.5091743119266054, hinge=7.210182678808859, ce=6.228669909982486
Local test acc @ epoch 22: 0.5092
Client 6 execute local training on 6 samples...
Local loss @ local epoch 0: 0.018925417214632034
Local loss @ local epoch 1: 0.04310847446322441
Local loss @ local epoch 2: 0.09427547454833984
Local loss @ local epoch 3: 0.0001755444536684081
Local loss @ local epoch 4: 0.0004853714781347662
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.44 seconds!
[tester] 
SST2Metric: acc=0.6605504587155964, hinge=1.5575043687579828, ce=0.8742266872026231
Local test acc @ epoch 22: 0.6606
Client 2 execute local training on 5 samples...
Local loss @ local epoch 0: 0.0008169513312168419
Local loss @ local epoch 1: 1.4305112472356996e-07
Local loss @ local epoch 2: 1.1920927533992653e-07
Local loss @ local epoch 3: 3.576278118089249e-07
Local loss @ local epoch 4: 9.298319127992727e-07
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.48 seconds!
[tester] 
SST2Metric: acc=0.5091743119266054, hinge=6.885155764194804, ce=5.903762116071273
Local test acc @ epoch 22: 0.5092
Client 7 execute local training on 4 samples...
Local loss @ local epoch 0: 0.00021396961528807878
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.41 seconds!
[tester] 
SST2Metric: acc=0.5091743119266054, hinge=8.881720236682018, ce=7.900069200801193
Local test acc @ epoch 22: 0.5092
Client 3 execute local training on 4 samples...
Local loss @ local epoch 0: 2.610569390526507e-05
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.5 seconds!
[tester] 
SST2Metric: acc=0.5103211009174312, hinge=6.137660291216789, ce=5.160422234086815
Local test acc @ epoch 22: 0.5103
Global evaluate on test data...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.47 seconds!
[tester] 
SST2Metric: acc=0.7178899082568807, hinge=3.02958729070261, ce=2.424081060760331
Global test acc @ epoch 22: 0.7179
Global epoch 23...
Client 1 execute local training on 6 samples...
Local loss @ local epoch 0: 0.0010252193314954638
Local loss @ local epoch 1: 0.12221900373697281
Local loss @ local epoch 2: 0.4600832760334015
Local loss @ local epoch 3: 0.00010143464896827936
Local loss @ local epoch 4: 0.00035648548509925604
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.57 seconds!
[tester] 
SST2Metric: acc=0.8669724770642202, hinge=0.7578131620763638, ce=0.4901667699482668
Local test acc @ epoch 23: 0.867
Client 3 execute local training on 4 samples...
Local loss @ local epoch 0: 5.9604641222676946e-08
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.49 seconds!
[tester] 
SST2Metric: acc=0.5091743119266054, hinge=11.736697549120002, ce=10.755196339493498
Local test acc @ epoch 23: 0.5092
Client 2 execute local training on 5 samples...
Local loss @ local epoch 0: 0.00027477944968268275
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 3.337859766361362e-07
Local loss @ local epoch 4: 1.7166115640065982e-06
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.4 seconds!
[tester] 
SST2Metric: acc=0.5160550458715596, hinge=5.315702643416343, ce=4.347649522330032
Local test acc @ epoch 23: 0.5161
Client 5 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.23 seconds!
[tester] 
SST2Metric: acc=0.591743119266055, hinge=4.4851553543991995, ce=3.65392290629925
Local test acc @ epoch 23: 0.5917
Client 8 execute local training on 4 samples...
Local loss @ local epoch 0: 2.9802320611338473e-08
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.26 seconds!
[tester] 
SST2Metric: acc=0.5149082568807339, hinge=7.076721475758684, ce=6.115129618469728
Local test acc @ epoch 23: 0.5149
Client 6 execute local training on 6 samples...
Local loss @ local epoch 0: 0.00808022916316986
Local loss @ local epoch 1: 0.020660171285271645
Local loss @ local epoch 2: 0.013754785060882568
Local loss @ local epoch 3: 0.00026568860630504787
Local loss @ local epoch 4: 0.5177057385444641
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.61 seconds!
[tester] 
SST2Metric: acc=0.8371559633027523, hinge=0.8816879738908295, ce=0.5303899368758254
Local test acc @ epoch 23: 0.8372
Client 0 execute local training on 23 samples...
Local loss @ local epoch 0: 0.0003085425414610654
Local loss @ local epoch 1: 0.0003220047219656408
Local loss @ local epoch 2: 0.0001958379289135337
Local loss @ local epoch 3: 0.000765326723922044
Local loss @ local epoch 4: 0.00012598538887687027
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.34 seconds!
[tester] 
SST2Metric: acc=0.5263761467889908, hinge=3.6641591746325886, ce=2.719766821062893
Local test acc @ epoch 23: 0.5264
Client 7 execute local training on 4 samples...
Local loss @ local epoch 0: 1.7881390590446244e-07
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.26 seconds!
[tester] 
SST2Metric: acc=0.5091743119266054, hinge=14.41759277483739, ce=13.435941363693377
Local test acc @ epoch 23: 0.5092
Client 9 execute local training on 4 samples...
Local loss @ local epoch 0: 1.4901159772762185e-07
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.4 seconds!
[tester] 
SST2Metric: acc=0.5103211009174312, hinge=11.263531807365768, ce=10.284888934651645
Local test acc @ epoch 23: 0.5103
Client 4 execute local training on 20 samples...
Local loss @ local epoch 0: 0.10613638907670975
Local loss @ local epoch 1: 0.012597758322954178
Local loss @ local epoch 2: 0.022800249978899956
Local loss @ local epoch 3: 0.0027207836974412203
Local loss @ local epoch 4: 0.0010641918051987886
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.54 seconds!
[tester] 
SST2Metric: acc=0.6880733944954128, hinge=1.5423914151727607, ce=0.8848641568350546
Local test acc @ epoch 23: 0.6881
Global evaluate on test data...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.5 seconds!
[tester] 
SST2Metric: acc=0.9071100917431193, hinge=0.5847781528846934, ce=0.3802138325857605
Global test acc @ epoch 23: 0.9071
Global epoch 24...
Client 7 execute local training on 4 samples...
Local loss @ local epoch 0: 8.43399357108865e-06
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 2.9802320611338473e-08
Local loss @ local epoch 3: 5.960463766996327e-08
Local loss @ local epoch 4: 8.940696005765858e-08
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.25 seconds!
[tester] 
SST2Metric: acc=0.6536697247706422, hinge=2.345539751266121, ce=1.6234568998846652
Local test acc @ epoch 24: 0.6537
Client 3 execute local training on 4 samples...
Local loss @ local epoch 0: 2.622597548906924e-06
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 1.4901159772762185e-07
Local loss @ local epoch 3: 2.9802320611338473e-08
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.51 seconds!
[tester] 
SST2Metric: acc=0.6788990825688074, hinge=2.341382533175136, ce=1.6549148730903427
Local test acc @ epoch 24: 0.6789
Client 5 execute local training on 4 samples...
Local loss @ local epoch 0: 9.105981735046953e-05
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 2.264973090859712e-06
Local loss @ local epoch 3: 2.6822035579243675e-06
Local loss @ local epoch 4: 2.9802316703353426e-07
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.93 seconds!
[tester] 
SST2Metric: acc=0.5527522935779816, hinge=4.067626733298695, ce=3.187604557756967
Local test acc @ epoch 24: 0.5528
Client 2 execute local training on 5 samples...
Local loss @ local epoch 0: 1.1444069514254807e-06
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.57 seconds!
[tester] 
SST2Metric: acc=0.5172018348623854, hinge=5.500212370802503, ce=4.537908001230397
Local test acc @ epoch 24: 0.5172
Client 6 execute local training on 6 samples...
Local loss @ local epoch 0: 0.00022507684479933232
Local loss @ local epoch 1: 0.0004748225037474185
Local loss @ local epoch 2: 4.4022235670126975e-05
Local loss @ local epoch 3: 6.178921466926113e-06
Local loss @ local epoch 4: 3.470637966529466e-05
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.34 seconds!
[tester] 
SST2Metric: acc=0.8543577981651376, hinge=1.0250221356886242, ce=0.7248062947567407
Local test acc @ epoch 24: 0.8544
Client 9 execute local training on 4 samples...
Local loss @ local epoch 0: 0.00022785899636801332
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 5.9604641222676946e-08
Local loss @ local epoch 3: 4.1723239974089665e-07
Local loss @ local epoch 4: 1.1920919860131107e-06
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.4 seconds!
[tester] 
SST2Metric: acc=0.5538990825688074, hinge=3.951718403111904, ce=3.0547986983432347
Local test acc @ epoch 24: 0.5539
Client 0 execute local training on 23 samples...
Local loss @ local epoch 0: 6.0383841628208756e-05
Local loss @ local epoch 1: 0.49533918499946594
Local loss @ local epoch 2: 5.725227310904302e-05
Local loss @ local epoch 3: 2.7843254429171793e-05
Local loss @ local epoch 4: 4.345804336480796e-05
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.54 seconds!
[tester] 
SST2Metric: acc=0.5848623853211009, hinge=2.872812143159569, ce=2.0310271981795993
Local test acc @ epoch 24: 0.5849
Client 1 execute local training on 6 samples...
Local loss @ local epoch 0: 4.4760654418496415e-05
Local loss @ local epoch 1: 3.476904566923622e-06
Local loss @ local epoch 2: 3.4967918054462643e-06
Local loss @ local epoch 3: 5.563092599913944e-07
Local loss @ local epoch 4: 5.761772285950428e-07
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.56 seconds!
[tester] 
SST2Metric: acc=0.7740825688073395, hinge=2.2875388153922667, ce=1.8138252851876875
Local test acc @ epoch 24: 0.7741
Client 4 execute local training on 20 samples...
Local loss @ local epoch 0: 0.10734918713569641
Local loss @ local epoch 1: 0.0012183991493657231
Local loss @ local epoch 2: 0.0009868574561551213
Local loss @ local epoch 3: 0.0006399059784598649
Local loss @ local epoch 4: 0.0006705590058118105
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.32 seconds!
[tester] 
SST2Metric: acc=0.8497706422018348, hinge=0.8620676614275766, ce=0.5357014072008021
Local test acc @ epoch 24: 0.8498
Client 8 execute local training on 4 samples...
Local loss @ local epoch 0: 2.115960796800209e-06
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 8.940695295223122e-08
Local loss @ local epoch 4: 2.9802320611338473e-08
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.54 seconds!
[tester] 
SST2Metric: acc=0.7362385321100917, hinge=2.560627263620359, ce=2.0015362987714895
Local test acc @ epoch 24: 0.7362
Global evaluate on test data...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.28 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=0.935491300504142, ce=0.7053885408874746
Global test acc @ epoch 24: 0.8922
Global epoch 25...
Client 8 execute local training on 4 samples...
Local loss @ local epoch 0: 2.9802320611338473e-08
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.55 seconds!
[tester] 
SST2Metric: acc=0.5688073394495413, hinge=4.56422847454701, ce=3.696190143397095
Local test acc @ epoch 25: 0.5688
Client 4 execute local training on 20 samples...
Local loss @ local epoch 0: 0.0013015082804486156
Local loss @ local epoch 1: 0.09557324647903442
Local loss @ local epoch 2: 0.0020820689387619495
Local loss @ local epoch 3: 0.004284232389181852
Local loss @ local epoch 4: 0.0019138716161251068
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.43 seconds!
[tester] 
SST2Metric: acc=0.7637614678899083, hinge=1.3053674767597006, ce=0.8120866525713184
Local test acc @ epoch 25: 0.7638
Client 7 execute local training on 4 samples...
Local loss @ local epoch 0: 1.1920927533992653e-07
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.52 seconds!
[tester] 
SST2Metric: acc=0.5091743119266054, hinge=5.6815960122904645, ce=4.712677462385335
Local test acc @ epoch 25: 0.5092
Client 2 execute local training on 5 samples...
Local loss @ local epoch 0: 7.152556946721234e-08
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.96 seconds!
[tester] 
SST2Metric: acc=0.536697247706422, hinge=5.146827567061153, ce=4.237080085441606
Local test acc @ epoch 25: 0.5367
Client 1 execute local training on 6 samples...
Local loss @ local epoch 0: 9.504245826974511e-05
Local loss @ local epoch 1: 0.002659813268110156
Local loss @ local epoch 2: 0.17540593445301056
Local loss @ local epoch 3: 1.6887952369870618e-06
Local loss @ local epoch 4: 7.306314364541322e-05
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.39 seconds!
[tester] 
SST2Metric: acc=0.7878440366972477, hinge=1.3816463477020964, ce=0.943235512419685
Local test acc @ epoch 25: 0.7878
Client 0 execute local training on 23 samples...
Local loss @ local epoch 0: 0.0009205018286593258
Local loss @ local epoch 1: 0.0025693054776638746
Local loss @ local epoch 2: 0.0001698662235867232
Local loss @ local epoch 3: 0.0003503606130834669
Local loss @ local epoch 4: 0.0011220844462513924
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.21 seconds!
[tester] 
SST2Metric: acc=0.5080275229357798, hinge=3.295716039084513, ce=2.3277064548173083
Local test acc @ epoch 25: 0.508
Client 3 execute local training on 4 samples...
Local loss @ local epoch 0: 2.175562030970468e-06
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.64 seconds!
[tester] 
SST2Metric: acc=0.6376146788990825, hinge=4.124569155754299, ce=3.3901609621873687
Local test acc @ epoch 25: 0.6376
Client 6 execute local training on 6 samples...
Local loss @ local epoch 0: 0.00020951800979673862
Local loss @ local epoch 1: 0.002490835264325142
Local loss @ local epoch 2: 1.565104365348816
Local loss @ local epoch 3: 0.06843090802431107
Local loss @ local epoch 4: 0.014177690260112286
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 79.06 seconds!
[tester] 
SST2Metric: acc=0.8486238532110092, hinge=0.8135166378743058, ce=0.45920043426711077
Local test acc @ epoch 25: 0.8486
Client 9 execute local training on 4 samples...
Local loss @ local epoch 0: 2.205369128205348e-06
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.6 seconds!
[tester] 
SST2Metric: acc=0.5091743119266054, hinge=6.827600527247157, ce=5.8486382143759945
Local test acc @ epoch 25: 0.5092
Client 5 execute local training on 4 samples...
Local loss @ local epoch 0: 4.4703472212859197e-07
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 79.05 seconds!
[tester] 
SST2Metric: acc=0.6548165137614679, hinge=3.2578444043430714, ce=2.5339629567749458
Local test acc @ epoch 25: 0.6548
Global evaluate on test data...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.77 seconds!
[tester] 
SST2Metric: acc=0.8910550458715596, hinge=1.0703470417118948, ce=0.8494830318030744
Global test acc @ epoch 25: 0.8911
Global epoch 26...
Client 3 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.65 seconds!
[tester] 
SST2Metric: acc=0.8314220183486238, hinge=2.0663428496603573, ce=1.7157830126421236
Local test acc @ epoch 26: 0.8314
Client 6 execute local training on 6 samples...
Local loss @ local epoch 0: 8.439867087872699e-05
Local loss @ local epoch 1: 0.003038961673155427
Local loss @ local epoch 2: 1.6435354948043823
Local loss @ local epoch 3: 0.0007830761023797095
Local loss @ local epoch 4: 0.014553566463291645
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.28 seconds!
[tester] 
SST2Metric: acc=0.7637614678899083, hinge=1.3712772873290089, ce=0.8576281301574696
Local test acc @ epoch 26: 0.7638
Client 5 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.42 seconds!
[tester] 
SST2Metric: acc=0.8795871559633027, hinge=1.304121331474103, ce=1.0700087674504637
Local test acc @ epoch 26: 0.8796
Client 2 execute local training on 5 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.28 seconds!
[tester] 
SST2Metric: acc=0.8727064220183486, hinge=1.5709904514321493, ce=1.3009169197709667
Local test acc @ epoch 26: 0.8727
Client 9 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.26 seconds!
[tester] 
SST2Metric: acc=0.8704128440366973, hinge=1.6689525563782508, ce=1.4035667260017228
Local test acc @ epoch 26: 0.8704
Client 1 execute local training on 6 samples...
Local loss @ local epoch 0: 4.927245299768401e-06
Local loss @ local epoch 1: 0.08644165843725204
Local loss @ local epoch 2: 1.6192238035728224e-05
Local loss @ local epoch 3: 0.0002363763633184135
Local loss @ local epoch 4: 0.004119446501135826
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.35 seconds!
[tester] 
SST2Metric: acc=0.7259174311926605, hinge=1.52413293828658, ce=0.9302685007008664
Local test acc @ epoch 26: 0.7259
Client 8 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.4 seconds!
[tester] 
SST2Metric: acc=0.8474770642201835, hinge=1.8711245768113967, ce=1.5600829095101205
Local test acc @ epoch 26: 0.8475
Client 7 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.5 seconds!
[tester] 
SST2Metric: acc=0.8635321100917431, hinge=1.6041062292429284, ce=1.317565303019765
Local test acc @ epoch 26: 0.8635
Client 4 execute local training on 20 samples...
Local loss @ local epoch 0: 0.0001196132943732664
Local loss @ local epoch 1: 1.9859354496002197
Local loss @ local epoch 2: 0.01617152988910675
Local loss @ local epoch 3: 0.0011544832959771156
Local loss @ local epoch 4: 0.0004795704735442996
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.45 seconds!
[tester] 
SST2Metric: acc=0.8451834862385321, hinge=0.8684462385440092, ce=0.5442092718125039
Local test acc @ epoch 26: 0.8452
Client 0 execute local training on 23 samples...
Local loss @ local epoch 0: 1.6008078773666057e-06
Local loss @ local epoch 1: 0.2012643963098526
Local loss @ local epoch 2: 0.0013175649801269174
Local loss @ local epoch 3: 0.01800370030105114
Local loss @ local epoch 4: 0.0004028048715554178
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.56 seconds!
[tester] 
SST2Metric: acc=0.7477064220183486, hinge=1.6249179099131068, ce=1.0676220199862205
Local test acc @ epoch 26: 0.7477
Global evaluate on test data...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.21 seconds!
[tester] 
SST2Metric: acc=0.911697247706422, hinge=0.753338978241343, ce=0.5789493208048034
Global test acc @ epoch 26: 0.9117
Global epoch 27...
Client 2 execute local training on 5 samples...
Local loss @ local epoch 0: 5.459763087856118e-06
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.21 seconds!
[tester] 
SST2Metric: acc=0.5103211009174312, hinge=6.492680428224966, ce=5.5193767851098965
Local test acc @ epoch 27: 0.5103
Client 4 execute local training on 20 samples...
Local loss @ local epoch 0: 2.9094016551971436
Local loss @ local epoch 1: 0.0005692224949598312
Local loss @ local epoch 2: 0.00032721436582505703
Local loss @ local epoch 3: 0.0005650401581078768
Local loss @ local epoch 4: 0.0005257303128018975
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.26 seconds!
[tester] 
SST2Metric: acc=0.7729357798165137, hinge=1.34923148961789, ce=0.8711428278501769
Local test acc @ epoch 27: 0.7729
Client 1 execute local training on 6 samples...
Local loss @ local epoch 0: 2.7219386993238004e-06
Local loss @ local epoch 1: 2.1457533421198605e-06
Local loss @ local epoch 2: 1.9868213740892315e-08
Local loss @ local epoch 3: 7.351229101004719e-07
Local loss @ local epoch 4: 1.3907747131725046e-07
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.23 seconds!
[tester] 
SST2Metric: acc=0.6938073394495413, hinge=4.612295231414498, ce=3.9941234392222924
Local test acc @ epoch 27: 0.6938
Client 3 execute local training on 4 samples...
Local loss @ local epoch 0: 3.1292320272768848e-06
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.29 seconds!
[tester] 
SST2Metric: acc=0.5240825688073395, hinge=6.342509081604284, ce=5.38749261528527
Local test acc @ epoch 27: 0.5241
Client 9 execute local training on 4 samples...
Local loss @ local epoch 0: 3.1590395792591153e-06
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.24 seconds!
[tester] 
SST2Metric: acc=0.5561926605504587, hinge=4.794541507139119, ce=3.9025942543777847
Local test acc @ epoch 27: 0.5562
Client 6 execute local training on 6 samples...
Local loss @ local epoch 0: 2.0662896531575825e-06
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 3.973642392907095e-08
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 1.9868213740892315e-08
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.25 seconds!
[tester] 
SST2Metric: acc=0.8451834862385321, hinge=1.885975598468693, ce=1.5762466482599597
Local test acc @ epoch 27: 0.8452
Client 5 execute local training on 4 samples...
Local loss @ local epoch 0: 5.364416892916779e-07
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.34 seconds!
[tester] 
SST2Metric: acc=0.6238532110091743, hinge=3.7499195512281647, ce=2.980045096111261
Local test acc @ epoch 27: 0.6239
Client 0 execute local training on 23 samples...
Local loss @ local epoch 0: 8.004041092135594e-07
Local loss @ local epoch 1: 1.021779917209642e-05
Local loss @ local epoch 2: 5.4049331083660945e-05
Local loss @ local epoch 3: 7.288735559995985e-06
Local loss @ local epoch 4: 6.471306278399425e-06
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.39 seconds!
[tester] 
SST2Metric: acc=0.5321100917431193, hinge=4.647040692217853, ce=3.700989282459294
Local test acc @ epoch 27: 0.5321
Client 7 execute local training on 4 samples...
Local loss @ local epoch 0: 3.6358712804940296e-06
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.26 seconds!
[tester] 
SST2Metric: acc=0.6192660550458715, hinge=3.276394921158432, ce=2.490995691575611
Local test acc @ epoch 27: 0.6193
Client 8 execute local training on 4 samples...
Local loss @ local epoch 0: 2.9206210001575528e-06
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.34 seconds!
[tester] 
SST2Metric: acc=0.5137614678899083, hinge=6.187192394099104, ce=5.213524880075673
Local test acc @ epoch 27: 0.5138
Global evaluate on test data...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.21 seconds!
[tester] 
SST2Metric: acc=0.8727064220183486, hinge=1.4158964081915146, ce=1.1587967927159941
Global test acc @ epoch 27: 0.8727
Global epoch 28...
Client 9 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.39 seconds!
[tester] 
SST2Metric: acc=0.8520642201834863, hinge=2.211345651691113, ce=1.9109670451048968
Local test acc @ epoch 28: 0.8521
Client 4 execute local training on 20 samples...
Local loss @ local epoch 0: 0.0014744473155587912
Local loss @ local epoch 1: 0.03431428223848343
Local loss @ local epoch 2: 0.0003649405553005636
Local loss @ local epoch 3: 0.002163797616958618
Local loss @ local epoch 4: 0.003559955395758152
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.51 seconds!
[tester] 
SST2Metric: acc=0.7201834862385321, hinge=1.4649488441987868, ce=0.8508054151652603
Local test acc @ epoch 28: 0.7202
Client 3 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.27 seconds!
[tester] 
SST2Metric: acc=0.8761467889908257, hinge=1.5057439856026151, ce=1.2491501180358364
Local test acc @ epoch 28: 0.8761
Client 0 execute local training on 23 samples...
Local loss @ local epoch 0: 2.843986067091464e-06
Local loss @ local epoch 1: 0.00025176574126817286
Local loss @ local epoch 2: 0.00010780482261907309
Local loss @ local epoch 3: 0.00030564842745661736
Local loss @ local epoch 4: 0.00027678199694491923
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.22 seconds!
[tester] 
SST2Metric: acc=0.5263761467889908, hinge=4.2233823228866685, ce=3.2714704621002215
Local test acc @ epoch 28: 0.5264
Client 2 execute local training on 5 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.21 seconds!
[tester] 
SST2Metric: acc=0.8704128440366973, hinge=1.8477907938147904, ce=1.58893906613415
Local test acc @ epoch 28: 0.8704
Client 1 execute local training on 6 samples...
Local loss @ local epoch 0: 4.967046720594226e-07
Local loss @ local epoch 1: 1.6093187014121213e-06
Local loss @ local epoch 2: 1.1920925402364446e-07
Local loss @ local epoch 3: 9.934104383546583e-08
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.5 seconds!
[tester] 
SST2Metric: acc=0.8451834862385321, hinge=2.723747335579417, ce=2.4152897759340752
Local test acc @ epoch 28: 0.8452
Client 8 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.36 seconds!
[tester] 
SST2Metric: acc=0.8715596330275229, hinge=1.6602202811098974, ce=1.404093469412146
Local test acc @ epoch 28: 0.8716
Client 5 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.54 seconds!
[tester] 
SST2Metric: acc=0.8738532110091743, hinge=1.5207016618426787, ce=1.2702281049519073
Local test acc @ epoch 28: 0.8739
Client 6 execute local training on 6 samples...
Local loss @ local epoch 0: 2.066281695078942e-06
Local loss @ local epoch 1: 0.001192108727991581
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 2.920602128142491e-06
Local loss @ local epoch 4: 0.009985113516449928
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.59 seconds!
[tester] 
SST2Metric: acc=0.819954128440367, hinge=2.1939010387713758, ce=1.8298002147426542
Local test acc @ epoch 28: 0.82
Client 7 execute local training on 4 samples...
Local loss @ local epoch 0: 1.7881387748275301e-07
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.55 seconds!
[tester] 
SST2Metric: acc=0.5126146788990825, hinge=10.54194393726664, ce=9.565350320361077
Local test acc @ epoch 28: 0.5126
Global evaluate on test data...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.21 seconds!
[tester] 
SST2Metric: acc=0.911697247706422, hinge=1.115929438296808, ce=0.9374652754662408
Global test acc @ epoch 28: 0.9117
Global epoch 29...
Client 2 execute local training on 5 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.28 seconds!
[tester] 
SST2Metric: acc=0.908256880733945, hinge=1.3730814354408771, ce=1.1897381213211617
Local test acc @ epoch 29: 0.9083
Client 4 execute local training on 20 samples...
Local loss @ local epoch 0: 0.00017571274656802416
Local loss @ local epoch 1: 4.6040688175708055e-05
Local loss @ local epoch 2: 7.81010530772619e-05
Local loss @ local epoch 3: 0.0002207997313234955
Local loss @ local epoch 4: 0.005088929086923599
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.31 seconds!
[tester] 
SST2Metric: acc=0.5974770642201835, hinge=2.5598388031261776, ce=1.7393533718097238
Local test acc @ epoch 29: 0.5975
Client 7 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.17 seconds!
[tester] 
SST2Metric: acc=0.9105504587155964, hinge=1.3430812477792076, ce=1.159635969392321
Local test acc @ epoch 29: 0.9106
Client 9 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.26 seconds!
[tester] 
SST2Metric: acc=0.8830275229357798, hinge=2.1419013567747327, ce=1.9025709141277483
Local test acc @ epoch 29: 0.883
Client 3 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.66 seconds!
[tester] 
SST2Metric: acc=0.908256880733945, hinge=1.2285231600660798, ce=1.046190855568417
Local test acc @ epoch 29: 0.9083
Client 5 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.44 seconds!
[tester] 
SST2Metric: acc=0.9094036697247706, hinge=1.1740342370413859, ce=0.994232729136848
Local test acc @ epoch 29: 0.9094
Client 8 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.48 seconds!
[tester] 
SST2Metric: acc=0.9094036697247706, hinge=1.237414795324343, ce=1.0562527458687645
Local test acc @ epoch 29: 0.9094
Client 6 execute local training on 6 samples...
Local loss @ local epoch 0: 5.960463411724959e-08
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.46 seconds!
[tester] 
SST2Metric: acc=0.911697247706422, hinge=1.1065870229530772, ce=0.9285477052012496
Local test acc @ epoch 29: 0.9117
Client 1 execute local training on 6 samples...
Local loss @ local epoch 0: 1.9868213740892315e-08
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.36 seconds!
[tester] 
SST2Metric: acc=0.908256880733945, hinge=1.3921189771606288, ce=1.2113642777101084
Local test acc @ epoch 29: 0.9083
Client 0 execute local training on 23 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 2.8950825026186067e-07
Local loss @ local epoch 2: 2.487993879185524e-05
Local loss @ local epoch 3: 2.1371919501689263e-05
Local loss @ local epoch 4: 1.8238588381791487e-05
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.37 seconds!
[tester] 
SST2Metric: acc=0.5435779816513762, hinge=4.072428818142742, ce=3.151186731062966
Local test acc @ epoch 29: 0.5436
Global evaluate on test data...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.46 seconds!
[tester] 
SST2Metric: acc=0.8979357798165137, hinge=1.2805307220428361, ce=1.073059002556549
Global test acc @ epoch 29: 0.8979
Global epoch 30...
Client 0 execute local training on 23 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 1.7029897492193413e-08
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.38 seconds!
[tester] 
SST2Metric: acc=0.6846330275229358, hinge=6.758797255131083, ce=6.125805205434834
Local test acc @ epoch 30: 0.6846
Client 2 execute local training on 5 samples...
Local loss @ local epoch 0: 7.152556946721234e-08
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 79.38 seconds!
[tester] 
SST2Metric: acc=0.8864678899082569, hinge=1.5241255690471842, ce=1.2925844936689828
Local test acc @ epoch 30: 0.8865
Client 1 execute local training on 6 samples...
Local loss @ local epoch 0: 1.3907748552810517e-07
Local loss @ local epoch 1: 9.934104383546583e-08
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.66 seconds!
[tester] 
SST2Metric: acc=0.8795871559633027, hinge=1.7959450062261808, ce=1.5443921238303706
Local test acc @ epoch 30: 0.8796
Client 8 execute local training on 4 samples...
Local loss @ local epoch 0: 5.9604641222676946e-08
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.49 seconds!
[tester] 
SST2Metric: acc=0.8692660550458715, hinge=1.847153514486934, ce=1.5752891276717675
Local test acc @ epoch 30: 0.8693
Client 5 execute local training on 4 samples...
Local loss @ local epoch 0: 2.9802320611338473e-08
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.56 seconds!
[tester] 
SST2Metric: acc=0.8692660550458715, hinge=2.1943180462635987, ce=1.9301035650913354
Local test acc @ epoch 30: 0.8693
Client 4 execute local training on 20 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 5.9604641222676946e-08
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.44 seconds!
[tester] 
SST2Metric: acc=0.8772935779816514, hinge=2.56896993627242, ce=2.3233764773592753
Local test acc @ epoch 30: 0.8773
Client 6 execute local training on 6 samples...
Local loss @ local epoch 0: 3.973642392907095e-08
Local loss @ local epoch 1: 5.960463411724959e-08
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.39 seconds!
[tester] 
SST2Metric: acc=0.9105504587155964, hinge=1.2553340459088667, ce=1.0774194171281646
Local test acc @ epoch 30: 0.9106
Client 3 execute local training on 4 samples...
Local loss @ local epoch 0: 2.9802320611338473e-08
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.2 seconds!
[tester] 
SST2Metric: acc=0.9094036697247706, hinge=1.1966524006576713, ce=1.0150999016234832
Local test acc @ epoch 30: 0.9094
Client 7 execute local training on 4 samples...
Local loss @ local epoch 0: 8.940696005765858e-08
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.45 seconds!
[tester] 
SST2Metric: acc=0.9048165137614679, hinge=1.0862204485803568, ce=0.8881938957492821
Local test acc @ epoch 30: 0.9048
Client 9 execute local training on 4 samples...
Local loss @ local epoch 0: 5.828659050166607e-05
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 1.4901159772762185e-07
Local loss @ local epoch 4: 8.046623634072603e-07
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.51 seconds!
[tester] 
SST2Metric: acc=0.5229357798165137, hinge=5.834457677438719, ce=4.880349871078421
Local test acc @ epoch 30: 0.5229
Global evaluate on test data...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.54 seconds!
[tester] 
SST2Metric: acc=0.8681192660550459, hinge=2.7554742620078794, ce=2.488478035425361
Global test acc @ epoch 30: 0.8681
Global epoch 31...
Client 5 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.28 seconds!
[tester] 
SST2Metric: acc=0.8681192660550459, hinge=2.7679451518102525, ce=2.500781043855726
Local test acc @ epoch 31: 0.8681
Client 3 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.32 seconds!
[tester] 
SST2Metric: acc=0.8681192660550459, hinge=2.7603993205302353, ce=2.4933657129642937
Local test acc @ epoch 31: 0.8681
Client 7 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.5 seconds!
[tester] 
SST2Metric: acc=0.8681192660550459, hinge=2.7558484722714907, ce=2.4888626618418628
Local test acc @ epoch 31: 0.8681
Client 6 execute local training on 6 samples...
Local loss @ local epoch 0: 9.934104383546583e-08
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 1.9868213740892315e-08
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.37 seconds!
[tester] 
SST2Metric: acc=0.8658256880733946, hinge=1.6118082160796594, ce=1.3424748306273162
Local test acc @ epoch 31: 0.8658
Client 0 execute local training on 23 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 1.311300025008677e-06
Local loss @ local epoch 2: 1.1920926823449918e-07
Local loss @ local epoch 3: 8.514948746096707e-08
Local loss @ local epoch 4: 3.4059794984386826e-08
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.44 seconds!
[tester] 
SST2Metric: acc=0.5894495412844036, hinge=5.940887340711891, ce=5.119196250455543
Local test acc @ epoch 31: 0.5894
Client 9 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.56 seconds!
[tester] 
SST2Metric: acc=0.8681192660550459, hinge=2.802619808037347, ce=2.5350840149393923
Local test acc @ epoch 31: 0.8681
Client 2 execute local training on 5 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.37 seconds!
[tester] 
SST2Metric: acc=0.8681192660550459, hinge=2.8016711312149645, ce=2.5343250861535247
Local test acc @ epoch 31: 0.8681
Client 1 execute local training on 6 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.7 seconds!
[tester] 
SST2Metric: acc=0.8876146788990825, hinge=2.254130324229188, ce=2.0242218284080304
Local test acc @ epoch 31: 0.8876
Client 8 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.42 seconds!
[tester] 
SST2Metric: acc=0.8681192660550459, hinge=2.799612442151122, ce=2.532248517189025
Local test acc @ epoch 31: 0.8681
Client 4 execute local training on 20 samples...
Local loss @ local epoch 0: 1.1920927533992653e-07
Local loss @ local epoch 1: 4.720248398371041e-05
Local loss @ local epoch 2: 5.06639196373726e-07
Local loss @ local epoch 3: 3.6656683732871898e-06
Local loss @ local epoch 4: 2.2351705410983413e-06
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.22 seconds!
[tester] 
SST2Metric: acc=0.908256880733945, hinge=0.9261746369644043, ce=0.7399639562460142
Local test acc @ epoch 31: 0.9083
Global evaluate on test data...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.33 seconds!
[tester] 
SST2Metric: acc=0.8956422018348624, hinge=1.9272886846863895, ce=1.716536605430153
Global test acc @ epoch 31: 0.8956
Global epoch 32...
Client 7 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.22 seconds!
[tester] 
SST2Metric: acc=0.8956422018348624, hinge=1.9265664562993092, ce=1.7158381340826094
Local test acc @ epoch 32: 0.8956
Client 6 execute local training on 6 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.39 seconds!
[tester] 
SST2Metric: acc=0.8956422018348624, hinge=1.927648404185925, ce=1.7169242706171777
Local test acc @ epoch 32: 0.8956
Client 3 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.33 seconds!
[tester] 
SST2Metric: acc=0.8956422018348624, hinge=1.9267616811968864, ce=1.7160241948311428
Local test acc @ epoch 32: 0.8956
Client 2 execute local training on 5 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.5 seconds!
[tester] 
SST2Metric: acc=0.8956422018348624, hinge=1.9263573036007924, ce=1.7156545292851904
Local test acc @ epoch 32: 0.8956
Client 4 execute local training on 20 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.54 seconds!
[tester] 
SST2Metric: acc=0.8944954128440367, hinge=2.053548056871519, ce=1.8399291472957964
Local test acc @ epoch 32: 0.8945
Client 9 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.43 seconds!
[tester] 
SST2Metric: acc=0.8956422018348624, hinge=1.9250714654495957, ce=1.7144052054441454
Local test acc @ epoch 32: 0.8956
Client 5 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.47 seconds!
[tester] 
SST2Metric: acc=0.8956422018348624, hinge=1.9271436891424547, ce=1.7163941423093088
Local test acc @ epoch 32: 0.8956
Client 8 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.33 seconds!
[tester] 
SST2Metric: acc=0.8956422018348624, hinge=1.926862508058548, ce=1.716125270328772
Local test acc @ epoch 32: 0.8956
Client 0 execute local training on 23 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.29 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.0253112048184105, ce=1.8127909863652498
Local test acc @ epoch 32: 0.8933
Client 1 execute local training on 6 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.28 seconds!
[tester] 
SST2Metric: acc=0.8956422018348624, hinge=1.9275056796336392, ce=1.7167459151874527
Local test acc @ epoch 32: 0.8956
Global evaluate on test data...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.41 seconds!
[tester] 
SST2Metric: acc=0.8956422018348624, hinge=1.9546119380708133, ce=1.7435602792071025
Global test acc @ epoch 32: 0.8956
Global epoch 33...
Client 6 execute local training on 6 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.4 seconds!
[tester] 
SST2Metric: acc=0.8956422018348624, hinge=1.9545567743821974, ce=1.743507114686272
Local test acc @ epoch 33: 0.8956
Client 1 execute local training on 6 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.2 seconds!
[tester] 
SST2Metric: acc=0.8956422018348624, hinge=1.9543077464497418, ce=1.7432132650062564
Local test acc @ epoch 33: 0.8956
Client 9 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.26 seconds!
[tester] 
SST2Metric: acc=0.8956422018348624, hinge=1.9516609769075288, ce=1.7405264517297434
Local test acc @ epoch 33: 0.8956
Client 7 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.49 seconds!
[tester] 
SST2Metric: acc=0.8956422018348624, hinge=1.9536167543142213, ce=1.7425206631074375
Local test acc @ epoch 33: 0.8956
Client 2 execute local training on 5 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.29 seconds!
[tester] 
SST2Metric: acc=0.8956422018348624, hinge=1.9533806473290154, ce=1.7423106919443279
Local test acc @ epoch 33: 0.8956
Client 5 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.46 seconds!
[tester] 
SST2Metric: acc=0.8956422018348624, hinge=1.9542832911834804, ce=1.7432102243981182
Local test acc @ epoch 33: 0.8956
Client 0 execute local training on 23 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.45 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.031915180316759, ce=1.8191386490243024
Local test acc @ epoch 33: 0.8933
Client 4 execute local training on 20 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.39 seconds!
[tester] 
SST2Metric: acc=0.8944954128440367, hinge=2.047806372472999, ce=1.8350698578451572
Local test acc @ epoch 33: 0.8945
Client 3 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.59 seconds!
[tester] 
SST2Metric: acc=0.8956422018348624, hinge=1.953912049258521, ce=1.7428176564282556
Local test acc @ epoch 33: 0.8956
Client 8 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.27 seconds!
[tester] 
SST2Metric: acc=0.8956422018348624, hinge=1.9540448977586327, ce=1.7429674936868909
Local test acc @ epoch 33: 0.8956
Global evaluate on test data...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.59 seconds!
[tester] 
SST2Metric: acc=0.8956422018348624, hinge=1.973519813030138, ce=1.7621107619961358
Global test acc @ epoch 33: 0.8956
Global epoch 34...
Client 7 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.21 seconds!
[tester] 
SST2Metric: acc=0.8956422018348624, hinge=1.9724463642737187, ce=1.7610149257943617
Local test acc @ epoch 34: 0.8956
Client 1 execute local training on 6 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.4 seconds!
[tester] 
SST2Metric: acc=0.8956422018348624, hinge=1.973045396832151, ce=1.7616026573053498
Local test acc @ epoch 34: 0.8956
Client 0 execute local training on 23 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.16 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.038492464417711, ce=1.8252624414516752
Local test acc @ epoch 34: 0.8933
Client 3 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.22 seconds!
[tester] 
SST2Metric: acc=0.8956422018348624, hinge=1.9727124764558372, ce=1.7612796628844687
Local test acc @ epoch 34: 0.8956
Client 4 execute local training on 20 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.43 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.047352278314599, ce=1.834769586024232
Local test acc @ epoch 34: 0.8933
Client 9 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.35 seconds!
[tester] 
SST2Metric: acc=0.8956422018348624, hinge=1.9705125725050585, ce=1.7590808015295296
Local test acc @ epoch 34: 0.8956
Client 6 execute local training on 6 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.52 seconds!
[tester] 
SST2Metric: acc=0.8956422018348624, hinge=1.973348014945284, ce=1.7619480352856092
Local test acc @ epoch 34: 0.8956
Client 2 execute local training on 5 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.45 seconds!
[tester] 
SST2Metric: acc=0.8956422018348624, hinge=1.9722566369476668, ce=1.7608582593444084
Local test acc @ epoch 34: 0.8956
Client 8 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.3 seconds!
[tester] 
SST2Metric: acc=0.8956422018348624, hinge=1.97290263550544, ce=1.7614822337490157
Local test acc @ epoch 34: 0.8956
Client 5 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.4 seconds!
[tester] 
SST2Metric: acc=0.8956422018348624, hinge=1.9732087221714334, ce=1.7617927295034754
Local test acc @ epoch 34: 0.8956
Global evaluate on test data...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.36 seconds!
[tester] 
SST2Metric: acc=0.8956422018348624, hinge=1.9880666267981224, ce=1.7763141410691146
Global test acc @ epoch 34: 0.8956
Global epoch 35...
Client 0 execute local training on 23 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.46 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.0449375653759057, ce=1.831084052963822
Local test acc @ epoch 35: 0.8933
Client 8 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.52 seconds!
[tester] 
SST2Metric: acc=0.8956422018348624, hinge=1.987411822474331, ce=1.7756464466990256
Local test acc @ epoch 35: 0.8956
Client 1 execute local training on 6 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.36 seconds!
[tester] 
SST2Metric: acc=0.8956422018348624, hinge=1.9875390738795657, ce=1.7757575616147916
Local test acc @ epoch 35: 0.8956
Client 9 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.11 seconds!
[tester] 
SST2Metric: acc=0.8956422018348624, hinge=1.9850759107038516, ce=1.7732977265015455
Local test acc @ epoch 35: 0.8956
Client 6 execute local training on 6 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.37 seconds!
[tester] 
SST2Metric: acc=0.8956422018348624, hinge=1.9877704455765015, ce=1.7760294281374986
Local test acc @ epoch 35: 0.8956
Client 5 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.38 seconds!
[tester] 
SST2Metric: acc=0.8956422018348624, hinge=1.9877056137684288, ce=1.7759474329544647
Local test acc @ epoch 35: 0.8956
Client 2 execute local training on 5 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.38 seconds!
[tester] 
SST2Metric: acc=0.8956422018348624, hinge=1.9867425238320586, ce=1.774998355631666
Local test acc @ epoch 35: 0.8956
Client 7 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.46 seconds!
[tester] 
SST2Metric: acc=0.8956422018348624, hinge=1.987008378319784, ce=1.7752309132141586
Local test acc @ epoch 35: 0.8956
Client 4 execute local training on 20 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.45 seconds!
[tester] 
SST2Metric: acc=0.8944954128440367, hinge=2.0489973208499612, ce=1.836427897528746
Local test acc @ epoch 35: 0.8945
Client 3 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.36 seconds!
[tester] 
SST2Metric: acc=0.8956422018348624, hinge=1.987269434360189, ce=1.7754926435369223
Local test acc @ epoch 35: 0.8956
Global evaluate on test data...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.5 seconds!
[tester] 
SST2Metric: acc=0.8944954128440367, hinge=1.9998118906119549, ce=1.7878444249154855
Global test acc @ epoch 35: 0.8945
Global epoch 36...
Client 1 execute local training on 6 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.32 seconds!
[tester] 
SST2Metric: acc=0.8944954128440367, hinge=1.9991703796277351, ce=1.7872013337691781
Local test acc @ epoch 36: 0.8945
Client 2 execute local training on 5 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.45 seconds!
[tester] 
SST2Metric: acc=0.8944954128440367, hinge=1.9984028421410727, ce=1.7864677806599973
Local test acc @ epoch 36: 0.8945
Client 4 execute local training on 20 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.26 seconds!
[tester] 
SST2Metric: acc=0.8944954128440367, hinge=2.051761832122409, ce=1.83906540699363
Local test acc @ epoch 36: 0.8945
Client 0 execute local training on 23 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.49 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.050969966234417, ce=1.8365817775878852
Local test acc @ epoch 36: 0.8933
Client 9 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.43 seconds!
[tester] 
SST2Metric: acc=0.8944954128440367, hinge=1.996845226222222, ce=1.784881431037485
Local test acc @ epoch 36: 0.8945
Client 7 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.54 seconds!
[tester] 
SST2Metric: acc=0.8944954128440367, hinge=1.9987287883638243, ce=1.786761081047786
Local test acc @ epoch 36: 0.8945
Client 3 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.48 seconds!
[tester] 
SST2Metric: acc=0.8944954128440367, hinge=1.9989674434475941, ce=1.787001710844161
Local test acc @ epoch 36: 0.8945
Client 8 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.57 seconds!
[tester] 
SST2Metric: acc=0.8944954128440367, hinge=1.9991385311981953, ce=1.7871818237729478
Local test acc @ epoch 36: 0.8945
Client 5 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.59 seconds!
[tester] 
SST2Metric: acc=0.8944954128440367, hinge=1.9994221151968754, ce=1.787474804426449
Local test acc @ epoch 36: 0.8945
Client 6 execute local training on 6 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.4 seconds!
[tester] 
SST2Metric: acc=0.8944954128440367, hinge=1.999447925785266, ce=1.7875181326006169
Local test acc @ epoch 36: 0.8945
Global evaluate on test data...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.28 seconds!
[tester] 
SST2Metric: acc=0.8944954128440367, hinge=2.009672542914338, ce=1.7975554516206673
Global test acc @ epoch 36: 0.8945
Global epoch 37...
Client 5 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.26 seconds!
[tester] 
SST2Metric: acc=0.8944954128440367, hinge=2.0091961281835484, ce=1.7971179428791217
Local test acc @ epoch 37: 0.8945
Client 1 execute local training on 6 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.36 seconds!
[tester] 
SST2Metric: acc=0.8944954128440367, hinge=2.0089113888663985, ce=1.7968691880022165
Local test acc @ epoch 37: 0.8945
Client 9 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.49 seconds!
[tester] 
SST2Metric: acc=0.8944954128440367, hinge=2.0066107142409053, ce=1.7945739518714938
Local test acc @ epoch 37: 0.8945
Client 0 execute local training on 23 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.57 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.056574594400345, ce=1.84174305219824
Local test acc @ epoch 37: 0.8933
Client 6 execute local training on 6 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.42 seconds!
[tester] 
SST2Metric: acc=0.8944954128440367, hinge=2.0092167821499185, ce=1.797160008919489
Local test acc @ epoch 37: 0.8945
Client 4 execute local training on 20 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.31 seconds!
[tester] 
SST2Metric: acc=0.8944954128440367, hinge=2.0552998834520304, ce=1.8422369659269124
Local test acc @ epoch 37: 0.8945
Client 7 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.34 seconds!
[tester] 
SST2Metric: acc=0.8944954128440367, hinge=2.0084890925556147, ce=1.796446962392459
Local test acc @ epoch 37: 0.8945
Client 2 execute local training on 5 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.27 seconds!
[tester] 
SST2Metric: acc=0.8944954128440367, hinge=2.0082075368920598, ce=1.7961956678028808
Local test acc @ epoch 37: 0.8945
Client 3 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.16 seconds!
[tester] 
SST2Metric: acc=0.8944954128440367, hinge=2.0087347930177635, ce=1.7966941095131137
Local test acc @ epoch 37: 0.8945
Client 8 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.04 seconds!
[tester] 
SST2Metric: acc=0.8944954128440367, hinge=2.008928713038427, ce=1.7968762698345608
Local test acc @ epoch 37: 0.8945
Global evaluate on test data...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.44 seconds!
[tester] 
SST2Metric: acc=0.8944954128440367, hinge=2.018113431307154, ce=1.805942179078472
Global test acc @ epoch 37: 0.8945
Global epoch 38...
Client 4 execute local training on 20 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.56 seconds!
[tester] 
SST2Metric: acc=0.8944954128440367, hinge=2.059058288095194, ce=1.8456242957380755
Local test acc @ epoch 38: 0.8945
Client 3 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.62 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.0171224221997304, ce=1.8050420595399652
Local test acc @ epoch 38: 0.8933
Client 0 execute local training on 23 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.38 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.0618039856263257, ce=1.84661928063358
Local test acc @ epoch 38: 0.8933
Client 6 execute local training on 6 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.29 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.0176376968348793, ce=1.8055292537145866
Local test acc @ epoch 38: 0.8933
Client 2 execute local training on 5 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.39 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.0165539880137926, ce=1.8045541374445662
Local test acc @ epoch 38: 0.8933
Client 9 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.19 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.0150443642237863, ce=1.8030261863935195
Local test acc @ epoch 38: 0.8933
Client 7 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.19 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.0168635808273194, ce=1.8048093037333737
Local test acc @ epoch 38: 0.8933
Client 8 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.35 seconds!
[tester] 
SST2Metric: acc=0.8944954128440367, hinge=2.017351560773106, ce=1.8052456918602378
Local test acc @ epoch 38: 0.8945
Client 5 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.39 seconds!
[tester] 
SST2Metric: acc=0.8944954128440367, hinge=2.0176546230502086, ce=1.8055251529815628
Local test acc @ epoch 38: 0.8945
Client 1 execute local training on 6 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.21 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.0173389105074997, ce=1.805263166578526
Local test acc @ epoch 38: 0.8933
Global evaluate on test data...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.49 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.025501405427215, ce=1.8133518468864804
Global test acc @ epoch 38: 0.8933
Global epoch 39...
Client 4 execute local training on 20 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.69 seconds!
[tester] 
SST2Metric: acc=0.8944954128440367, hinge=2.0629094560211954, ce=1.8491311172077574
Local test acc @ epoch 39: 0.8945
Client 3 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.66 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.024485103034098, ce=1.8124337064900828
Local test acc @ epoch 39: 0.8933
Client 8 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.81 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.0246956315609292, ce=1.8126144905305042
Local test acc @ epoch 39: 0.8933
Client 2 execute local training on 5 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.45 seconds!
[tester] 
SST2Metric: acc=0.8944954128440367, hinge=2.0238823669219235, ce=1.8119114598257051
Local test acc @ epoch 39: 0.8945
Client 0 execute local training on 23 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.63 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.0666797796794034, ce=1.8512227933911969
Local test acc @ epoch 39: 0.8933
Client 9 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.36 seconds!
[tester] 
SST2Metric: acc=0.8944954128440367, hinge=2.0223876415589532, ce=1.8104512982277439
Local test acc @ epoch 39: 0.8945
Client 6 execute local training on 6 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.32 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.0249845563271722, ce=1.8129013388374415
Local test acc @ epoch 39: 0.8933
Client 5 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.41 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.02501645766267, ce=1.8129135787154789
Local test acc @ epoch 39: 0.8933
Client 7 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.37 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.0242172672387655, ce=1.8121918304932312
Local test acc @ epoch 39: 0.8933
Client 1 execute local training on 6 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.35 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.024686143895902, ce=1.8126374394000986
Local test acc @ epoch 39: 0.8933
Global evaluate on test data...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.18 seconds!
[tester] 
SST2Metric: acc=0.8944954128440367, hinge=2.0321915326041915, ce=1.819985180701161
Global test acc @ epoch 39: 0.8945
Global epoch 40...
Client 7 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.2 seconds!
[tester] 
SST2Metric: acc=0.8944954128440367, hinge=2.0307834033572347, ce=1.818764158065641
Local test acc @ epoch 40: 0.8945
Client 4 execute local training on 20 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.39 seconds!
[tester] 
SST2Metric: acc=0.8944954128440367, hinge=2.0667422553268047, ce=1.8526574055005338
Local test acc @ epoch 40: 0.8945
Client 0 execute local training on 23 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.28 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.0712381974546186, ce=1.855582334523933
Local test acc @ epoch 40: 0.8933
Client 8 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.26 seconds!
[tester] 
SST2Metric: acc=0.8944954128440367, hinge=2.0313544066947533, ce=1.819249420444521
Local test acc @ epoch 40: 0.8945
Client 2 execute local training on 5 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.18 seconds!
[tester] 
SST2Metric: acc=0.8944954128440367, hinge=2.030473997423408, ce=1.8185387836852447
Local test acc @ epoch 40: 0.8945
Client 5 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.12 seconds!
[tester] 
SST2Metric: acc=0.8944954128440367, hinge=2.0317589208620404, ce=1.8195265391158284
Local test acc @ epoch 40: 0.8945
Client 6 execute local training on 6 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.21 seconds!
[tester] 
SST2Metric: acc=0.8944954128440367, hinge=2.0316142573815967, ce=1.8195048984888496
Local test acc @ epoch 40: 0.8945
Client 9 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.26 seconds!
[tester] 
SST2Metric: acc=0.8944954128440367, hinge=2.029186692943267, ce=1.8171046692516464
Local test acc @ epoch 40: 0.8945
Client 1 execute local training on 6 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.14 seconds!
[tester] 
SST2Metric: acc=0.8944954128440367, hinge=2.03129580288852, ce=1.819239760181363
Local test acc @ epoch 40: 0.8945
Client 3 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.12 seconds!
[tester] 
SST2Metric: acc=0.8944954128440367, hinge=2.0311669700462884, ce=1.8190643473939994
Local test acc @ epoch 40: 0.8945
Global evaluate on test data...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.14 seconds!
[tester] 
SST2Metric: acc=0.8944954128440367, hinge=2.03852390914882, ce=1.8260109467136603
Global test acc @ epoch 40: 0.8945
Global epoch 41...
Client 7 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.14 seconds!
[tester] 
SST2Metric: acc=0.8944954128440367, hinge=2.0370833176538485, ce=1.8247638431433426
Local test acc @ epoch 41: 0.8945
Client 4 execute local training on 20 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.22 seconds!
[tester] 
SST2Metric: acc=0.8944954128440367, hinge=2.0704941537675507, ce=1.856155062119385
Local test acc @ epoch 41: 0.8945
Client 5 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.38 seconds!
[tester] 
SST2Metric: acc=0.8944954128440367, hinge=2.0380467626206373, ce=1.8255158168418275
Local test acc @ epoch 41: 0.8945
Client 8 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.03 seconds!
[tester] 
SST2Metric: acc=0.8944954128440367, hinge=2.0376497728562137, ce=1.8252406179464469
Local test acc @ epoch 41: 0.8945
Client 3 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.24 seconds!
[tester] 
SST2Metric: acc=0.8944954128440367, hinge=2.0374568443779553, ce=1.8250539825549
Local test acc @ epoch 41: 0.8945
Client 2 execute local training on 5 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.26 seconds!
[tester] 
SST2Metric: acc=0.8944954128440367, hinge=2.036754604873307, ce=1.8245165756416435
Local test acc @ epoch 41: 0.8945
Client 6 execute local training on 6 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.24 seconds!
[tester] 
SST2Metric: acc=0.8944954128440367, hinge=2.037896971226832, ce=1.825484655279583
Local test acc @ epoch 41: 0.8945
Client 1 execute local training on 6 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.33 seconds!
[tester] 
SST2Metric: acc=0.8944954128440367, hinge=2.0375748104458555, ce=1.8252180120639758
Local test acc @ epoch 41: 0.8945
Client 0 execute local training on 23 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.19 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.0755069146462537, ce=1.8597157377167164
Local test acc @ epoch 41: 0.8933
Client 9 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.05 seconds!
[tester] 
SST2Metric: acc=0.8944954128440367, hinge=2.035515217059249, ce=1.8231442788043686
Local test acc @ epoch 41: 0.8945
Global evaluate on test data...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.17 seconds!
[tester] 
SST2Metric: acc=0.8944954128440367, hinge=2.044255111742457, ce=1.8315303546707313
Global test acc @ epoch 41: 0.8945
Global epoch 42...
Client 0 execute local training on 23 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.18 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.0795064178902076, ce=1.8636333233618005
Local test acc @ epoch 42: 0.8922
Client 8 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.12 seconds!
[tester] 
SST2Metric: acc=0.8944954128440367, hinge=2.0433664721086484, ce=1.8307459764152034
Local test acc @ epoch 42: 0.8945
Client 1 execute local training on 6 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.35 seconds!
[tester] 
SST2Metric: acc=0.8944954128440367, hinge=2.043282996897304, ce=1.8307163898169778
Local test acc @ epoch 42: 0.8945
Client 5 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.45 seconds!
[tester] 
SST2Metric: acc=0.8944954128440367, hinge=2.0437561664559425, ce=1.8310205423510755
Local test acc @ epoch 42: 0.8945
Client 7 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.27 seconds!
[tester] 
SST2Metric: acc=0.8944954128440367, hinge=2.0428055973227965, ce=1.8302768608345148
Local test acc @ epoch 42: 0.8945
Client 3 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.3 seconds!
[tester] 
SST2Metric: acc=0.8944954128440367, hinge=2.0431689424525707, ce=1.8305602760908084
Local test acc @ epoch 42: 0.8945
Client 2 execute local training on 5 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.24 seconds!
[tester] 
SST2Metric: acc=0.8944954128440367, hinge=2.042468491223974, ce=1.8300168558829262
Local test acc @ epoch 42: 0.8945
Client 6 execute local training on 6 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.95 seconds!
[tester] 
SST2Metric: acc=0.8944954128440367, hinge=2.043610092149962, ce=1.8309875033316552
Local test acc @ epoch 42: 0.8945
Client 9 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.24 seconds!
[tester] 
SST2Metric: acc=0.8944954128440367, hinge=2.0412351312713883, ce=1.828667012605645
Local test acc @ epoch 42: 0.8945
Client 4 execute local training on 20 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.07 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.0741491263065863, ce=1.8596048203688993
Local test acc @ epoch 42: 0.8933
Global evaluate on test data...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.0 seconds!
[tester] 
SST2Metric: acc=0.8944954128440367, hinge=2.0498889701082073, ce=1.8366474032205686
Global test acc @ epoch 42: 0.8945
Global epoch 43...
Client 0 execute local training on 23 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.17 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.08328379307865, ce=1.8673757333292702
Local test acc @ epoch 43: 0.8922
Client 8 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.49 seconds!
[tester] 
SST2Metric: acc=0.8944954128440367, hinge=2.0489491705500753, ce=1.8358457832470718
Local test acc @ epoch 43: 0.8945
Client 5 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.21 seconds!
[tester] 
SST2Metric: acc=0.8944954128440367, hinge=2.0493522886562783, ce=1.836124590983786
Local test acc @ epoch 43: 0.8945
Client 7 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.15 seconds!
[tester] 
SST2Metric: acc=0.8944954128440367, hinge=2.048344264610098, ce=1.835365267025933
Local test acc @ epoch 43: 0.8945
Client 3 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.15 seconds!
[tester] 
SST2Metric: acc=0.8944954128440367, hinge=2.0487444004334443, ce=1.835657954593824
Local test acc @ epoch 43: 0.8945
Client 4 execute local training on 20 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.22 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.0776643177511493, ce=1.8629651862547327
Local test acc @ epoch 43: 0.8933
Client 6 execute local training on 6 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.21 seconds!
[tester] 
SST2Metric: acc=0.8944954128440367, hinge=2.049202944844141, ce=1.836094055218145
Local test acc @ epoch 43: 0.8945
Client 1 execute local training on 6 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.45 seconds!
[tester] 
SST2Metric: acc=0.8944954128440367, hinge=2.048873594731366, ce=1.8358202199234814
Local test acc @ epoch 43: 0.8945
Client 2 execute local training on 5 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.47 seconds!
[tester] 
SST2Metric: acc=0.8944954128440367, hinge=2.048003906230314, ce=1.8351166523095244
Local test acc @ epoch 43: 0.8945
Client 9 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.64 seconds!
[tester] 
SST2Metric: acc=0.8944954128440367, hinge=2.0468652661240427, ce=1.833812325620267
Local test acc @ epoch 43: 0.8945
Global evaluate on test data...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.39 seconds!
[tester] 
SST2Metric: acc=0.8944954128440367, hinge=2.0551126093492593, ce=1.8414298299396896
Global test acc @ epoch 43: 0.8945
Global epoch 44...
Client 1 execute local training on 6 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.2 seconds!
[tester] 
SST2Metric: acc=0.8944954128440367, hinge=2.0540449369117755, ce=1.8405532125737818
Local test acc @ epoch 44: 0.8945
Client 9 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.5 seconds!
[tester] 
SST2Metric: acc=0.8944954128440367, hinge=2.0520597826176825, ce=1.8385805297711366
Local test acc @ epoch 44: 0.8945
Client 3 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.57 seconds!
[tester] 
SST2Metric: acc=0.8944954128440367, hinge=2.0539325977410745, ce=1.8404091995983678
Local test acc @ epoch 44: 0.8945
Client 2 execute local training on 5 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.75 seconds!
[tester] 
SST2Metric: acc=0.8944954128440367, hinge=2.053180459032365, ce=1.8398492950782015
Local test acc @ epoch 44: 0.8945
Client 5 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.46 seconds!
[tester] 
SST2Metric: acc=0.8944954128440367, hinge=2.054534513469136, ce=1.8408748173273792
Local test acc @ epoch 44: 0.8945
Client 6 execute local training on 6 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.39 seconds!
[tester] 
SST2Metric: acc=0.8944954128440367, hinge=2.0543766173474287, ce=1.8408282381532486
Local test acc @ epoch 44: 0.8945
Client 4 execute local training on 20 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.67 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.0810506402625952, ce=1.8662441384095203
Local test acc @ epoch 44: 0.8922
Client 8 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.56 seconds!
[tester] 
SST2Metric: acc=0.8944954128440367, hinge=2.0541570029400904, ce=1.8406109760383422
Local test acc @ epoch 44: 0.8945
Client 7 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.4 seconds!
[tester] 
SST2Metric: acc=0.8944954128440367, hinge=2.0535381771282317, ce=1.840119158512295
Local test acc @ epoch 44: 0.8945
Client 0 execute local training on 23 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.38 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.0868698681166413, ce=1.8709646601925403
Local test acc @ epoch 44: 0.8922
Global evaluate on test data...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.7 seconds!
[tester] 
SST2Metric: acc=0.8944954128440367, hinge=2.0599393465923606, ce=1.8459098807869723
Global test acc @ epoch 44: 0.8945
Global epoch 45...
Client 7 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.52 seconds!
[tester] 
SST2Metric: acc=0.8944954128440367, hinge=2.0583299332802447, ce=1.8445637898969829
Local test acc @ epoch 45: 0.8945
Client 3 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.29 seconds!
[tester] 
SST2Metric: acc=0.8944954128440367, hinge=2.058740229098075, ce=1.8448744077309474
Local test acc @ epoch 45: 0.8945
Client 6 execute local training on 6 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.48 seconds!
[tester] 
SST2Metric: acc=0.8944954128440367, hinge=2.0591875739874097, ce=1.8452907863906733
Local test acc @ epoch 45: 0.8945
Client 0 execute local training on 23 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.5 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.090241826182112, ce=1.8743709855105983
Local test acc @ epoch 45: 0.8922
Client 8 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.5 seconds!
[tester] 
SST2Metric: acc=0.8944954128440367, hinge=2.0589679869216515, ce=1.8450751940645425
Local test acc @ epoch 45: 0.8945
Client 2 execute local training on 5 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.38 seconds!
[tester] 
SST2Metric: acc=0.8944954128440367, hinge=2.0580014497588532, ce=1.8443191589709405
Local test acc @ epoch 45: 0.8945
Client 1 execute local training on 6 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.27 seconds!
[tester] 
SST2Metric: acc=0.8944954128440367, hinge=2.0588668813125803, ce=1.8450276827612906
Local test acc @ epoch 45: 0.8945
Client 4 execute local training on 20 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.41 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.0843184074950876, ce=1.869446381834378
Local test acc @ epoch 45: 0.8922
Client 5 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.44 seconds!
[tester] 
SST2Metric: acc=0.8944954128440367, hinge=2.059374951988185, ce=1.8453742902734709
Local test acc @ epoch 45: 0.8945
Client 9 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.79 seconds!
[tester] 
SST2Metric: acc=0.8944954128440367, hinge=2.0569149556509947, ce=1.843100583186197
Local test acc @ epoch 45: 0.8945
Global evaluate on test data...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.63 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.064446425766026, ce=1.8501520643070426
Global test acc @ epoch 45: 0.8933
Global epoch 46...
Client 8 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.05 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.0634513218468484, ce=1.8492918921405108
Local test acc @ epoch 46: 0.8933
Client 9 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.1 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.0614025040504034, ce=1.8473351289987796
Local test acc @ epoch 46: 0.8933
Client 7 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.2 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.0628162392782508, ce=1.8487845155559262
Local test acc @ epoch 46: 0.8933
Client 6 execute local training on 6 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.34 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.063675605785956, ce=1.8495146436110597
Local test acc @ epoch 46: 0.8933
Client 5 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.91 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.063831389223764, ce=1.8495735366507593
Local test acc @ epoch 46: 0.8933
Client 2 execute local training on 5 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.83 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.0624805947235965, ce=1.8485282135976813
Local test acc @ epoch 46: 0.8933
Client 4 execute local training on 20 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.32 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.087445765597011, ce=1.872546607435615
Local test acc @ epoch 46: 0.8922
Client 1 execute local training on 6 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.42 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.0633521639152406, ce=1.849247527335318
Local test acc @ epoch 46: 0.8933
Client 3 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.92 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.063241735100746, ce=1.8491148639841715
Local test acc @ epoch 46: 0.8933
Client 0 execute local training on 23 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.91 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.0934651170028458, ce=1.8776556200037553
Local test acc @ epoch 46: 0.8922
Global evaluate on test data...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.04 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.0686564791366595, ce=1.854169944755616
Global test acc @ epoch 46: 0.8933
Global epoch 47...
Client 3 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.91 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.067420238475187, ce=1.8531061460936789
Local test acc @ epoch 47: 0.8933
Client 9 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.94 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.065620802274538, ce=1.8513684468436185
Local test acc @ epoch 47: 0.8933
Client 0 execute local training on 23 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.86 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.096510490693084, ce=1.8807849498277796
Local test acc @ epoch 47: 0.8922
Client 6 execute local training on 6 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.82 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.0678859608709264, ce=1.8535314695516916
Local test acc @ epoch 47: 0.8933
Client 2 execute local training on 5 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.39 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.0666878034762286, ce=1.8525393657190972
Local test acc @ epoch 47: 0.8933
Client 7 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.14 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.067017074435129, ce=1.8527927948306362
Local test acc @ epoch 47: 0.8933
Client 4 execute local training on 20 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.96 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.0904726317716302, ce=1.8755763759412982
Local test acc @ epoch 47: 0.8922
Client 1 execute local training on 6 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.07 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.0675614066080215, ce=1.85326320589998
Local test acc @ epoch 47: 0.8933
Client 8 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.07 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.0676698526111217, ce=1.8533171533929471
Local test acc @ epoch 47: 0.8933
Client 5 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.99 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.0680496700859945, ce=1.853605983350317
Local test acc @ epoch 47: 0.8933
Global evaluate on test data...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.29 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.0726187543584667, ce=1.858004471042799
Global test acc @ epoch 47: 0.8933
Global epoch 48...
Client 2 execute local training on 5 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.25 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.0706255792751227, ce=1.856343563720203
Local test acc @ epoch 48: 0.8933
Client 3 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.13 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.0713756754310855, ce=1.8569328576574937
Local test acc @ epoch 48: 0.8933
Client 7 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.06 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.0709500629967508, ce=1.8565957386634218
Local test acc @ epoch 48: 0.8933
Client 8 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.14 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.0716017376665676, ce=1.8571187259366984
Local test acc @ epoch 48: 0.8933
Client 6 execute local training on 6 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.95 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.0718262407484405, ce=1.8573406935235428
Local test acc @ epoch 48: 0.8933
Client 0 execute local training on 23 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.18 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.099429901313344, ce=1.8838058961209392
Local test acc @ epoch 48: 0.8922
Client 5 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.46 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.0719832548307715, ce=1.8574157947156833
Local test acc @ epoch 48: 0.8933
Client 9 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.13 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.0696065997311828, ce=1.8552318587934218
Local test acc @ epoch 48: 0.8933
Client 4 execute local training on 20 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.16 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.0934747993399245, ce=1.8785127910101247
Local test acc @ epoch 48: 0.8922
Client 1 execute local training on 6 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.18 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.0715089522370507, ce=1.8570813162715798
Local test acc @ epoch 48: 0.8933
Global evaluate on test data...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.08 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.0763522508220933, ce=1.8616626878821556
Global test acc @ epoch 48: 0.8922
Global epoch 49...
Client 6 execute local training on 6 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.18 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.0755505185881886, ce=1.860989010071256
Local test acc @ epoch 49: 0.8922
Client 1 execute local training on 6 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.98 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.0752081500554302, ce=1.8607054661567193
Local test acc @ epoch 49: 0.8922
Client 5 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.92 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.075693148538607, ce=1.8610544103033293
Local test acc @ epoch 49: 0.8922
Client 4 execute local training on 20 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.88 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.096411267688515, ce=1.8813544015727315
Local test acc @ epoch 49: 0.8922
Client 3 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.15 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.0751036484853937, ce=1.8605882374272877
Local test acc @ epoch 49: 0.8922
Client 2 execute local training on 5 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.09 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.0743607151398966, ce=1.85999741446526
Local test acc @ epoch 49: 0.8922
Client 9 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.22 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.0733460689629983, ce=1.8589012818320259
Local test acc @ epoch 49: 0.8922
Client 0 execute local training on 23 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.88 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.1022241659667515, ce=1.8867166119749001
Local test acc @ epoch 49: 0.8922
Client 7 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.07 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.074691333081744, ce=1.8602591645723376
Local test acc @ epoch 49: 0.8922
Client 8 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.27 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.0753484268527513, ce=1.860787563770023
Local test acc @ epoch 49: 0.8922
Global evaluate on test data...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.14 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.079889099805727, ce=1.8651697829672522
Global test acc @ epoch 49: 0.8922
Global epoch 50...
Client 0 execute local training on 23 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.02 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.1048868195999653, ce=1.8895080633520986
Local test acc @ epoch 50: 0.8922
Client 1 execute local training on 6 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.05 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.07874583159018, ce=1.864210246005302
Local test acc @ epoch 50: 0.8922
Client 9 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.01 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.076895329656951, ce=1.8624243905077866
Local test acc @ epoch 50: 0.8922
Client 3 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.96 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.078629140460163, ce=1.8640854672127074
Local test acc @ epoch 50: 0.8922
Client 6 execute local training on 6 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.96 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.0790880532986526, ce=1.8644954770214466
Local test acc @ epoch 50: 0.8922
Client 4 execute local training on 20 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.14 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.0992991336441915, ce=1.884131084146511
Local test acc @ epoch 50: 0.8922
Client 2 execute local training on 5 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.07 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.0778944306417344, ce=1.8634972283986602
Local test acc @ epoch 50: 0.8922
Client 5 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.99 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.0792329744734896, ce=1.8645684221474323
Local test acc @ epoch 50: 0.8922
Client 7 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.21 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.0782111779265446, ce=1.8637481399062334
Local test acc @ epoch 50: 0.8922
Client 8 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.29 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.078871737926378, ce=1.864279940522111
Local test acc @ epoch 50: 0.8922
Global evaluate on test data...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.21 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.083245078768205, ce=1.8685345105398223
Global test acc @ epoch 50: 0.8922
Global epoch 51...
Client 7 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.21 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.081556566264651, ce=1.8670784466650046
Local test acc @ epoch 51: 0.8922
Client 4 execute local training on 20 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.0 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.1020637895535987, ce=1.8868059356345683
Local test acc @ epoch 51: 0.8922
Client 3 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.01 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.0819742916100616, ce=1.8674407164650992
Local test acc @ epoch 51: 0.8922
Client 1 execute local training on 6 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.9 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.0820916374342158, ce=1.8675649149710123
Local test acc @ epoch 51: 0.8922
Client 5 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.21 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.0825703036074246, ce=1.8679166558248026
Local test acc @ epoch 51: 0.8922
Client 2 execute local training on 5 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.06 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.0812966610313555, ce=1.8668457865324661
Local test acc @ epoch 51: 0.8922
Client 9 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.96 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.080377084672998, ce=1.8658062089908751
Local test acc @ epoch 51: 0.8922
Client 6 execute local training on 6 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.94 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.0824336174431197, ce=1.8678489086544343
Local test acc @ epoch 51: 0.8922
Client 0 execute local training on 23 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.84 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.1074583381414413, ce=1.8922188356001428
Local test acc @ epoch 51: 0.8922
Client 8 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.78 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.082220891746906, ce=1.8676370172342478
Local test acc @ epoch 51: 0.8922
Global evaluate on test data...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.96 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.086464234585062, ce=1.8717615333030155
Global test acc @ epoch 51: 0.8922
Global epoch 52...
Client 3 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.83 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.0852840020569094, ce=1.8706547031307201
Local test acc @ epoch 52: 0.8922
Client 9 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.85 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.0837092333977374, ce=1.8690459910004116
Local test acc @ epoch 52: 0.8922
Client 0 execute local training on 23 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.92 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.1099074138413876, ce=1.8948160871738875
Local test acc @ epoch 52: 0.8922
Client 2 execute local training on 5 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.95 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.0846194421206046, ce=1.8700682800936075
Local test acc @ epoch 52: 0.8922
Client 5 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.81 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.085808566677461, ce=1.871132887141821
Local test acc @ epoch 52: 0.8922
Client 8 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.84 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.0854994188481513, ce=1.8708519752186659
Local test acc @ epoch 52: 0.8922
Client 7 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.83 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.084884871583466, ce=1.8703079764683501
Local test acc @ epoch 52: 0.8922
Client 1 execute local training on 6 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.86 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.0854123534139144, ce=1.8707879520590371
Local test acc @ epoch 52: 0.8922
Client 6 execute local training on 6 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.72 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.085709312503491, ce=1.8710707779966451
Local test acc @ epoch 52: 0.8922
Client 4 execute local training on 20 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.75 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.104751642827594, ce=1.88942221663573
Local test acc @ epoch 52: 0.8922
Global evaluate on test data...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.61 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.089632781547144, ce=1.8748644749219934
Global test acc @ epoch 52: 0.8922
Global epoch 53...
Client 4 execute local training on 20 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.82 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.1073318413638193, ce=1.8919460331665119
Local test acc @ epoch 53: 0.8922
Client 7 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.86 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.088046038642936, ce=1.8734036457090844
Local test acc @ epoch 53: 0.8922
Client 5 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.51 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.0889661539038387, ce=1.8742272053940097
Local test acc @ epoch 53: 0.8922
Client 9 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.75 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.0869054725957574, ce=1.8721806801798424
Local test acc @ epoch 53: 0.8922
Client 6 execute local training on 6 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.75 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.0888755664639516, ce=1.874170012984718
Local test acc @ epoch 53: 0.8922
Client 8 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.55 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.0886605399737666, ce=1.8739471007238167
Local test acc @ epoch 53: 0.8922
Client 3 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.85 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.0884565087633393, ce=1.8737623336777436
Local test acc @ epoch 53: 0.8922
Client 2 execute local training on 5 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.9 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.0877815517810507, ce=1.873163342418356
Local test acc @ epoch 53: 0.8922
Client 0 execute local training on 23 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.75 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.112340782487064, ce=1.8973386503586365
Local test acc @ epoch 53: 0.8922
Client 1 execute local training on 6 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.82 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.088581597039459, ce=1.8738913437884115
Local test acc @ epoch 53: 0.8922
Global evaluate on test data...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.77 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.0926658180328683, ce=1.877857366244071
Global test acc @ epoch 53: 0.8922
Global epoch 54...
Client 1 execute local training on 6 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.9 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.0915923076211858, ce=1.8768635501259017
Local test acc @ epoch 54: 0.8922
Client 5 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.97 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.091977551032644, ce=1.8772009346794436
Local test acc @ epoch 54: 0.8922
Client 2 execute local training on 5 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.85 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.090804553770144, ce=1.8761462581201334
Local test acc @ epoch 54: 0.8922
Client 0 execute local training on 23 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.01 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.114727677008428, ce=1.8997718712481628
Local test acc @ epoch 54: 0.8922
Client 4 execute local training on 20 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.02 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.1098368267673964, ce=1.8944103441791802
Local test acc @ epoch 54: 0.8922
Client 8 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.18 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.091696916233509, ce=1.8769437314979676
Local test acc @ epoch 54: 0.8922
Client 3 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.19 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.091489742245149, ce=1.8767585197978405
Local test acc @ epoch 54: 0.8922
Client 7 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.96 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.0910739454380964, ce=1.8763924801456118
Local test acc @ epoch 54: 0.8922
Client 9 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.87 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.08995626962513, ce=1.8751967893620565
Local test acc @ epoch 54: 0.8922
Client 6 execute local training on 6 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.95 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.091910358142415, ce=1.8771651162254117
Local test acc @ epoch 54: 0.8922
Global evaluate on test data...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.9 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.095570009235942, ce=1.880745678408954
Global test acc @ epoch 54: 0.8922
Global epoch 55...
Client 4 execute local training on 20 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.13 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.112248482644011, ce=1.8967930236711283
Local test acc @ epoch 55: 0.8922
Client 6 execute local training on 6 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.13 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.0947984426940254, ce=1.8800380440153543
Local test acc @ epoch 55: 0.8922
Client 8 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.1 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.0945867110009586, ce=1.8798167096456306
Local test acc @ epoch 55: 0.8922
Client 2 execute local training on 5 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.04 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.093716425359796, ce=1.8790413499055596
Local test acc @ epoch 55: 0.8922
Client 5 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.05 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.0948817738152425, ce=1.8800902141785318
Local test acc @ epoch 55: 0.8922
Client 7 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.16 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.09396378682294, ce=1.8792667818327526
Local test acc @ epoch 55: 0.8922
Client 3 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.08 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.094375116015793, ce=1.8796295936725982
Local test acc @ epoch 55: 0.8922
Client 9 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.07 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.092867396566846, ce=1.8780958448881635
Local test acc @ epoch 55: 0.8922
Client 1 execute local training on 6 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.25 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.094497383734502, ce=1.8797531365402267
Local test acc @ epoch 55: 0.8922
Client 0 execute local training on 23 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.17 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.117061965509292, ce=1.902132937539179
Local test acc @ epoch 55: 0.8922
Global evaluate on test data...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.3 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.0983532412883337, ce=1.8835307048129093
Global test acc @ epoch 55: 0.8922
Global epoch 56...
Client 2 execute local training on 5 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.33 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.096491604223164, ce=1.8818192436564083
Local test acc @ epoch 56: 0.8922
Client 5 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.07 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.0976443378203506, ce=1.8828570583643074
Local test acc @ epoch 56: 0.8922
Client 9 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.06 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.0956583498814783, ce=1.8808936892679136
Local test acc @ epoch 56: 0.8922
Client 7 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.14 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.09673218989591, ce=1.8820394445175788
Local test acc @ epoch 56: 0.8922
Client 8 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.27 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.0973593984995413, ce=1.882591930544158
Local test acc @ epoch 56: 0.8922
Client 3 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.95 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.097146190907977, ce=1.8824054375690555
Local test acc @ epoch 56: 0.8922
Client 1 execute local training on 6 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.09 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.09726982078421, ce=1.8825287440325356
Local test acc @ epoch 56: 0.8922
Client 6 execute local training on 6 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.27 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.0975676803687295, ce=1.8828097676904607
Local test acc @ epoch 56: 0.8922
Client 4 execute local training on 20 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.35 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.114583162402888, ce=1.8991096999492412
Local test acc @ epoch 56: 0.8922
Client 0 execute local training on 23 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.9 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.1193670333798873, ce=1.9044210606225707
Local test acc @ epoch 56: 0.8922
Global evaluate on test data...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.05 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.1010451370149577, ce=1.8862151562550136
Global test acc @ epoch 56: 0.8922
Global epoch 57...
Client 2 execute local training on 5 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.05 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.099227689685078, ce=1.8845071850477744
Local test acc @ epoch 57: 0.8922
Client 5 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.99 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.1003022239022298, ce=1.8855349757177893
Local test acc @ epoch 57: 0.8922
Client 1 execute local training on 6 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.19 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.100010430730811, ce=1.8852128802273012
Local test acc @ epoch 57: 0.8922
Client 9 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.18 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.09835018518321, ce=1.8836008546637182
Local test acc @ epoch 57: 0.8922
Client 0 execute local training on 23 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.18 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.121581380520392, ce=1.9066268656643273
Local test acc @ epoch 57: 0.8933
Client 4 execute local training on 20 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.13 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.1168535154893857, ce=1.9013708925359643
Local test acc @ epoch 57: 0.8922
Client 8 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.23 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.100075185845751, ce=1.8852899647699575
Local test acc @ epoch 57: 0.8922
Client 3 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.19 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.0998914047665553, ce=1.8851081354601584
Local test acc @ epoch 57: 0.8922
Client 7 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.16 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.099485364255555, ce=1.8847394774641621
Local test acc @ epoch 57: 0.8922
Client 6 execute local training on 6 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.02 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.1002908376378753, ce=1.8855111257270856
Local test acc @ epoch 57: 0.8922
Global evaluate on test data...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.16 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.10369529704982, ce=1.8888168937593635
Global test acc @ epoch 57: 0.8922
Global epoch 58...
Client 5 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.29 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.1029351521796045, ce=1.8881342943138602
Local test acc @ epoch 58: 0.8922
Client 7 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.33 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.1021186787327495, ce=1.8873244242518399
Local test acc @ epoch 58: 0.8922
Client 3 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.22 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.102526595422981, ce=1.887695686017318
Local test acc @ epoch 58: 0.8922
Client 0 execute local training on 23 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.03 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.1237332946663603, ce=1.9087781370673484
Local test acc @ epoch 58: 0.8933
Client 2 execute local training on 5 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.09 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.101892939538037, ce=1.8871234057706066
Local test acc @ epoch 58: 0.8922
Client 8 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.82 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.1027187378308096, ce=1.887885414378529
Local test acc @ epoch 58: 0.8922
Client 6 execute local training on 6 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.19 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.1029333770275116, ce=1.8881050551742218
Local test acc @ epoch 58: 0.8922
Client 4 execute local training on 20 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.15 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.1190471755802083, ce=1.9035655922859591
Local test acc @ epoch 58: 0.8922
Client 1 execute local training on 6 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.17 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.102674786924222, ce=1.8878285783176556
Local test acc @ epoch 58: 0.8922
Client 9 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.46 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.1010396577622914, ce=1.8862433498334057
Local test acc @ epoch 58: 0.8922
Global evaluate on test data...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.22 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.106252406988669, ce=1.8913379303266205
Global test acc @ epoch 58: 0.8922
Global epoch 59...
Client 6 execute local training on 6 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.04 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.1054803422831614, ce=1.8906171243695333
Local test acc @ epoch 59: 0.8922
Client 7 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.09 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.1046755215443604, ce=1.8898472524249834
Local test acc @ epoch 59: 0.8922
Client 3 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.09 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.105079935504756, ce=1.8902166068871031
Local test acc @ epoch 59: 0.8922
Client 5 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.3 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.10548393209593, ce=1.8906486593151237
Local test acc @ epoch 59: 0.8922
Client 8 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.14 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.1052670172595103, ce=1.890397962667355
Local test acc @ epoch 59: 0.8922
Client 0 execute local training on 23 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.14 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.125816106249433, ce=1.9108687193480878
Local test acc @ epoch 59: 0.8933
Client 9 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.08 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.103607334948461, ce=1.8887784304583468
Local test acc @ epoch 59: 0.8922
Client 4 execute local training on 20 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.21 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.1211774362883435, ce=1.9057052348457257
Local test acc @ epoch 59: 0.8922
Client 2 execute local training on 5 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.02 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.1044591248308846, ce=1.8896560749430473
Local test acc @ epoch 59: 0.8922
Client 1 execute local training on 6 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.19 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.1052309518013526, ce=1.8903506559427292
Local test acc @ epoch 59: 0.8922
Global evaluate on test data...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.1 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.1087133315725066, ce=1.8937768634617225
Global test acc @ epoch 59: 0.8922
Global epoch 60...
Client 7 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.02 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.1071350331153345, ce=1.8922864883702344
Local test acc @ epoch 60: 0.8922
Client 2 execute local training on 5 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.24 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.106914611311134, ce=1.8920891084848797
Local test acc @ epoch 60: 0.8922
Client 6 execute local training on 6 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.2 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.1079414454075174, ce=1.8930561813867877
Local test acc @ epoch 60: 0.8922
Client 4 execute local training on 20 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.29 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.1232330274144444, ce=1.9077775654081255
Local test acc @ epoch 60: 0.8922
Client 0 execute local training on 23 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.07 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.127841239128638, ce=1.912907209207759
Local test acc @ epoch 60: 0.8933
Client 8 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.25 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.1077459872862616, ce=1.8928535132713906
Local test acc @ epoch 60: 0.8922
Client 9 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.08 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.1060997634579284, ce=1.8912494676758624
Local test acc @ epoch 60: 0.8922
Client 3 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.06 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.1075589531058565, ce=1.8926742999716244
Local test acc @ epoch 60: 0.8922
Client 1 execute local training on 6 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.16 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.107693819277877, ce=1.8927932376183625
Local test acc @ epoch 60: 0.8922
Client 5 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.39 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.107955275735724, ce=1.8930984341061112
Local test acc @ epoch 60: 0.8922
Global evaluate on test data...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.29 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.1110942060247475, ce=1.896145068339643
Global test acc @ epoch 60: 0.8922
Global epoch 61...
Client 7 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.44 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.1095181992021175, ce=1.8946599257255046
Local test acc @ epoch 61: 0.8933
Client 2 execute local training on 5 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.05 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.109302628477779, ce=1.8944665770832645
Local test acc @ epoch 61: 0.8933
Client 4 execute local training on 20 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.25 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.125240487789889, ce=1.9098073811201839
Local test acc @ epoch 61: 0.8922
Client 3 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.97 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.10993301458315, ce=1.8950377135778458
Local test acc @ epoch 61: 0.8933
Client 6 execute local training on 6 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.02 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.1103300841849877, ce=1.8954334723409814
Local test acc @ epoch 61: 0.8933
Client 1 execute local training on 6 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.0 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.1100709036402745, ce=1.8951583424545633
Local test acc @ epoch 61: 0.8933
Client 9 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.05 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.1084933622714574, ce=1.8936335733097516
Local test acc @ epoch 61: 0.8933
Client 5 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.27 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.1103255960646026, ce=1.8954570072578718
Local test acc @ epoch 61: 0.8933
Client 8 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.15 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.110121157327923, ce=1.895217128129106
Local test acc @ epoch 61: 0.8933
Client 0 execute local training on 23 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.04 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.129803027855147, ce=1.9148876958335532
Local test acc @ epoch 61: 0.8933
Global evaluate on test data...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.16 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.1133948373138356, ce=1.898443351437353
Global test acc @ epoch 61: 0.8933
Global epoch 62...
Client 7 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.22 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.1118173779697593, ce=1.8969587186561148
Local test acc @ epoch 62: 0.8933
Client 1 execute local training on 6 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.25 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.112364028025111, ce=1.8974512617107044
Local test acc @ epoch 62: 0.8933
Client 5 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.28 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.112615825933054, ce=1.8977461626424488
Local test acc @ epoch 62: 0.8933
Client 3 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.02 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.112230190716752, ce=1.8973345936649355
Local test acc @ epoch 62: 0.8933
Client 2 execute local training on 5 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.1 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.111607518628103, ce=1.8967714763163324
Local test acc @ epoch 62: 0.8933
Client 4 execute local training on 20 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.12 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.127188543387509, ce=1.911784899233897
Local test acc @ epoch 62: 0.8933
Client 9 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.08 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.11081664988754, ce=1.8959562582658327
Local test acc @ epoch 62: 0.8933
Client 0 execute local training on 23 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.23 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.131705210296386, ce=1.9168145150808018
Local test acc @ epoch 62: 0.8933
Client 8 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.4 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.112428492635762, ce=1.8975229341088284
Local test acc @ epoch 62: 0.8933
Client 6 execute local training on 6 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.27 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.112637978764849, ce=1.8977401422538687
Local test acc @ epoch 62: 0.8933
Global evaluate on test data...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.35 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.1156215317752385, ce=1.9006765027948538
Global test acc @ epoch 62: 0.8933
Global epoch 63...
Client 7 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.21 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.1140488356078437, ce=1.8991983885984407
Local test acc @ epoch 63: 0.8933
Client 9 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.24 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.113058487072997, ce=1.8982051763646524
Local test acc @ epoch 63: 0.8933
Client 1 execute local training on 6 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.32 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.114587674310448, ce=1.8996830956457051
Local test acc @ epoch 63: 0.8933
Client 0 execute local training on 23 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.5 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.133546975364379, ce=1.918686727264139
Local test acc @ epoch 63: 0.8933
Client 5 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.5 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.1148422916821383, ce=1.899980826938445
Local test acc @ epoch 63: 0.8933
Client 6 execute local training on 6 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.28 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.1148572007723905, ce=1.899966491677621
Local test acc @ epoch 63: 0.8933
Client 8 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.31 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.1146527163752724, ce=1.8997539362319276
Local test acc @ epoch 63: 0.8933
Client 4 execute local training on 20 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.68 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.129086801765162, ce=1.9137194396621284
Local test acc @ epoch 63: 0.8933
Client 3 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.36 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.114472245267772, ce=1.8995842616504146
Local test acc @ epoch 63: 0.8933
Client 2 execute local training on 5 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.81 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.1138545688959436, ce=1.8990266015886956
Local test acc @ epoch 63: 0.8933
Global evaluate on test data...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.61 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.117777676483907, ce=1.9028462042524719
Global test acc @ epoch 63: 0.8933
Global epoch 64...
Client 8 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.44 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.11680027951888, ce=1.9019165652913648
Local test acc @ epoch 64: 0.8933
Client 5 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.5 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.116986745267833, ce=1.9021390582375104
Local test acc @ epoch 64: 0.8933
Client 1 execute local training on 6 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.39 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.1167528756441327, ce=1.9018626667156868
Local test acc @ epoch 64: 0.8933
Client 4 execute local training on 20 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.06 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.1309260202383777, ce=1.9155972165416382
Local test acc @ epoch 64: 0.8933
Client 0 execute local training on 23 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.08 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.1353472201922616, ce=1.9205218621168743
Local test acc @ epoch 64: 0.8933
Client 7 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.08 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.116219890773843, ce=1.9013855992602626
Local test acc @ epoch 64: 0.8933
Client 9 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.32 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.115248874786797, ce=1.9004129010452149
Local test acc @ epoch 64: 0.8933
Client 2 execute local training on 5 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.27 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.116022600356592, ce=1.9012097133620698
Local test acc @ epoch 64: 0.8933
Client 6 execute local training on 6 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.18 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.1170244633878044, ce=1.902148580424387
Local test acc @ epoch 64: 0.8933
Client 3 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.08 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.116627737879753, ce=1.9017562948169353
Local test acc @ epoch 64: 0.8933
Global evaluate on test data...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.13 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.119871626890034, ce=1.9049614091253
Global test acc @ epoch 64: 0.8933
Global epoch 65...
Client 5 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.02 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.1190807584228866, ce=1.9042545522567644
Local test acc @ epoch 65: 0.8933
Client 6 execute local training on 6 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.99 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.119105419024415, ce=1.904251073819966
Local test acc @ epoch 65: 0.8933
Client 9 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.17 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.117354302368033, ce=1.9025403663947376
Local test acc @ epoch 65: 0.8933
Client 2 execute local training on 5 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.16 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.1181066432677276, ce=1.9033177225651687
Local test acc @ epoch 65: 0.8933
Client 8 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.22 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.1188979870682463, ce=1.9040351172728973
Local test acc @ epoch 65: 0.8933
Client 4 execute local training on 20 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.25 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.132718786323836, ce=1.9174354142914616
Local test acc @ epoch 65: 0.8933
Client 1 execute local training on 6 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.12 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.1188536614453026, ce=1.903986921541699
Local test acc @ epoch 65: 0.8933
Client 7 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.06 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.1183078874141796, ce=1.9034981592545444
Local test acc @ epoch 65: 0.8933
Client 0 execute local training on 23 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.16 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.137094315859156, ce=1.9223080043837137
Local test acc @ epoch 65: 0.8933
Client 3 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.06 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.118725288488449, ce=1.903877187384501
Local test acc @ epoch 65: 0.8933
Global evaluate on test data...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.07 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.121898067517018, ce=1.9070156087981094
Global test acc @ epoch 65: 0.8933
Global epoch 66...
Client 5 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.13 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.121102739091313, ce=1.9063051002927929
Local test acc @ epoch 66: 0.8933
Client 3 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.15 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.1207420834160726, ce=1.9059227558890428
Local test acc @ epoch 66: 0.8933
Client 4 execute local training on 20 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.18 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.134468776780531, ce=1.9192338086697516
Local test acc @ epoch 66: 0.8933
Client 0 execute local training on 23 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.1 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.1387948150481653, ce=1.924051253256857
Local test acc @ epoch 66: 0.8933
Client 9 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.02 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.1194156285272827, ce=1.9046300497194657
Local test acc @ epoch 66: 0.8933
Client 6 execute local training on 6 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.28 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.121149008837315, ce=1.906323777239181
Local test acc @ epoch 66: 0.8933
Client 2 execute local training on 5 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.07 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.1201552570959845, ce=1.9053957546159412
Local test acc @ epoch 66: 0.8933
Client 7 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.31 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.12034210562706, ce=1.9055623476288657
Local test acc @ epoch 66: 0.8933
Client 8 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.26 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.120940116567349, ce=1.906105506395048
Local test acc @ epoch 66: 0.8933
Client 1 execute local training on 6 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.18 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.1208905369863595, ce=1.9060521080418813
Local test acc @ epoch 66: 0.8933
Global evaluate on test data...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.0 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.1238719137436752, ce=1.9090228440638362
Global test acc @ epoch 66: 0.8933
Global epoch 67...
Client 5 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.04 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.1230736657840397, ce=1.9083096304932436
Local test acc @ epoch 67: 0.8933
Client 8 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.12 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.122903859670009, ce=1.908103095502578
Local test acc @ epoch 67: 0.8933
Client 2 execute local training on 5 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.1 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.122126403204892, ce=1.9074015851734087
Local test acc @ epoch 67: 0.8933
Client 0 execute local training on 23 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.18 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.1404485709350043, ce=1.9257526067221744
Local test acc @ epoch 67: 0.8933
Client 1 execute local training on 6 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.16 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.122861888567242, ce=1.908058305844682
Local test acc @ epoch 67: 0.8933
Client 3 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.35 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.122725287982083, ce=1.9079413195370791
Local test acc @ epoch 67: 0.8933
Client 6 execute local training on 6 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.46 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.123120187345995, ce=1.908328329578193
Local test acc @ epoch 67: 0.8933
Client 9 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.17 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.121402713683767, ce=1.906652777083664
Local test acc @ epoch 67: 0.8933
Client 7 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.06 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.1223162542515936, ce=1.9075713493648072
Local test acc @ epoch 67: 0.8933
Client 4 execute local training on 20 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.03 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.1361786818832433, ce=1.9209980944734781
Local test acc @ epoch 67: 0.8933
Global evaluate on test data...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.17 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.1257829191761277, ce=1.9109727873087652
Global test acc @ epoch 67: 0.8933
Global epoch 68...
Client 6 execute local training on 6 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.88 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.125021518507135, ce=1.910270846459866
Local test acc @ epoch 68: 0.8933
Client 0 execute local training on 23 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.07 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.142060839254922, ce=1.9274158700844544
Local test acc @ epoch 68: 0.8933
Client 9 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.08 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.1233368248020836, ce=1.9086272964190312
Local test acc @ epoch 68: 0.8933
Client 5 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.25 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.1249930109452766, ce=1.910267807015725
Local test acc @ epoch 68: 0.8933
Client 8 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.12 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.124826027029151, ce=1.910064338245091
Local test acc @ epoch 68: 0.8933
Client 3 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.02 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.124640301280065, ce=1.9098970527959227
Local test acc @ epoch 68: 0.8933
Client 1 execute local training on 6 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.98 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.1247787229511714, ce=1.9100158998494496
Local test acc @ epoch 68: 0.8933
Client 4 execute local training on 20 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.11 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.137839845834522, ce=1.9227164857176675
Local test acc @ epoch 68: 0.8933
Client 2 execute local training on 5 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.04 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.1240598538873394, ce=1.9093762406715804
Local test acc @ epoch 68: 0.8933
Client 7 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.06 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.1242387363123236, ce=1.909534341781525
Local test acc @ epoch 68: 0.8933
Global evaluate on test data...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.13 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.1276470597730865, ce=1.9128799308404514
Global test acc @ epoch 68: 0.8933
Global epoch 69...
Client 9 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.2 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.1252151224591316, ce=1.9105503221689102
Local test acc @ epoch 69: 0.8933
Client 5 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.96 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.126847391418361, ce=1.9121667802249152
Local test acc @ epoch 69: 0.8933
Client 6 execute local training on 6 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.23 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.1268902865298296, ce=1.9121832707371202
Local test acc @ epoch 69: 0.8933
Client 3 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.14 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.126499754846643, ce=1.9118014562623067
Local test acc @ epoch 69: 0.8933
Client 8 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.02 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.126689091598222, ce=1.9119713899365403
Local test acc @ epoch 69: 0.8933
Client 2 execute local training on 5 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.16 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.1259152249458735, ce=1.9112783368129782
Local test acc @ epoch 69: 0.8933
Client 1 execute local training on 6 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.0 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.1266379245650877, ce=1.911920492090337
Local test acc @ epoch 69: 0.8933
Client 4 execute local training on 20 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.2 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.139449246432803, ce=1.9243865103963342
Local test acc @ epoch 69: 0.8933
Client 7 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.28 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.12609883404653, ce=1.9114404329239043
Local test acc @ epoch 69: 0.8933
Client 0 execute local training on 23 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.2 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.143638880291116, ce=1.9290491074441125
Local test acc @ epoch 69: 0.8933
Global evaluate on test data...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.28 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.1294578440965863, ce=1.9147379109621392
Global test acc @ epoch 69: 0.8933
Global epoch 70...
Client 2 execute local training on 5 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.0 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.1277363922344437, ce=1.9131476730286285
Local test acc @ epoch 70: 0.8933
Client 7 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.24 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.127903877595149, ce=1.913295710218929
Local test acc @ epoch 70: 0.8933
Client 5 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.04 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.1286551584617808, ce=1.9140224978762435
Local test acc @ epoch 70: 0.8933
Client 3 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.14 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.12831014859567, ce=1.9136621051598561
Local test acc @ epoch 70: 0.8933
Client 1 execute local training on 6 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.19 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.12845138344196, ce=1.9137824312948535
Local test acc @ epoch 70: 0.8933
Client 8 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.99 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.1284990728995123, ce=1.9138307392502305
Local test acc @ epoch 70: 0.8933
Client 0 execute local training on 23 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.13 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.145165912465218, ce=1.9306327068292581
Local test acc @ epoch 70: 0.8933
Client 9 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.08 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.127049371463443, ce=1.9124333707071643
Local test acc @ epoch 70: 0.8933
Client 6 execute local training on 6 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.35 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.128704881859482, ce=1.914046272779223
Local test acc @ epoch 70: 0.8933
Client 4 execute local training on 20 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.29 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.141032036017934, ce=1.9260341239437444
Local test acc @ epoch 70: 0.8933
Global evaluate on test data...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.1 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.131217120847571, ce=1.9165489864742105
Global test acc @ epoch 70: 0.8933
Global epoch 71...
Client 6 execute local training on 6 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.07 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.130463332074498, ce=1.9158570816052776
Local test acc @ epoch 71: 0.8933
Client 3 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.29 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.130076948109023, ce=1.9154824299020634
Local test acc @ epoch 71: 0.8933
Client 7 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.11 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.1296699449009853, ce=1.9151164645384668
Local test acc @ epoch 71: 0.8933
Client 8 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.09 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.1302648039039123, ce=1.9156482698595698
Local test acc @ epoch 71: 0.8933
Client 4 execute local training on 20 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.07 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.142565243834749, ce=1.927634336976594
Local test acc @ epoch 71: 0.8933
Client 9 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.99 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.128828360126653, ce=1.9142655838939127
Local test acc @ epoch 71: 0.8933
Client 2 execute local training on 5 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.16 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.129509487283339, ce=1.9149746660311415
Local test acc @ epoch 71: 0.8933
Client 1 execute local training on 6 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.25 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.1302192252163494, ce=1.9156045370593326
Local test acc @ epoch 71: 0.8933
Client 0 execute local training on 23 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.15 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.146667201858048, ce=1.9321939243991568
Local test acc @ epoch 71: 0.8933
Client 5 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.11 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.130425329191969, ce=1.9158448092987768
Local test acc @ epoch 71: 0.8933
Global evaluate on test data...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.06 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.132932115858848, ce=1.9183195900998795
Global test acc @ epoch 71: 0.8933
Global epoch 72...
Client 8 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.07 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.131981061139238, ce=1.917420623230234
Local test acc @ epoch 72: 0.8933
Client 0 execute local training on 23 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.01 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.148122375164557, ce=1.9337116688923315
Local test acc @ epoch 72: 0.8933
Client 4 execute local training on 20 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.12 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.1440774442952706, ce=1.9292160276477965
Local test acc @ epoch 72: 0.8933
Client 3 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.21 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.1318085613327287, ce=1.9172706024827422
Local test acc @ epoch 72: 0.8933
Client 2 execute local training on 5 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.16 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.131239254266844, ce=1.9167612164310543
Local test acc @ epoch 72: 0.8933
Client 7 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.21 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.1314002141493176, ce=1.9169044334979424
Local test acc @ epoch 72: 0.8933
Client 6 execute local training on 6 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.1 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.1321941060484004, ce=1.9176441871172591
Local test acc @ epoch 72: 0.8933
Client 1 execute local training on 6 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.08 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.131944768199133, ce=1.917386556569438
Local test acc @ epoch 72: 0.8933
Client 5 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.03 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.132140272937783, ce=1.9176147768116276
Local test acc @ epoch 72: 0.8933
Client 9 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.23 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.1305733108465823, ce=1.9160670142271208
Local test acc @ epoch 72: 0.8933
Global evaluate on test data...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.16 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.1346125304698944, ce=1.920059414178889
Global test acc @ epoch 72: 0.8933
Global epoch 73...
Client 2 execute local training on 5 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.37 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.1329197659405, ce=1.9185024630759344
Local test acc @ epoch 73: 0.8933
Client 6 execute local training on 6 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.26 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.1338655426414737, ce=1.9193748112721227
Local test acc @ epoch 73: 0.8933
Client 3 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.09 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.133482557644538, ce=1.919005071191417
Local test acc @ epoch 73: 0.8933
Client 0 execute local training on 23 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.92 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.149551042996415, ce=1.9352065167219923
Local test acc @ epoch 73: 0.8933
Client 1 execute local training on 6 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.05 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.133625688629413, ce=1.919127998896734
Local test acc @ epoch 73: 0.8933
Client 8 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.14 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.1336706000457117, ce=1.9191695002777494
Local test acc @ epoch 73: 0.8933
Client 4 execute local training on 20 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.02 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.145552018640238, ce=1.9307633810628193
Local test acc @ epoch 73: 0.8933
Client 9 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.22 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.1322713661084483, ce=1.9178248628954906
Local test acc @ epoch 73: 0.8933
Client 7 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.22 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.1330814909770948, ce=1.9186459027920568
Local test acc @ epoch 73: 0.8933
Client 5 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.16 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.1338180958404456, ce=1.9193523953740752
Local test acc @ epoch 73: 0.8933
Global evaluate on test data...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.06 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.136244849737631, ce=1.9217535150990799
Global test acc @ epoch 73: 0.8933
Global epoch 74...
Client 2 execute local training on 5 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.93 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.134558510616285, ce=1.920205867162443
Local test acc @ epoch 74: 0.8933
Client 5 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.3 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.1354391621340305, ce=1.9210359766681047
Local test acc @ epoch 74: 0.8933
Client 7 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.15 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.134709626846357, ce=1.9203377997999467
Local test acc @ epoch 74: 0.8933
Client 4 execute local training on 20 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.14 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.1469855324937663, ce=1.932271898029981
Local test acc @ epoch 74: 0.8933
Client 3 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.24 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.1351232709140953, ce=1.920709745840906
Local test acc @ epoch 74: 0.8933
Client 0 execute local training on 23 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.94 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.1509534326988624, ce=1.9366756337640927
Local test acc @ epoch 74: 0.8933
Client 6 execute local training on 6 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.08 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.1355125277961067, ce=1.9210851675700626
Local test acc @ epoch 74: 0.8933
Client 1 execute local training on 6 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.01 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.1352663271197487, ce=1.9208315238490723
Local test acc @ epoch 74: 0.8933
Client 8 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.97 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.1353137257995956, ce=1.9208756118854784
Local test acc @ epoch 74: 0.8933
Client 9 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.9 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.1339209850774994, ce=1.9195379959534435
Local test acc @ epoch 74: 0.8933
Global evaluate on test data...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.9 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.1378389408282183, ce=1.9234125907379798
Global test acc @ epoch 74: 0.8933
Global epoch 75...
Client 7 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.91 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.136308782144424, ce=1.922003155023483
Local test acc @ epoch 75: 0.8933
Client 4 execute local training on 20 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.0 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.1483904393987916, ce=1.9337537384944958
Local test acc @ epoch 75: 0.8933
Client 3 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.95 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.136723389319324, ce=1.9223758795642678
Local test acc @ epoch 75: 0.8933
Client 1 execute local training on 6 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.98 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.1368554312428203, ce=1.9224880439663041
Local test acc @ epoch 75: 0.8933
Client 2 execute local training on 5 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.78 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.136167275249411, ce=1.9218806916189373
Local test acc @ epoch 75: 0.8933
Client 8 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.79 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.136904003696704, ce=1.9225309040746221
Local test acc @ epoch 75: 0.8933
Client 9 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.88 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.135530424500824, ce=1.9212123480662509
Local test acc @ epoch 75: 0.8933
Client 0 execute local training on 23 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.02 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.1523201186722574, ce=1.9381128261417768
Local test acc @ epoch 75: 0.8933
Client 6 execute local training on 6 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.31 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.1371068413104486, ce=1.9227447433162435
Local test acc @ epoch 75: 0.8933
Client 5 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.05 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.1370420215326713, ce=1.9227032310563483
Local test acc @ epoch 75: 0.8933
Global evaluate on test data...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.15 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.1393911507698373, ce=1.9250326112350684
Global test acc @ epoch 75: 0.8933
Global epoch 76...
Client 0 execute local training on 23 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.93 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.1536475496554592, ce=1.939512148317194
Local test acc @ epoch 76: 0.8933
Client 6 execute local training on 6 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.87 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.1386611956522006, ce=1.924367840446544
Local test acc @ epoch 76: 0.8933
Client 4 execute local training on 20 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.13 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.149772713490582, ce=1.935214299052404
Local test acc @ epoch 76: 0.8933
Client 1 execute local training on 6 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.98 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.1384235255488564, ce=1.9241241165386325
Local test acc @ epoch 76: 0.8933
Client 5 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.99 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.138596185850441, ce=1.9243259676377085
Local test acc @ epoch 76: 0.8933
Client 7 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.09 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.1378762713812907, ce=1.9236417884357082
Local test acc @ epoch 76: 0.8933
Client 9 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.08 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.137113756542906, ce=1.9228634152869963
Local test acc @ epoch 76: 0.8933
Client 3 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.21 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.1382869052777598, ce=1.9240078257092665
Local test acc @ epoch 76: 0.8933
Client 2 execute local training on 5 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.19 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.137739796430693, ce=1.9235227159829225
Local test acc @ epoch 76: 0.8933
Client 8 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.25 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.13846794495342, ce=1.9241632725499855
Local test acc @ epoch 76: 0.8933
Global evaluate on test data...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.99 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.1409140602437726, ce=1.926625478547617
Global test acc @ epoch 76: 0.8933
Global epoch 77...
Client 2 execute local training on 5 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.05 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.13926094414991, ce=1.9251143647626718
Local test acc @ epoch 77: 0.8933
Client 5 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.13 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.140112251590151, ce=1.925911850996426
Local test acc @ epoch 77: 0.8933
Client 7 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.13 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.1393977805561977, ce=1.9252340399880856
Local test acc @ epoch 77: 0.8933
Client 3 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.01 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.1397979855537415, ce=1.9255908373874173
Local test acc @ epoch 77: 0.8933
Client 1 execute local training on 6 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.96 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.1399406479039325, ce=1.9257119017878048
Local test acc @ epoch 77: 0.8933
Client 0 execute local training on 23 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.99 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.1549597073585613, ce=1.9408981156013085
Local test acc @ epoch 77: 0.8933
Client 9 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.96 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.1386505149373223, ce=1.9244712809786133
Local test acc @ epoch 77: 0.8933
Client 6 execute local training on 6 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.78 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.140184567198841, ce=1.9259610582697353
Local test acc @ epoch 77: 0.8933
Client 4 execute local training on 20 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.78 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.1511205511355618, ce=1.9366428683543915
Local test acc @ epoch 77: 0.8933
Client 8 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.87 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.139996247964168, ce=1.925762261556212
Local test acc @ epoch 77: 0.8933
Global evaluate on test data...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.83 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.1423972333789965, ce=1.92818064131565
Global test acc @ epoch 77: 0.8933
Global epoch 78...
Client 1 execute local training on 6 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.97 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.141427977642882, ce=1.927272254202291
Local test acc @ epoch 78: 0.8933
Client 0 execute local training on 23 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.14 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.156236241990273, ce=1.942249422059794
Local test acc @ epoch 78: 0.8933
Client 2 execute local training on 5 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.86 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.140753813293002, ce=1.9266801614359101
Local test acc @ epoch 78: 0.8933
Client 8 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.9 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.141475778380665, ce=1.927313515681129
Local test acc @ epoch 78: 0.8933
Client 6 execute local training on 6 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.74 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.1416712329475156, ce=1.927520595111844
Local test acc @ epoch 78: 0.8933
Client 4 execute local training on 20 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.74 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.152439129461936, ce=1.938044018292348
Local test acc @ epoch 78: 0.8933
Client 9 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.75 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.140152895395909, ce=1.9260463107020396
Local test acc @ epoch 78: 0.8933
Client 7 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.62 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.140892010763151, ce=1.9268012791340852
Local test acc @ epoch 78: 0.8933
Client 3 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.65 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.141295182595559, ce=1.9271604156809217
Local test acc @ epoch 78: 0.8933
Client 5 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.67 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.141601681162458, ce=1.9274723813641308
Local test acc @ epoch 78: 0.8933
Global evaluate on test data...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.95 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.143849257208885, ce=1.9297061841332273
Global test acc @ epoch 78: 0.8933
Global epoch 79...
Client 3 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.84 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.1427439407471125, ce=1.9286849351810902
Local test acc @ epoch 79: 0.8933
Client 0 execute local training on 23 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.87 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.157487766458354, ce=1.9435788892264765
Local test acc @ epoch 79: 0.8933
Client 7 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.82 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.142346112279717, ce=1.9283314135729912
Local test acc @ epoch 79: 0.8933
Client 1 execute local training on 6 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.75 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.142890349714034, ce=1.9288092390462028
Local test acc @ epoch 79: 0.8933
Client 6 execute local training on 6 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.84 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.1431265532150183, ce=1.9290510251106991
Local test acc @ epoch 79: 0.8933
Client 2 execute local training on 5 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.92 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.142216352968041, ce=1.9282183029276145
Local test acc @ epoch 79: 0.8933
Client 9 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.02 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.141617953914021, ce=1.9275863121432801
Local test acc @ epoch 79: 0.8933
Client 8 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.8 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.1429313264035303, ce=1.9288445284887574
Local test acc @ epoch 79: 0.8933
Client 4 execute local training on 20 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.92 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.1537298065259916, ce=1.9394183649720615
Local test acc @ epoch 79: 0.8933
Client 5 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.91 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.143051385195977, ce=1.9289965689110946
Local test acc @ epoch 79: 0.8933
Global evaluate on test data...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.21 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.1452642766707535, ce=1.9311983772101833
Global test acc @ epoch 79: 0.8933
Global epoch 80...
Client 0 execute local training on 23 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.09 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.158711051995601, ce=1.9448802527270042
Local test acc @ epoch 80: 0.8933
Client 5 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.04 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.144464121089069, ce=1.930485677192537
Local test acc @ epoch 80: 0.8933
Client 8 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.91 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.14435286874618, ce=1.9303426929482577
Local test acc @ epoch 80: 0.8933
Client 4 execute local training on 20 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.1 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.1549926435728683, ce=1.9407670371192445
Local test acc @ epoch 80: 0.8933
Client 3 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.9 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.144174357619854, ce=1.9301932163942546
Local test acc @ epoch 80: 0.8933
Client 1 execute local training on 6 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.02 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.144311731971732, ce=1.9303089278432335
Local test acc @ epoch 80: 0.8933
Client 2 execute local training on 5 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.91 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.1436748802661896, ce=1.9297265347693011
Local test acc @ epoch 80: 0.8933
Client 7 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.99 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.143795757118715, ce=1.9298333928607034
Local test acc @ epoch 80: 0.8933
Client 9 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.99 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.1430549342697915, ce=1.9291007307232195
Local test acc @ epoch 80: 0.8933
Client 6 execute local training on 6 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.12 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.1445500238226094, ce=1.9305509477523517
Local test acc @ epoch 80: 0.8933
Global evaluate on test data...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.27 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.1466565807478144, ce=1.9326675803309175
Global test acc @ epoch 80: 0.8933
Global epoch 81...
Client 4 execute local training on 20 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.36 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.1562593289471548, ce=1.9420854474893445
Local test acc @ epoch 81: 0.8933
Client 5 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.38 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.1458572823246684, ce=1.9319560826298507
Local test acc @ epoch 81: 0.8933
Client 9 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.91 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.1444731803115356, ce=1.9305835938925568
Local test acc @ epoch 81: 0.8933
Client 1 execute local training on 6 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.94 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.14573439326855, ce=1.9317810610641617
Local test acc @ epoch 81: 0.8933
Client 8 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.96 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.145757464914147, ce=1.9318155239156478
Local test acc @ epoch 81: 0.8933
Client 0 execute local training on 23 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.14 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.159917078980612, ce=1.946166933366519
Local test acc @ epoch 81: 0.8933
Client 2 execute local training on 5 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.94 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.1451221975164674, ce=1.93120273077694
Local test acc @ epoch 81: 0.8933
Client 7 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.97 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.145240731742404, ce=1.9313077366750726
Local test acc @ epoch 81: 0.8933
Client 6 execute local training on 6 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.19 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.1459557282815287, ce=1.932021617742291
Local test acc @ epoch 81: 0.8933
Client 3 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.1 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.1455974945234595, ce=1.93166386398918
Local test acc @ epoch 81: 0.8933
Global evaluate on test data...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.28 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.148037613805281, ce=1.9341058696775755
Global test acc @ epoch 81: 0.8933
Global epoch 82...
Client 0 execute local training on 23 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.99 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.161130810276084, ce=1.9474240090411676
Local test acc @ epoch 82: 0.8933
Client 5 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.19 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.1472594612235323, ce=1.933392588309543
Local test acc @ epoch 82: 0.8933
Client 9 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.08 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.145901169525374, ce=1.9320412379495606
Local test acc @ epoch 82: 0.8933
Client 1 execute local training on 6 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.05 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.1471446202982456, ce=1.933222251810127
Local test acc @ epoch 82: 0.8933
Client 3 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.06 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.147006146951553, ce=1.9331033459186928
Local test acc @ epoch 82: 0.8933
Client 2 execute local training on 5 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.33 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.146535656867771, ce=1.9326471493055088
Local test acc @ epoch 82: 0.8933
Client 6 execute local training on 6 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.22 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.1473631205361916, ce=1.933458647957533
Local test acc @ epoch 82: 0.8933
Client 8 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.98 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.1471686128082625, ce=1.9332562223767167
Local test acc @ epoch 82: 0.8933
Client 4 execute local training on 20 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.08 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.1575328363191097, ce=1.9433975847969822
Local test acc @ epoch 82: 0.8933
Client 7 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.1 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.1466523020639334, ce=1.9327506145339097
Local test acc @ epoch 82: 0.8933
Global evaluate on test data...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.32 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.1494147813101425, ce=1.935514964220543
Global test acc @ epoch 82: 0.8933
Global epoch 83...
Client 4 execute local training on 20 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.16 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.15876494033621, ce=1.9446707860842651
Local test acc @ epoch 83: 0.8933
Client 1 execute local training on 6 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.11 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.148527883608407, ce=1.934638551526042
Local test acc @ epoch 83: 0.8933
Client 9 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.16 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.147296958013412, ce=1.9334686858751784
Local test acc @ epoch 83: 0.8933
Client 3 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.15 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.148390697229893, ce=1.934520197556436
Local test acc @ epoch 83: 0.8933
Client 8 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.17 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.1485486235640465, ce=1.9346678216584596
Local test acc @ epoch 83: 0.8933
Client 7 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.22 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.1480349999502164, ce=1.9341656581346183
Local test acc @ epoch 83: 0.8933
Client 6 execute local training on 6 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.1 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.148743261984729, ce=1.9348710065647627
Local test acc @ epoch 83: 0.8933
Client 2 execute local training on 5 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.08 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.147924478994597, ce=1.9340687217735935
Local test acc @ epoch 83: 0.8933
Client 5 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.27 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.1486367167682823, ce=1.934801257986241
Local test acc @ epoch 83: 0.8933
Client 0 execute local training on 23 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.31 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.1624136069499023, ce=1.9486616553398644
Local test acc @ epoch 83: 0.8933
Global evaluate on test data...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.17 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.150763544741027, ce=1.9368965128768472
Global test acc @ epoch 83: 0.8933
Global epoch 84...
Client 0 execute local training on 23 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.43 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.1636702647996606, ce=1.9498750317526605
Local test acc @ epoch 84: 0.8933
Client 2 execute local training on 5 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.08 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.1492829071272403, ce=1.9354612895622145
Local test acc @ epoch 84: 0.8933
Client 9 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.02 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.1486666352923858, ce=1.9348714953308368
Local test acc @ epoch 84: 0.8933
Client 1 execute local training on 6 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.19 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.149882017472468, ce=1.9360262070112888
Local test acc @ epoch 84: 0.8933
Client 6 execute local training on 6 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.97 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.15009795334361, ce=1.9362588102238851
Local test acc @ epoch 84: 0.8933
Client 8 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.04 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.149904674072878, ce=1.9360579216910045
Local test acc @ epoch 84: 0.8933
Client 4 execute local training on 20 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.27 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.159981793493306, ce=1.945929320247455
Local test acc @ epoch 84: 0.8933
Client 5 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.22 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.1499905271814503, ce=1.9361879411590925
Local test acc @ epoch 84: 0.8933
Client 7 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.23 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.1493920659253356, ce=1.9355581297167979
Local test acc @ epoch 84: 0.8933
Client 3 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.21 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.1497492366427675, ce=1.935912646336699
Local test acc @ epoch 84: 0.8933
Global evaluate on test data...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.13 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.1520914380703497, ce=1.9382595802954508
Global test acc @ epoch 84: 0.8933
Global epoch 85...
Client 4 execute local training on 20 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.03 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.161171055168187, ce=1.9471614239667479
Local test acc @ epoch 85: 0.8933
Client 5 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.31 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.151323908786161, ce=1.9375454016657583
Local test acc @ epoch 85: 0.8933
Client 0 execute local training on 23 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.26 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.1649074291964188, ce=1.951071665269106
Local test acc @ epoch 85: 0.8933
Client 2 execute local training on 5 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.28 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.150683958049214, ce=1.9368319280982706
Local test acc @ epoch 85: 0.8933
Client 3 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.18 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.1511162262444103, ce=1.9372757355579473
Local test acc @ epoch 85: 0.8933
Client 7 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.18 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.1507790971786602, ce=1.9369216822417814
Local test acc @ epoch 85: 0.8933
Client 1 execute local training on 6 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.18 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.151241354439237, ce=1.9373965988363187
Local test acc @ epoch 85: 0.8933
Client 8 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.29 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.1512486221046623, ce=1.9374280209350891
Local test acc @ epoch 85: 0.8933
Client 6 execute local training on 6 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.09 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.1514446090120787, ce=1.93762459581438
Local test acc @ epoch 85: 0.8933
Client 9 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.23 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.1500732108540492, ce=1.9362508457304826
Local test acc @ epoch 85: 0.8933
Global evaluate on test data...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.12 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.1534441077927933, ce=1.9395948341678981
Global test acc @ epoch 85: 0.8933
Global epoch 86...
Client 8 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.19 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.1526244922515447, ce=1.9387579156717019
Local test acc @ epoch 86: 0.8933
Client 0 execute local training on 23 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.26 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.1661188375512395, ce=1.9522439057883831
Local test acc @ epoch 86: 0.8933
Client 6 execute local training on 6 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.96 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.152825384785276, ce=1.9389593692733569
Local test acc @ epoch 86: 0.8933
Client 3 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.08 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.152497856441988, ce=1.9386121556295375
Local test acc @ epoch 86: 0.8933
Client 5 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.99 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.1527059750819424, ce=1.9388808524321273
Local test acc @ epoch 86: 0.8933
Client 2 execute local training on 5 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.06 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.152069157963499, ce=1.9381722305943938
Local test acc @ epoch 86: 0.8933
Client 4 execute local training on 20 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.01 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.16241769506297, ce=1.948382813202131
Local test acc @ epoch 86: 0.8933
Client 9 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.43 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.151469293264074, ce=1.9376010682694782
Local test acc @ epoch 86: 0.8933
Client 1 execute local training on 6 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.3 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.1526252307476255, ce=1.9387357342780607
Local test acc @ epoch 86: 0.8933
Client 7 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.08 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.152165643118937, ce=1.938263645001976
Local test acc @ epoch 86: 0.8933
Global evaluate on test data...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.17 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.1547978449305263, ce=1.9409050739755915
Global test acc @ epoch 86: 0.8933
Global epoch 87...
Client 9 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.05 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.1528361909980074, ce=1.9389245518171079
Local test acc @ epoch 87: 0.8933
Client 8 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.19 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.153980392928517, ce=1.9400697694142652
Local test acc @ epoch 87: 0.8933
Client 1 execute local training on 6 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.2 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.153977057802568, ce=1.9400443277980632
Local test acc @ epoch 87: 0.8933
Client 4 execute local training on 20 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.4 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.16364285213138, ce=1.9495719441601111
Local test acc @ epoch 87: 0.8933
Client 5 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.05 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.1540621631189225, ce=1.9401927065140574
Local test acc @ epoch 87: 0.8933
Client 6 execute local training on 6 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.05 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.1541832726482952, ce=1.9402737600611784
Local test acc @ epoch 87: 0.8933
Client 0 execute local training on 23 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.29 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.167316358023827, ce=1.953404283342048
Local test acc @ epoch 87: 0.8933
Client 2 execute local training on 5 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.2 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.1534363840698103, ce=1.9394969259442325
Local test acc @ epoch 87: 0.8933
Client 3 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.09 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.153861706683395, ce=1.9399329615928702
Local test acc @ epoch 87: 0.8933
Client 7 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.21 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.1535252967012037, ce=1.939580234468553
Local test acc @ epoch 87: 0.8933
Global evaluate on test data...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.45 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.1561247605249423, ce=1.9421898308037515
Global test acc @ epoch 87: 0.8933
Global epoch 88...
Client 9 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.13 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.1541744271002776, ce=1.940221569627112
Local test acc @ epoch 88: 0.8933
Client 4 execute local training on 20 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.18 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.16485061804089, ce=1.9507450708981098
Local test acc @ epoch 88: 0.8933
Client 6 execute local training on 6 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.19 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.1555144475140704, ce=1.9415633338741758
Local test acc @ epoch 88: 0.8933
Client 7 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.08 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.1548511771433945, ce=1.940865416242793
Local test acc @ epoch 88: 0.8933
Client 8 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.11 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.155316219690743, ce=1.9413641403984407
Local test acc @ epoch 88: 0.8933
Client 2 execute local training on 5 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.27 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.154768759230955, ce=1.9407884130399526
Local test acc @ epoch 88: 0.8933
Client 3 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.23 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.155189788943037, ce=1.9412204384936949
Local test acc @ epoch 88: 0.8933
Client 1 execute local training on 6 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.49 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.155311751256295, ce=1.9413382545337314
Local test acc @ epoch 88: 0.8933
Client 5 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.28 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.155388776862293, ce=1.9414769662069897
Local test acc @ epoch 88: 0.8933
Client 0 execute local training on 23 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.18 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.1684882225246604, ce=1.9545402881760146
Local test acc @ epoch 88: 0.8933
Global evaluate on test data...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.33 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.157430164584326, ce=1.9434550015742451
Global test acc @ epoch 88: 0.8933
Global epoch 89...
Client 1 execute local training on 6 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.14 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.1566180523929246, ce=1.942605687195303
Local test acc @ epoch 89: 0.8933
Client 3 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.06 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.156488440999197, ce=1.9424794910740464
Local test acc @ epoch 89: 0.8933
Client 8 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.26 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.1566223234211632, ce=1.9426303126142512
Local test acc @ epoch 89: 0.8933
Client 4 execute local training on 20 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.34 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.1660392112141356, ce=1.9519009440350799
Local test acc @ epoch 89: 0.8922
Client 9 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.88 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.155499606504353, ce=1.941506818347755
Local test acc @ epoch 89: 0.8933
Client 2 execute local training on 5 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.99 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.156080335105231, ce=1.9420607136374604
Local test acc @ epoch 89: 0.8933
Client 6 execute local training on 6 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.07 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.1568185654255227, ce=1.9428280382138576
Local test acc @ epoch 89: 0.8933
Client 0 execute local training on 23 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.06 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.1696378208628486, ce=1.9556568071158293
Local test acc @ epoch 89: 0.8933
Client 5 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.56 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.1566967190405646, ce=1.9427447365827433
Local test acc @ epoch 89: 0.8933
Client 7 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.49 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.156166301407945, ce=1.9421422580238863
Local test acc @ epoch 89: 0.8933
Global evaluate on test data...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.35 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.1587099017353233, ce=1.944696913014555
Global test acc @ epoch 89: 0.8933
Global epoch 90...
Client 2 execute local training on 5 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.02 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.1573651216992547, ce=1.9433090740172838
Local test acc @ epoch 90: 0.8933
Client 3 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.18 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.1577783453901973, ce=1.9437321639227674
Local test acc @ epoch 90: 0.8933
Client 8 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.27 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.157904481669085, ce=1.9438745989666104
Local test acc @ epoch 90: 0.8933
Client 5 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.19 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.1579694860025285, ce=1.9439789677158765
Local test acc @ epoch 90: 0.8933
Client 0 execute local training on 23 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.25 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.1707637561570614, ce=1.9567504325776672
Local test acc @ epoch 90: 0.8933
Client 6 execute local training on 6 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.41 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.1581031548867533, ce=1.9440750740767883
Local test acc @ epoch 90: 0.8933
Client 4 execute local training on 20 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.21 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.1672126328179595, ce=1.9530431856593167
Local test acc @ epoch 90: 0.8922
Client 9 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.22 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.1567978659354217, ce=1.942767513562549
Local test acc @ epoch 90: 0.8933
Client 7 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.33 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.1574495563266476, ce=1.9433886666221054
Local test acc @ epoch 90: 0.8933
Client 1 execute local training on 6 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.25 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.157908387687228, ce=1.9438584738238636
Local test acc @ epoch 90: 0.8933
Global evaluate on test data...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.07 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.159967448186437, ce=1.9459179173409455
Global test acc @ epoch 90: 0.8933
Global epoch 91...
Client 7 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.02 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.15870466390881, ce=1.944608560948265
Local test acc @ epoch 91: 0.8933
Client 0 execute local training on 23 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.39 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.1718794312498986, ce=1.95783601710917
Local test acc @ epoch 91: 0.8933
Client 2 execute local training on 5 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.52 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.1586311590780904, ce=1.9445396087423155
Local test acc @ epoch 91: 0.8933
Client 9 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.42 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.1580666705555873, ce=1.94399995961129
Local test acc @ epoch 91: 0.8933
Client 4 execute local training on 20 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.14 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.1683614100884956, ce=1.954162431268405
Local test acc @ epoch 91: 0.8922
Client 6 execute local training on 6 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.22 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.1593669185397824, ce=1.945302613909958
Local test acc @ epoch 91: 0.8933
Client 3 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.15 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.159046414795272, ce=1.9449646662010516
Local test acc @ epoch 91: 0.8933
Client 5 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.3 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.1592348838618043, ce=1.9452078468063825
Local test acc @ epoch 91: 0.8933
Client 8 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.36 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.1591736778206783, ce=1.94510786418861
Local test acc @ epoch 91: 0.8933
Client 1 execute local training on 6 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.31 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.1591688514849463, ce=1.9450838019676346
Local test acc @ epoch 91: 0.8933
Global evaluate on test data...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.32 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.1612026489109075, ce=1.9471184158274604
Global test acc @ epoch 91: 0.8933
Global epoch 92...
Client 4 execute local training on 20 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.13 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.169485707075224, ce=1.9552584887481979
Local test acc @ epoch 92: 0.8922
Client 5 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.48 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.1604693747988533, ce=1.9464073209237576
Local test acc @ epoch 92: 0.8933
Client 9 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.27 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.159318132684865, ce=1.9452171499748152
Local test acc @ epoch 92: 0.8933
Client 6 execute local training on 6 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.18 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.160602014272585, ce=1.9465036753559872
Local test acc @ epoch 92: 0.8933
Client 0 execute local training on 23 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.36 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.1729809308270798, ce=1.958908193417195
Local test acc @ epoch 92: 0.8933
Client 3 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.23 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.160287760539886, ce=1.9461720621800163
Local test acc @ epoch 92: 0.8933
Client 2 execute local training on 5 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.51 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.159882550392676, ce=1.945757420325896
Local test acc @ epoch 92: 0.8933
Client 7 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.23 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.15995276931229, ce=1.9458228404864424
Local test acc @ epoch 92: 0.8933
Client 8 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.29 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.160414669491829, ce=1.9463143753513317
Local test acc @ epoch 92: 0.8933
Client 1 execute local training on 6 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.28 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.1604106880109244, ce=1.9462914666910904
Local test acc @ epoch 92: 0.8933
Global evaluate on test data...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.28 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.162419766461084, ce=1.9483022921272466
Global test acc @ epoch 92: 0.8933
Global epoch 93...
Client 0 execute local training on 23 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.14 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.1740539571560853, ce=1.9599544156375963
Local test acc @ epoch 93: 0.8933
Client 3 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.17 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.1615036523670232, ce=1.947355923945902
Local test acc @ epoch 93: 0.8933
Client 1 execute local training on 6 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.18 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.161628575499998, ce=1.947476958315805
Local test acc @ epoch 93: 0.8933
Client 6 execute local training on 6 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.45 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.161820331297883, ce=1.9476889823481374
Local test acc @ epoch 93: 0.8933
Client 2 execute local training on 5 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.24 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.161102483578778, ce=1.946945451592092
Local test acc @ epoch 93: 0.8933
Client 8 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.05 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.161631402892804, ce=1.9474983274208002
Local test acc @ epoch 93: 0.8933
Client 7 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.33 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.161172131606198, ce=1.9470102408053744
Local test acc @ epoch 93: 0.8933
Client 9 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.18 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.1605486120652717, ce=1.9464150739395083
Local test acc @ epoch 93: 0.8933
Client 5 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.25 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.161685630269007, ce=1.94759029750198
Local test acc @ epoch 93: 0.8933
Client 4 execute local training on 20 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.44 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.1706030355134143, ce=1.9563490034207167
Local test acc @ epoch 93: 0.8922
Global evaluate on test data...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.53 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.16361370901449, ce=1.9494649395074122
Global test acc @ epoch 93: 0.8933
Global epoch 94...
Client 6 execute local training on 6 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.19 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.1630155900749592, ce=1.9488533917049005
Local test acc @ epoch 94: 0.8922
Client 7 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.26 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.1623670600969858, ce=1.9481745451522883
Local test acc @ epoch 94: 0.8922
Client 2 execute local training on 5 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.23 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.1623000393220044, ce=1.948112289081155
Local test acc @ epoch 94: 0.8922
Client 4 execute local training on 20 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.46 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.1716932848506016, ce=1.9574141537917615
Local test acc @ epoch 94: 0.8922
Client 0 execute local training on 23 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.41 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.175117542710873, ce=1.9609922390473176
Local test acc @ epoch 94: 0.8922
Client 5 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.26 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.162884795064226, ce=1.9487576870476977
Local test acc @ epoch 94: 0.8933
Client 9 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.1 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.1617618492983897, ce=1.9475965117004814
Local test acc @ epoch 94: 0.8933
Client 1 execute local training on 6 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.46 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.1628293457928054, ce=1.9486472355198459
Local test acc @ epoch 94: 0.8922
Client 3 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.43 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.162704586982727, ce=1.9485262894268003
Local test acc @ epoch 94: 0.8922
Client 8 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.39 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.1628309121919336, ce=1.9486665252989956
Local test acc @ epoch 94: 0.8922
Global evaluate on test data...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.32 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.164790320287057, ce=1.9506110183584624
Global test acc @ epoch 94: 0.8922
Global epoch 95...
Client 0 execute local training on 23 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.2 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.1761578849149408, ce=1.9620075542660322
Local test acc @ epoch 95: 0.8922
Client 6 execute local training on 6 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.41 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.16419793542372, ce=1.9500060388553917
Local test acc @ epoch 95: 0.8922
Client 5 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.26 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.164057204756168, ce=1.9498992973629625
Local test acc @ epoch 95: 0.8933
Client 3 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.31 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.163881213293163, ce=1.9496735065729456
Local test acc @ epoch 95: 0.8922
Client 7 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.36 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.16354597763184, ce=1.9493248323872885
Local test acc @ epoch 95: 0.8922
Client 9 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.38 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.1629456620697582, ce=1.948750643593013
Local test acc @ epoch 95: 0.8922
Client 2 execute local training on 5 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.26 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.1634816464481004, ce=1.949264812668105
Local test acc @ epoch 95: 0.8922
Client 1 execute local training on 6 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.14 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.1640054449575756, ce=1.9497940313900477
Local test acc @ epoch 95: 0.8922
Client 8 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.18 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.164005596703346, ce=1.9498116373416081
Local test acc @ epoch 95: 0.8922
Client 4 execute local training on 20 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.13 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.172773481782423, ce=1.958470572550826
Local test acc @ epoch 95: 0.8922
Global evaluate on test data...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.21 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.1659431069269095, ce=1.9517355220418695
Global test acc @ epoch 95: 0.8922
Global epoch 96...
Client 8 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.39 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.1651615027440796, ce=1.9509393387559217
Local test acc @ epoch 96: 0.8922
Client 6 execute local training on 6 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.27 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.1653527319431305, ce=1.9511325820220156
Local test acc @ epoch 96: 0.8922
Client 5 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.37 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.1652104449928355, ce=1.9510238866133605
Local test acc @ epoch 96: 0.8922
Client 3 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.19 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.165036885563387, ce=1.9508009218968603
Local test acc @ epoch 96: 0.8922
Client 0 execute local training on 23 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.3 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.177186055467763, ce=1.9630134215648276
Local test acc @ epoch 96: 0.8922
Client 1 execute local training on 6 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.25 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.1651641275357765, ce=1.9509252856192152
Local test acc @ epoch 96: 0.8922
Client 7 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.13 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.164707540645512, ce=1.950458627853717
Local test acc @ epoch 96: 0.8922
Client 4 execute local training on 20 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.37 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.1738312424869712, ce=1.959505800031642
Local test acc @ epoch 96: 0.8922
Client 2 execute local training on 5 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.22 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.1646520932880016, ce=1.9504075841526196
Local test acc @ epoch 96: 0.8922
Client 9 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.16 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.164115487435542, ce=1.9498917667468447
Local test acc @ epoch 96: 0.8922
Global evaluate on test data...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.12 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.1670783394520434, ce=1.9528435005451175
Global test acc @ epoch 96: 0.8922
Global epoch 97...
Client 1 execute local training on 6 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.17 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.1662996799573984, ce=1.9520339395214243
Local test acc @ epoch 97: 0.8922
Client 7 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.22 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.165843467373367, ce=1.9515682282062117
Local test acc @ epoch 97: 0.8922
Client 6 execute local training on 6 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.06 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.166485110827542, ce=1.9522383113194188
Local test acc @ epoch 97: 0.8922
Client 8 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.13 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.1662975188789018, ce=1.952048234876099
Local test acc @ epoch 97: 0.8922
Client 0 execute local training on 23 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.27 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.178201537340059, ce=1.9640071346915915
Local test acc @ epoch 97: 0.8922
Client 5 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.17 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.1663465587370987, ce=1.9521327588244575
Local test acc @ epoch 97: 0.8922
Client 2 execute local training on 5 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.32 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.165788882642711, ce=1.9515182044787491
Local test acc @ epoch 97: 0.8922
Client 9 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.19 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.1652617684198083, ce=1.9510115407477922
Local test acc @ epoch 97: 0.8922
Client 3 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.25 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.16617575151111, ce=1.9519138052684901
Local test acc @ epoch 97: 0.8922
Client 4 execute local training on 20 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.18 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.1748740022335578, ce=1.9605271943191493
Local test acc @ epoch 97: 0.8922
Global evaluate on test data...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.16 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.1681934841182255, ce=1.9539328221531773
Global test acc @ epoch 97: 0.8922
Global epoch 98...
Client 5 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.09 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.1674610797418365, ce=1.9532210429055097
Local test acc @ epoch 98: 0.8922
Client 2 execute local training on 5 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.22 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.166909265408822, ce=1.952613097012138
Local test acc @ epoch 98: 0.8922
Client 8 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.36 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.1674176265887164, ce=1.9531424476618986
Local test acc @ epoch 98: 0.8922
Client 1 execute local training on 6 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.2 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.1674226363864513, ce=1.9531319030377245
Local test acc @ epoch 98: 0.8922
Client 6 execute local training on 6 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.28 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.167607245368695, ce=1.953334772751392
Local test acc @ epoch 98: 0.8922
Client 4 execute local training on 20 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.15 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.175895480661217, ce=1.9615281206615347
Local test acc @ epoch 98: 0.8922
Client 3 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.27 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.1672958018036064, ce=1.9530088106091372
Local test acc @ epoch 98: 0.8922
Client 0 execute local training on 23 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.37 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.1791954795154957, ce=1.9649805048058688
Local test acc @ epoch 98: 0.8922
Client 9 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.09 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.166395082659678, ce=1.9521190487611513
Local test acc @ epoch 98: 0.8922
Client 7 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.36 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.1669711579970263, ce=1.9526707491437838
Local test acc @ epoch 98: 0.8922
Global evaluate on test data...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.45 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.16929315016904, ce=1.9550081694461183
Global test acc @ epoch 98: 0.8922
Global epoch 99...
Client 2 execute local training on 5 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.43 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.1680137118615144, ce=1.9536938781240965
Local test acc @ epoch 99: 0.8922
Client 3 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.1 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.168395343723647, ce=1.954084280294349
Local test acc @ epoch 99: 0.8922
Client 8 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.17 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.168522776813682, ce=1.9542233110883092
Local test acc @ epoch 99: 0.8922
Client 9 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.21 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.1674983022956673, ce=1.9531974366715479
Local test acc @ epoch 99: 0.8922
Client 7 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.15 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.16806322718979, ce=1.953739045170144
Local test acc @ epoch 99: 0.8922
Client 6 execute local training on 6 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.1 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.16870813577547, ce=1.9544116337382809
Local test acc @ epoch 99: 0.8922
Client 4 execute local training on 20 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.26 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.1769066130349395, ce=1.9625203410891285
Local test acc @ epoch 99: 0.8922
Client 1 execute local training on 6 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.25 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.1685247259949327, ce=1.9542097105308431
Local test acc @ epoch 99: 0.8922
Client 5 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.37 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.1685615110834804, ce=1.954296424113066
Local test acc @ epoch 99: 0.8922
Client 0 execute local training on 23 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.25 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.1801757257465924, ce=1.9659418405177362
Local test acc @ epoch 99: 0.8922
Global evaluate on test data...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.17 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.170369909170571, ce=1.9560615310828562
Global test acc @ epoch 99: 0.8922
Global epoch 100...
Client 5 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.2 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.1696434734611336, ce=1.9553548297325074
Local test acc @ epoch 100: 0.8922
Client 6 execute local training on 6 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.27 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.169790222830729, ce=1.955470278817894
Local test acc @ epoch 100: 0.8922
Client 2 execute local training on 5 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.18 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.169102213524897, ce=1.9547595165588432
Local test acc @ epoch 100: 0.8922
Client 9 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.15 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.168593907301579, ce=1.954270279102961
Local test acc @ epoch 100: 0.8922
Client 8 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.24 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.169601456014388, ce=1.9552792601344586
Local test acc @ epoch 100: 0.8922
Client 7 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.34 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.16915322498444, ce=1.9548061781327772
Local test acc @ epoch 100: 0.8922
Client 3 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.39 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.169478361212879, ce=1.9551442821909273
Local test acc @ epoch 100: 0.8922
Client 1 execute local training on 6 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.35 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.1696030768232606, ce=1.95526546650564
Local test acc @ epoch 100: 0.8922
Client 4 execute local training on 20 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.16 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.177898276289669, ce=1.9634940431047985
Local test acc @ epoch 100: 0.8922
Client 0 execute local training on 23 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.32 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.18114220303133, ce=1.9668902127683305
Local test acc @ epoch 100: 0.8922
Global evaluate on test data...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.16 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.1714342933729154, ce=1.9571035143380946
Global test acc @ epoch 100: 0.8922
Global epoch 101...
Client 5 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.16 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.1707043628627005, ce=1.9563931662667005
Local test acc @ epoch 101: 0.8922
Client 8 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.4 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.1706683805229465, ce=1.9563233157298054
Local test acc @ epoch 101: 0.8922
Client 2 execute local training on 5 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.35 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.170173261963993, ce=1.955808788719423
Local test acc @ epoch 101: 0.8922
Client 9 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.11 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.1696693749602782, ce=1.9553233275677828
Local test acc @ epoch 101: 0.8922
Client 1 execute local training on 6 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.33 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.1706707679897272, ce=1.956311269567727
Local test acc @ epoch 101: 0.8922
Client 7 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.48 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.170216934670002, ce=1.9558487121885433
Local test acc @ epoch 101: 0.8922
Client 0 execute local training on 23 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.39 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.1820896300700827, ce=1.9678207252983315
Local test acc @ epoch 101: 0.8922
Client 6 execute local training on 6 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.27 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.170862020702537, ce=1.9565203906994184
Local test acc @ epoch 101: 0.8922
Client 4 execute local training on 20 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.24 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.1788827459746543, ce=1.964461492801834
Local test acc @ epoch 101: 0.8922
Client 3 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.24 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.170550303174815, ce=1.9561947455847506
Local test acc @ epoch 101: 0.8922
Global evaluate on test data...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.33 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.172480164864741, ce=1.958128349267572
Global test acc @ epoch 101: 0.8922
Global epoch 102...
Client 6 execute local training on 6 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.33 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.1719077511118092, ce=1.957545078249904
Local test acc @ epoch 102: 0.8922
Client 7 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.14 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.1712698684919864, ce=1.95688155561464
Local test acc @ epoch 102: 0.8922
Client 8 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.29 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.1717214942525285, ce=1.9573557697170167
Local test acc @ epoch 102: 0.8922
Client 4 execute local training on 20 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.3 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.1798482365017637, ce=1.9654110552582225
Local test acc @ epoch 102: 0.8922
Client 3 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.14 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.1715984188635415, ce=1.9572222280262683
Local test acc @ epoch 102: 0.8922
Client 5 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.46 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.171756028308781, ce=1.957423408414046
Local test acc @ epoch 102: 0.8922
Client 9 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.48 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.1707302282709593, ce=1.956363201353065
Local test acc @ epoch 102: 0.8922
Client 1 execute local training on 6 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.31 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.171728174894228, ce=1.9573483271729235
Local test acc @ epoch 102: 0.8922
Client 2 execute local training on 5 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.12 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.1712274201419377, ce=1.9568427835381643
Local test acc @ epoch 102: 0.8922
Client 0 execute local training on 23 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.29 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.183028296866548, ce=1.9687444162070222
Local test acc @ epoch 102: 0.8922
Global evaluate on test data...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.3 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.1735117987755244, ce=1.959140295820042
Global test acc @ epoch 102: 0.8922
Global epoch 103...
Client 7 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.02 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.172305204999556, ce=1.9578974776171008
Local test acc @ epoch 103: 0.8922
Client 3 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.38 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.1726317405700684, ce=1.9582359212536653
Local test acc @ epoch 103: 0.8922
Client 6 execute local training on 6 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.86 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.172938854595937, ce=1.9585567854001367
Local test acc @ epoch 103: 0.8922
Client 2 execute local training on 5 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.88 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.172263642516705, ce=1.9578597726061435
Local test acc @ epoch 103: 0.8922
Client 4 execute local training on 20 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.24 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.18079660955919, ce=1.9663449297732662
Local test acc @ epoch 103: 0.8922
Client 5 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.17 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.1727890112531294, ce=1.9584361676796398
Local test acc @ epoch 103: 0.8922
Client 1 execute local training on 6 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.3 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.172760799117045, ce=1.9583615579623574
Local test acc @ epoch 103: 0.8922
Client 8 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.06 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.172754973993389, ce=1.9583698591135053
Local test acc @ epoch 103: 0.8922
Client 0 execute local training on 23 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.1 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.183954319549263, ce=1.9696556175034923
Local test acc @ epoch 103: 0.8922
Client 9 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.42 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.171778089956406, ce=1.957391255169781
Local test acc @ epoch 103: 0.8922
Global evaluate on test data...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.61 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.1745284970747223, ce=1.9601382630631385
Global test acc @ epoch 103: 0.8922
Global epoch 104...
Client 4 execute local training on 20 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.16 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.181738551603545, ce=1.9672731418387979
Local test acc @ epoch 104: 0.8922
Client 0 execute local training on 23 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.35 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.1848657524913824, ce=1.9705538464709655
Local test acc @ epoch 104: 0.8922
Client 5 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.24 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.17380877290297, ce=1.9594366212879146
Local test acc @ epoch 104: 0.8922
Client 9 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.5 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.172806985607935, ce=1.9584008914489708
Local test acc @ epoch 104: 0.8922
Client 6 execute local training on 6 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 79.03 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.173961260996827, ce=1.9595605557371438
Local test acc @ epoch 104: 0.8922
Client 7 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.54 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.1733312863822376, ce=1.9589057381696957
Local test acc @ epoch 104: 0.8922
Client 1 execute local training on 6 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.26 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.1737833692940005, ce=1.9593660924965388
Local test acc @ epoch 104: 0.8922
Client 8 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.28 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.173781391130675, ce=1.9593774414539467
Local test acc @ epoch 104: 0.8922
Client 3 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.2 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.1736591759078, ce=1.9592452001829455
Local test acc @ epoch 104: 0.8922
Client 2 execute local training on 5 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.45 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.173295886691557, ce=1.9588733168931987
Local test acc @ epoch 104: 0.8922
Global evaluate on test data...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.18 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.175533033292228, ce=1.9611253768000483
Global test acc @ epoch 104: 0.8922
Global epoch 105...
Client 8 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.31 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.1747798408390184, ce=1.9603579401991025
Local test acc @ epoch 105: 0.8922
Client 2 execute local training on 5 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.23 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.174296975409219, ce=1.959856866143542
Local test acc @ epoch 105: 0.8922
Client 0 execute local training on 23 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.18 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.185764027844875, ce=1.9714401600339904
Local test acc @ epoch 105: 0.8922
Client 7 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.4 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.174338585739836, ce=1.959895235150373
Local test acc @ epoch 105: 0.8922
Client 3 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.14 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.174662176895579, ce=1.9602308619687223
Local test acc @ epoch 105: 0.8922
Client 1 execute local training on 6 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.47 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.1747904674175684, ce=1.9603556164906415
Local test acc @ epoch 105: 0.8922
Client 5 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.44 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.1748113224812604, ce=1.960420782655083
Local test acc @ epoch 105: 0.8922
Client 9 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.22 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.173817930418417, ce=1.9593940368831617
Local test acc @ epoch 105: 0.8922
Client 4 execute local training on 20 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.08 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.1826657630981656, ce=1.9681878008784668
Local test acc @ epoch 105: 0.8922
Client 6 execute local training on 6 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.14 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.1749690429333155, ce=1.9605508929521
Local test acc @ epoch 105: 0.8922
Global evaluate on test data...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.28 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.176518272916111, ce=1.9620933202040107
Global test acc @ epoch 105: 0.8922
Global epoch 106...
Client 5 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.14 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.1757978783288134, ce=1.9613901523565953
Local test acc @ epoch 106: 0.8922
Client 2 execute local training on 5 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.3 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.175290675338255, ce=1.9608345084782888
Local test acc @ epoch 106: 0.8922
Client 3 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.56 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.1756490630294203, ce=1.9612014071913106
Local test acc @ epoch 106: 0.8922
Client 1 execute local training on 6 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.21 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.1757775301780176, ce=1.961326366271772
Local test acc @ epoch 106: 0.8922
Client 7 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.17 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.175326050968345, ce=1.9608661416257231
Local test acc @ epoch 106: 0.8922
Client 9 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.2 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.174814446256795, ce=1.9603736179941536
Local test acc @ epoch 106: 0.8922
Client 6 execute local training on 6 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.42 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.175956265095177, ce=1.961521276110672
Local test acc @ epoch 106: 0.8922
Client 8 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.39 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.175773362227536, ce=1.9613347666194667
Local test acc @ epoch 106: 0.8922
Client 4 execute local training on 20 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.13 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.1835753504836233, ce=1.9690854623100487
Local test acc @ epoch 106: 0.8922
Client 0 execute local training on 23 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.29 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.186651595962157, ce=1.9723163804601624
Local test acc @ epoch 106: 0.8922
Global evaluate on test data...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.29 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.177492814873337, ce=1.9630520901070492
Global test acc @ epoch 106: 0.8922
Global epoch 107...
Client 1 execute local training on 6 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.4 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.176752599007493, ce=1.9622858298929056
Local test acc @ epoch 107: 0.8922
Client 4 execute local training on 20 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.49 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.184473720985815, ce=1.969972740435043
Local test acc @ epoch 107: 0.8922
Client 5 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.45 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.1767706417162485, ce=1.962346575877309
Local test acc @ epoch 107: 0.8922
Client 3 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.24 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.1766262483706167, ce=1.9621628433995681
Local test acc @ epoch 107: 0.8922
Client 9 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.25 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.1757984916004567, ce=1.9613422212743203
Local test acc @ epoch 107: 0.8922
Client 7 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.33 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.1763013541151626, ce=1.9618264363037798
Local test acc @ epoch 107: 0.8922
Client 2 execute local training on 5 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.27 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.17626953125, ce=1.9617978177588897
Local test acc @ epoch 107: 0.8922
Client 8 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.33 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.176748944805303, ce=1.9622942983144516
Local test acc @ epoch 107: 0.8922
Client 6 execute local training on 6 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.22 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.1769314933260646, ce=1.9624805941894237
Local test acc @ epoch 107: 0.8922
Client 0 execute local training on 23 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.44 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.1875258025226243, ce=1.9731809302526975
Local test acc @ epoch 107: 0.8922
Global evaluate on test data...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.5 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.1784505958950846, ce=1.9639949479553827
Global test acc @ epoch 107: 0.8922
Global epoch 108...
Client 2 execute local training on 5 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.66 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.177233764337837, ce=1.9627472490717786
Local test acc @ epoch 108: 0.8922
Client 5 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.43 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.177727950002075, ce=1.9632890349954013
Local test acc @ epoch 108: 0.8922
Client 3 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.25 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.1775888699457187, ce=1.9631112457334479
Local test acc @ epoch 108: 0.8922
Client 7 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.54 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.1772624541313275, ce=1.962773421689913
Local test acc @ epoch 108: 0.8922
Client 6 execute local training on 6 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.34 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.177889899923167, ce=1.9634241769901266
Local test acc @ epoch 108: 0.8922
Client 1 execute local training on 6 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.29 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.177713588290258, ce=1.9632325967717459
Local test acc @ epoch 108: 0.8922
Client 8 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.28 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.177706845309756, ce=1.9632378533279553
Local test acc @ epoch 108: 0.8922
Client 0 execute local training on 23 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.25 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.1883850526919058, ce=1.974031511362099
Local test acc @ epoch 108: 0.8922
Client 9 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.3 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.1767677172608333, ce=1.9622963233074278
Local test acc @ epoch 108: 0.8922
Client 4 execute local training on 20 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.42 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.1853610260770955, ce=1.9708501160060112
Local test acc @ epoch 108: 0.8922
Global evaluate on test data...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.52 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.1793940042683837, ce=1.9649247903125209
Global test acc @ epoch 108: 0.8922
Global epoch 109...
Client 9 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.48 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.1777227980281233, ce=1.9632370863215647
Local test acc @ epoch 109: 0.8922
Client 1 execute local training on 6 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.38 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.1786623837750985, ce=1.9641679321481837
Local test acc @ epoch 109: 0.8922
Client 6 execute local training on 6 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.22 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.1788354729293684, ce=1.9643557938625484
Local test acc @ epoch 109: 0.8922
Client 4 execute local training on 20 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.38 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.186234314233885, ce=1.971714026077581
Local test acc @ epoch 109: 0.8922
Client 7 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.28 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.178212776096589, ce=1.963710160037327
Local test acc @ epoch 109: 0.8922
Client 3 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.18 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.17853618570424, ce=1.964044797460019
Local test acc @ epoch 109: 0.8922
Client 0 execute local training on 23 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.41 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.189237287285131, ce=1.974876133477265
Local test acc @ epoch 109: 0.8922
Client 2 execute local training on 5 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.36 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.178187921506549, ce=1.9636878699697606
Local test acc @ epoch 109: 0.8922
Client 8 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.67 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.178660522907152, ce=1.964177467785592
Local test acc @ epoch 109: 0.8922
Client 5 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.4 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.1786766834215285, ce=1.9642228062809983
Local test acc @ epoch 109: 0.8922
Global evaluate on test data...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.37 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.1803218520015752, ce=1.9658394641929864
Global test acc @ epoch 109: 0.8922
Global epoch 110...
Client 2 execute local training on 5 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.23 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.179116469731025, ce=1.9646035620613398
Local test acc @ epoch 110: 0.8922
Client 1 execute local training on 6 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.29 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.1795952363845403, ce=1.9650877521089316
Local test acc @ epoch 110: 0.8922
Client 6 execute local training on 6 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.28 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.1797673510848927, ce=1.965274705248177
Local test acc @ epoch 110: 0.8922
Client 8 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.27 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.1795912909945216, ce=1.9650950628479387
Local test acc @ epoch 110: 0.8922
Client 0 execute local training on 23 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.62 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.190074447371544, ce=1.9757060818592675
Local test acc @ epoch 110: 0.8922
Client 9 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.48 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.1786666126973038, ce=1.9641676512216018
Local test acc @ epoch 110: 0.8922
Client 7 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.28 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.179147716235677, ce=1.9646323372932595
Local test acc @ epoch 110: 0.8922
Client 3 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.15 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.179469781184415, ce=1.9649655655995386
Local test acc @ epoch 110: 0.8922
Client 4 execute local training on 20 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.19 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.187101876243539, ce=1.97257316380127
Local test acc @ epoch 110: 0.8922
Client 5 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.16 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.1796101058295014, ce=1.965143117115493
Local test acc @ epoch 110: 0.8922
Global evaluate on test data...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.25 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.181242852582844, ce=1.9667482274507506
Global test acc @ epoch 110: 0.8922
Global epoch 111...
Client 8 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.31 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.1805124813263568, ce=1.966004078406807
Local test acc @ epoch 111: 0.8922
Client 6 execute local training on 6 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.58 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.1806878584240557, ce=1.9661831909659377
Local test acc @ epoch 111: 0.8922
Client 3 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.37 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.1803889394900122, ce=1.96587278509933
Local test acc @ epoch 111: 0.8922
Client 7 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.32 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.1800670694867406, ce=1.9655399563520817
Local test acc @ epoch 111: 0.8922
Client 5 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.21 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.180524373820069, ce=1.9660448102152217
Local test acc @ epoch 111: 0.8922
Client 4 execute local training on 20 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.31 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.187953686222024, ce=1.9734176308519555
Local test acc @ epoch 111: 0.8922
Client 2 execute local training on 5 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.1 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.1800474568244512, ce=1.9655222839153275
Local test acc @ epoch 111: 0.8922
Client 9 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.25 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.1795954573045084, ce=1.9650842554822223
Local test acc @ epoch 111: 0.8922
Client 0 execute local training on 23 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.32 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.190902972440107, ce=1.9765286665661586
Local test acc @ epoch 111: 0.8922
Client 1 execute local training on 6 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.53 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.180521458387375, ce=1.966002497870243
Local test acc @ epoch 111: 0.8922
Global evaluate on test data...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.39 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.182147789712346, ce=1.96764220005216
Global test acc @ epoch 111: 0.8922
Global epoch 112...
Client 3 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.21 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.181298323727529, ce=1.9667708211871495
Local test acc @ epoch 112: 0.8922
Client 9 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.27 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.1805084167270485, ce=1.9659857279909305
Local test acc @ epoch 112: 0.8922
Client 6 execute local training on 6 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.23 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.181597406164222, ce=1.96708143521583
Local test acc @ epoch 112: 0.8922
Client 0 execute local training on 23 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.23 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.1917187459425094, ce=1.9773396471388842
Local test acc @ epoch 112: 0.8922
Client 7 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.09 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.1809775864859238, ce=1.9664393986256363
Local test acc @ epoch 112: 0.8922
Client 5 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.3 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.1814323734799657, ce=1.9669410796041469
Local test acc @ epoch 112: 0.8922
Client 2 execute local training on 5 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.5 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.1809576588486315, ce=1.9664215639935103
Local test acc @ epoch 112: 0.8922
Client 1 execute local training on 6 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.33 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.18142869166278, ce=1.966898316825847
Local test acc @ epoch 112: 0.8922
Client 8 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.36 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.18142202797286, ce=1.9669025618373264
Local test acc @ epoch 112: 0.8922
Client 4 execute local training on 20 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.29 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.1887910125452446, ce=1.9742485227471973
Local test acc @ epoch 112: 0.8922
Global evaluate on test data...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.33 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.183041034215087, ce=1.96852489729109
Global test acc @ epoch 112: 0.8922
Global epoch 113...
Client 3 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.19 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.1821927015387685, ce=1.967655393051579
Local test acc @ epoch 113: 0.8922
Client 1 execute local training on 6 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.31 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.1823218347282585, ce=1.9677812374340036
Local test acc @ epoch 113: 0.8922
Client 6 execute local training on 6 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.37 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.182492895684111, ce=1.967966550551839
Local test acc @ epoch 113: 0.8922
Client 5 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.34 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.1823218541407803, ce=1.96781947741402
Local test acc @ epoch 113: 0.8922
Client 9 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.36 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.1814103378068417, ce=1.9668769239130799
Local test acc @ epoch 113: 0.8922
Client 4 execute local training on 20 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.18 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.1896186536605207, ce=1.9750707194830024
Local test acc @ epoch 113: 0.8922
Client 7 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.35 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.1818737967298665, ce=1.9673255596267594
Local test acc @ epoch 113: 0.8922
Client 2 execute local training on 5 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.2 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.181856099618684, ce=1.967309752099913
Local test acc @ epoch 113: 0.8922
Client 8 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.33 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.182317973276891, ce=1.9677875736058903
Local test acc @ epoch 113: 0.8922
Client 0 execute local training on 23 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.53 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.192526123665888, ce=1.9781435485346037
Local test acc @ epoch 113: 0.8922
Global evaluate on test data...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.44 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.183919357597281, ce=1.969393562741609
Global test acc @ epoch 113: 0.8922
Global epoch 114...
Client 9 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.25 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.182303419353765, ce=1.9677601345272748
Local test acc @ epoch 114: 0.8922
Client 1 execute local training on 6 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.19 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.1832073828495973, ce=1.968657616280988
Local test acc @ epoch 114: 0.8922
Client 6 execute local training on 6 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.32 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.1833775710622105, ce=1.968841640111696
Local test acc @ epoch 114: 0.8922
Client 0 execute local training on 23 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.1 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.1933582321219487, ce=1.9789337352139773
Local test acc @ epoch 114: 0.8922
Client 5 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.37 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.183206807856166, ce=1.9686941273375471
Local test acc @ epoch 114: 0.8922
Client 2 execute local training on 5 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.39 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.182741283823591, ce=1.9681854885783765
Local test acc @ epoch 114: 0.8922
Client 3 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.43 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.18308102510391, ce=1.9685343854607331
Local test acc @ epoch 114: 0.8922
Client 4 execute local training on 20 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.18 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.190435581251022, ce=1.9758828111362967
Local test acc @ epoch 114: 0.8922
Client 7 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.38 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.1827621984919277, ce=1.9682044828542207
Local test acc @ epoch 114: 0.8922
Client 8 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.37 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.1832058612906606, ce=1.9686663903680832
Local test acc @ epoch 114: 0.8922
Global evaluate on test data...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.31 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.1847907963695876, ce=1.9702561976533808
Global test acc @ epoch 114: 0.8922
Global epoch 115...
Client 6 execute local training on 6 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.25 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.184247567839579, ce=1.9697030936375712
Local test acc @ epoch 115: 0.8922
Client 0 execute local training on 23 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.3 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.1942147771152882, ce=1.979718012000938
Local test acc @ epoch 115: 0.8922
Client 3 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.43 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.1839542566636285, ce=1.9693986316417187
Local test acc @ epoch 115: 0.8922
Client 7 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.27 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.1836353671660116, ce=1.9690691816929535
Local test acc @ epoch 115: 0.8922
Client 9 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.36 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.1831841895339688, ce=1.9686318195044719
Local test acc @ epoch 115: 0.8922
Client 1 execute local training on 6 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.33 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.1840812317821956, ce=1.9695228569572156
Local test acc @ epoch 115: 0.8922
Client 4 execute local training on 20 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.22 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.191245192508085, ce=1.9766887258331853
Local test acc @ epoch 115: 0.8922
Client 5 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.39 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.184079907629468, ce=1.9695582030293535
Local test acc @ epoch 115: 0.8922
Client 2 execute local training on 5 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.05 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.183620926437028, ce=1.969055797718185
Local test acc @ epoch 115: 0.8922
Client 8 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.29 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.184076327796376, ce=1.9695277284937955
Local test acc @ epoch 115: 0.8922
Global evaluate on test data...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.34 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.185653390960956, ce=1.9711110283599214
Global test acc @ epoch 115: 0.8922
Global epoch 116...
Client 6 execute local training on 6 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.44 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.1851096076702854, ce=1.970557066979713
Local test acc @ epoch 116: 0.8922
Client 7 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.46 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.1844921144870444, ce=1.9699181340756342
Local test acc @ epoch 116: 0.8922
Client 0 execute local training on 23 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.36 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.195065030537614, ce=1.9804957504737917
Local test acc @ epoch 116: 0.8922
Client 5 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.23 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.1849380564799, ce=1.9704079786360467
Local test acc @ epoch 116: 0.8922
Client 1 execute local training on 6 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.24 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.1849433219214096, ce=1.9703769920888858
Local test acc @ epoch 116: 0.8922
Client 4 execute local training on 20 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.47 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.1920428106544216, ce=1.977483211136786
Local test acc @ epoch 116: 0.8922
Client 8 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.42 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.1849394128956927, ce=1.9703827662244204
Local test acc @ epoch 116: 0.8922
Client 2 execute local training on 5 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.4 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.184484554266711, ce=1.969911919028741
Local test acc @ epoch 116: 0.8922
Client 3 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.29 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.184815982612995, ce=1.9702526568810548
Local test acc @ epoch 116: 0.8922
Client 9 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.36 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.18405726524668, ce=1.9694963677127275
Local test acc @ epoch 116: 0.8922
Global evaluate on test data...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.34 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.1864948450425348, ce=1.9719456884933597
Global test acc @ epoch 116: 0.8922
Global epoch 117...
Client 0 execute local training on 23 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.15 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.195894562049743, ce=1.9812586448823664
Local test acc @ epoch 117: 0.8922
Client 5 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.11 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.1857839680592948, ce=1.9712461833022923
Local test acc @ epoch 117: 0.8922
Client 1 execute local training on 6 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.29 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.1857918473558686, ce=1.9712188113077498
Local test acc @ epoch 117: 0.8922
Client 2 execute local training on 5 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.35 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.1853344609978, ce=1.9707541126285955
Local test acc @ epoch 117: 0.8922
Client 7 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.33 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.1853448964587043, ce=1.9707637536144842
Local test acc @ epoch 117: 0.8922
Client 8 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.25 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.1857845643791585, ce=1.971221063748448
Local test acc @ epoch 117: 0.8922
Client 3 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.23 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.1856665660481935, ce=1.9710964216552098
Local test acc @ epoch 117: 0.8922
Client 9 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.24 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.1849081890845516, ce=1.9703403248242126
Local test acc @ epoch 117: 0.8922
Client 6 execute local training on 6 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.37 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.1859588929272573, ce=1.9713993377060615
Local test acc @ epoch 117: 0.8922
Client 4 execute local training on 20 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.39 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.1928305713408585, ce=1.9782680705535185
Local test acc @ epoch 117: 0.8922
Global evaluate on test data...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.22 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.187331026300378, ce=1.9727753225119458
Global test acc @ epoch 117: 0.8922
Global epoch 118...
Client 0 execute local training on 23 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.24 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.1967230584096473, ce=1.9820182769904955
Local test acc @ epoch 118: 0.8922
Client 3 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.32 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.186503927642052, ce=1.97192777746602
Local test acc @ epoch 118: 0.8922
Client 5 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.21 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.1866217578222993, ce=1.9720767357579303
Local test acc @ epoch 118: 0.8922
Client 7 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.05 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.186184128490063, ce=1.9715968309291378
Local test acc @ epoch 118: 0.8922
Client 9 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.16 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.1857563052155555, ce=1.9711815484983402
Local test acc @ epoch 118: 0.8922
Client 2 execute local training on 5 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.2 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.1861777682916834, ce=1.9715911904850394
Local test acc @ epoch 118: 0.8922
Client 6 execute local training on 6 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.24 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.186797389743525, ce=1.9722314602105586
Local test acc @ epoch 118: 0.8922
Client 1 execute local training on 6 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.42 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.186631271598536, ce=1.972051906705242
Local test acc @ epoch 118: 0.8922
Client 8 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.32 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.186625203955064, ce=1.9720551055269435
Local test acc @ epoch 118: 0.8922
Client 4 execute local training on 20 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.32 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.193609922304066, ce=1.9790456902340916
Local test acc @ epoch 118: 0.8922
Global evaluate on test data...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.14 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.1881541437512144, ce=1.9735935231577664
Global test acc @ epoch 118: 0.8922
Global epoch 119...
Client 3 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.23 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.1873304239653666, ce=1.972748266174069
Local test acc @ epoch 119: 0.8922
Client 2 execute local training on 5 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.13 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.1870026339631563, ce=1.9724105175932785
Local test acc @ epoch 119: 0.8922
Client 0 execute local training on 23 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.21 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.1975398525732373, ce=1.98276614044919
Local test acc @ epoch 119: 0.8922
Client 4 execute local training on 20 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.08 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.194377078649101, ce=1.979812158123576
Local test acc @ epoch 119: 0.8922
Client 7 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.98 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.187015963256906, ce=1.9724232460315025
Local test acc @ epoch 119: 0.8922
Client 6 execute local training on 6 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.26 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.187626148035767, ce=1.9730547577399695
Local test acc @ epoch 119: 0.8922
Client 5 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.41 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.187450790624006, ce=1.9728999700252259
Local test acc @ epoch 119: 0.8922
Client 9 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.39 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.1865905329174953, ce=1.9720097273174015
Local test acc @ epoch 119: 0.8922
Client 8 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.27 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.187450794451827, ce=1.9728748796756193
Local test acc @ epoch 119: 0.8922
Client 1 execute local training on 6 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.2 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.187464669756933, ce=1.9728795287068694
Local test acc @ epoch 119: 0.8922
Global evaluate on test data...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.04 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.188972019547716, ce=1.9744059666515184
Global test acc @ epoch 119: 0.8922
Global epoch 120...
Client 9 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.08 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.1874104626134994, ce=1.9728243311697895
Local test acc @ epoch 120: 0.8922
Client 1 execute local training on 6 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.02 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.1882756338753833, ce=1.9736857443958253
Local test acc @ epoch 120: 0.8922
Client 8 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.33 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.1882652129055162, ce=1.9736843206467984
Local test acc @ epoch 120: 0.8922
Client 3 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.35 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.1881490940347725, ce=1.9735621274334199
Local test acc @ epoch 120: 0.8922
Client 0 execute local training on 23 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.45 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.1983517402902657, ce=1.9835120312043268
Local test acc @ epoch 120: 0.8922
Client 2 execute local training on 5 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.19 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.1878285719714032, ce=1.9732311968625205
Local test acc @ epoch 120: 0.8922
Client 7 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.23 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.1878335541541425, ce=1.9732358479061534
Local test acc @ epoch 120: 0.8922
Client 5 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.22 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.1882640473339534, ce=1.9737074263765602
Local test acc @ epoch 120: 0.8922
Client 4 execute local training on 20 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.13 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.1951351852045144, ce=1.9805704080584348
Local test acc @ epoch 120: 0.8922
Client 6 execute local training on 6 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.26 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.1884444238395866, ce=1.97386772724274
Local test acc @ epoch 120: 0.8922
Global evaluate on test data...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.17 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.1897716276142574, ce=1.975201531391874
Global test acc @ epoch 120: 0.8922
Global epoch 121...
Client 5 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.23 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.1890668040568677, ce=1.974505995558723
Local test acc @ epoch 121: 0.8922
Client 3 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.22 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.188953661317125, ce=1.9743623869842204
Local test acc @ epoch 121: 0.8922
Client 0 execute local training on 23 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.22 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.1991497373909032, ce=1.984246693405009
Local test acc @ epoch 121: 0.8922
Client 1 execute local training on 6 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.03 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.1890851909961175, ce=1.974491223667359
Local test acc @ epoch 121: 0.8922
Client 8 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.18 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.1890750338724994, ce=1.9744895231614579
Local test acc @ epoch 121: 0.8922
Client 9 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.16 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.1882275198030907, ce=1.9736367307065925
Local test acc @ epoch 121: 0.8922
Client 6 execute local training on 6 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.01 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.189246810630921, ce=1.9746659398442443
Local test acc @ epoch 121: 0.8922
Client 4 execute local training on 20 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.18 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.1958862318905124, ce=1.9813220492698274
Local test acc @ epoch 121: 0.8922
Client 2 execute local training on 5 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.31 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.1886352451022613, ce=1.9740332841086436
Local test acc @ epoch 121: 0.8922
Client 7 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.13 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.1886417972385335, ce=1.9740399747985078
Local test acc @ epoch 121: 0.8922
Global evaluate on test data...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.17 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.1905655111741584, ce=1.9759925825852367
Global test acc @ epoch 121: 0.8922
Global epoch 122...
Client 9 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.2 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.189026933470997, ce=1.974432590199193
Local test acc @ epoch 122: 0.8922
Client 1 execute local training on 6 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.22 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.1898773974234906, ce=1.9752800238552541
Local test acc @ epoch 122: 0.8922
Client 0 execute local training on 23 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.19 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.1999343078617657, ce=1.9849699408473618
Local test acc @ epoch 122: 0.8922
Client 6 execute local training on 6 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.07 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.1900437816020544, ce=1.9754596002902716
Local test acc @ epoch 122: 0.8922
Client 5 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.17 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.189857262810436, ce=1.9752924166183705
Local test acc @ epoch 122: 0.8922
Client 7 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.14 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.189434784541436, ce=1.9748298114337506
Local test acc @ epoch 122: 0.8922
Client 3 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.39 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.189752147832048, ce=1.975157909344716
Local test acc @ epoch 122: 0.8922
Client 2 execute local training on 5 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.19 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.18943310276084, ce=1.974827334642779
Local test acc @ epoch 122: 0.8922
Client 4 execute local training on 20 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.13 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.1966262241022303, ce=1.9820631121541614
Local test acc @ epoch 122: 0.8922
Client 8 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.24 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.189870403720698, ce=1.9752818541089832
Local test acc @ epoch 122: 0.8922
Global evaluate on test data...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.12 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.1913501510926343, ce=1.9767745948591295
Global test acc @ epoch 122: 0.8922
Global epoch 123...
Client 6 execute local training on 6 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.19 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.1908290722501387, ce=1.9762420806386582
Local test acc @ epoch 123: 0.8922
Client 1 execute local training on 6 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.25 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.1906637748993867, ce=1.9760637405099233
Local test acc @ epoch 123: 0.8922
Client 7 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.29 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.1902198846187066, ce=1.9756122321831018
Local test acc @ epoch 123: 0.8922
Client 5 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.28 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.1906435246314477, ce=1.9760754442832424
Local test acc @ epoch 123: 0.8922
Client 8 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.18 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.190653804245345, ce=1.9760623325214342
Local test acc @ epoch 123: 0.8922
Client 2 execute local training on 5 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.18 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.190217373021152, ce=1.9756090262960444
Local test acc @ epoch 123: 0.8922
Client 3 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.16 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.190534298846481, ce=1.9759371668120873
Local test acc @ epoch 123: 0.8922
Client 4 execute local training on 20 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.18 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.1973549163122787, ce=1.9827945859699547
Local test acc @ epoch 123: 0.8922
Client 9 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.32 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.1898221895782224, ce=1.9752245120018574
Local test acc @ epoch 123: 0.8922
Client 0 execute local training on 23 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.22 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.2007139048991946, ce=1.9856889710880612
Local test acc @ epoch 123: 0.8922
Global evaluate on test data...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.11 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.192123519991516, ce=1.9775463333828773
Global test acc @ epoch 123: 0.8922
Global epoch 124...
Client 6 execute local training on 6 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.19 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.1915987459344604, ce=1.977009483722578
Local test acc @ epoch 124: 0.8922
Client 7 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.37 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.1909932573454096, ce=1.976383900868852
Local test acc @ epoch 124: 0.8922
Client 8 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.19 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.1914301049818685, ce=1.976836606739761
Local test acc @ epoch 124: 0.8922
Client 4 execute local training on 20 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.01 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.198136205520105, ce=1.9835169085811721
Local test acc @ epoch 124: 0.8922
Client 0 execute local training on 23 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.25 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.201479120538869, ce=1.986394892231922
Local test acc @ epoch 124: 0.8922
Client 2 execute local training on 5 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.19 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.191000668554131, ce=1.9763898174650536
Local test acc @ epoch 124: 0.8922
Client 3 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.12 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.1913173718189976, ce=1.976718250018946
Local test acc @ epoch 124: 0.8922
Client 9 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.2 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.1906025089802, ce=1.9760026513927162
Local test acc @ epoch 124: 0.8922
Client 1 execute local training on 6 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.43 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.1914433555318675, ce=1.9768416420255654
Local test acc @ epoch 124: 0.8922
Client 5 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.17 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.1914207697461503, ce=1.9768502979736526
Local test acc @ epoch 124: 0.8922
Global evaluate on test data...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.17 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.1928870852934113, ce=1.9783086808627035
Global test acc @ epoch 124: 0.8922
Global epoch 125...
Client 8 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.2 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.1921965332206237, ce=1.9776014845973897
Local test acc @ epoch 125: 0.8922
Client 2 execute local training on 5 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.16 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.1917674954331248, ce=1.97715510466845
Local test acc @ epoch 125: 0.8922
Client 1 execute local training on 6 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.22 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.192209176240711, ce=1.9776063081947417
Local test acc @ epoch 125: 0.8922
Client 7 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.36 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.1917615360623106, ce=1.9771507536664945
Local test acc @ epoch 125: 0.8922
Client 6 execute local training on 6 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.14 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.192367246938408, ce=1.9777770354365496
Local test acc @ epoch 125: 0.8922
Client 5 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.26 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.1921833906698662, ce=1.977611021383371
Local test acc @ epoch 125: 0.8922
Client 3 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.24 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.192080040317063, ce=1.977479680811411
Local test acc @ epoch 125: 0.8922
Client 9 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.14 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.191371726606964, ce=1.9767701600957677
Local test acc @ epoch 125: 0.8922
Client 4 execute local training on 20 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.29 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.19892191941585, ce=1.9842336283496351
Local test acc @ epoch 125: 0.8922
Client 0 execute local training on 23 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.16 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.20224367861354, ce=1.9871003737501371
Local test acc @ epoch 125: 0.8922
Global evaluate on test data...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.23 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.1936391570152494, ce=1.9790604261752855
Global test acc @ epoch 125: 0.8922
Global epoch 126...
Client 3 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.93 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.192836187848257, ce=1.978235241831465
Local test acc @ epoch 126: 0.8922
Client 5 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.34 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.192934857869367, ce=1.9783616544773142
Local test acc @ epoch 126: 0.8922
Client 6 execute local training on 6 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.16 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.1931210214938592, ce=1.9785300774553407
Local test acc @ epoch 126: 0.8922
Client 2 execute local training on 5 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.33 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.192524349470751, ce=1.9779109646980606
Local test acc @ epoch 126: 0.8922
Client 1 execute local training on 6 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.25 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.19296254258637, ce=1.9783588227940507
Local test acc @ epoch 126: 0.8922
Client 7 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.06 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.192520631562679, ce=1.9779093610576444
Local test acc @ epoch 126: 0.8922
Client 8 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.01 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.192951661731125, ce=1.9783560577887227
Local test acc @ epoch 126: 0.8922
Client 0 execute local training on 23 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.2 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.202994960710543, ce=1.9877940959873932
Local test acc @ epoch 126: 0.8922
Client 4 execute local training on 20 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.04 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.199695696251108, ce=1.9849444262649192
Local test acc @ epoch 126: 0.8922
Client 9 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.3 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.192138140901513, ce=1.977535643958949
Local test acc @ epoch 126: 0.8922
Global evaluate on test data...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.22 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.1943832799928997, ce=1.9798049487635598
Global test acc @ epoch 126: 0.8922
Global epoch 127...
Client 5 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.11 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.193683818939629, ce=1.9791100192801128
Local test acc @ epoch 127: 0.8922
Client 4 execute local training on 20 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.22 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.200463304825879, ce=1.9856463924428898
Local test acc @ epoch 127: 0.8922
Client 7 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.12 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.193270389913419, ce=1.9786594210071844
Local test acc @ epoch 127: 0.8922
Client 8 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.27 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.1937026296733717, ce=1.9791068266919545
Local test acc @ epoch 127: 0.8922
Client 9 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.37 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.1928895588861694, ce=1.9782864482230553
Local test acc @ epoch 127: 0.8922
Client 2 execute local training on 5 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.47 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.1932763725245765, ce=1.9786627816976363
Local test acc @ epoch 127: 0.8922
Client 6 execute local training on 6 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.28 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.193870217428295, ce=1.9792793592494993
Local test acc @ epoch 127: 0.8922
Client 1 execute local training on 6 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.2 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.1937137926937242, ce=1.9791106813075705
Local test acc @ epoch 127: 0.8922
Client 3 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.27 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.193583687511059, ce=1.978983167728091
Local test acc @ epoch 127: 0.8922
Client 0 execute local training on 23 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.33 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.2037396846561257, ce=1.9884835600338926
Local test acc @ epoch 127: 0.8922
Global evaluate on test data...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.11 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.195120569216002, ce=1.9805436589648362
Global test acc @ epoch 127: 0.8922
Global epoch 128...
Client 5 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.08 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.1944190127040266, ce=1.9798463491514593
Local test acc @ epoch 128: 0.8922
Client 2 execute local training on 5 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.27 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.1940158810637413, ce=1.9794026243820904
Local test acc @ epoch 128: 0.8922
Client 8 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.2 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.19443848701792, ce=1.9798436614653547
Local test acc @ epoch 128: 0.8922
Client 9 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.14 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.1936316096454584, ce=1.9790294195340468
Local test acc @ epoch 128: 0.8922
Client 4 execute local training on 20 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.35 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.2012165369243797, ce=1.9863382471368458
Local test acc @ epoch 128: 0.8922
Client 3 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.27 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.1943251359353373, ce=1.979725314844758
Local test acc @ epoch 128: 0.8922
Client 1 execute local training on 6 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.11 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.1944509203280878, ce=1.9798486645938234
Local test acc @ epoch 128: 0.8922
Client 0 execute local training on 23 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.27 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.2044715482160586, ce=1.9891620639580385
Local test acc @ epoch 128: 0.8922
Client 7 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.15 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.1940098123266063, ce=1.9793995200723498
Local test acc @ epoch 128: 0.8922
Client 6 execute local training on 6 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.52 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.194611299201983, ce=1.9800214603138875
Local test acc @ epoch 128: 0.8922
Global evaluate on test data...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.27 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.1958506902970307, ce=1.9812758047320012
Global test acc @ epoch 128: 0.8922
Global epoch 129...
Client 3 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.03 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.1950562115109293, ce=1.9804582512079965
Local test acc @ epoch 129: 0.8922
Client 9 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.13 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.1943673067683473, ce=1.97976594016043
Local test acc @ epoch 129: 0.8922
Client 1 execute local training on 6 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.12 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.195182590036217, ce=1.9805822087367546
Local test acc @ epoch 129: 0.8922
Client 7 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.07 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.194739949812583, ce=1.9801310769947922
Local test acc @ epoch 129: 0.8922
Client 0 execute local training on 23 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.21 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.2051983355382165, ce=1.9898357820195918
Local test acc @ epoch 129: 0.8922
Client 8 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.45 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.195168861281981, ce=1.9805759715741091
Local test acc @ epoch 129: 0.8922
Client 4 execute local training on 20 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.21 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.201969605246815, ce=1.9870287908535673
Local test acc @ epoch 129: 0.8922
Client 2 execute local training on 5 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.12 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.1947532695367795, ce=1.9801418514737974
Local test acc @ epoch 129: 0.8922
Client 5 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.19 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.1951519316489545, ce=1.9805799916160705
Local test acc @ epoch 129: 0.8922
Client 6 execute local training on 6 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.14 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.1953403293539626, ce=1.9807523690051183
Local test acc @ epoch 129: 0.8922
Global evaluate on test data...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.21 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.1965636616453117, ce=1.9819915113488449
Global test acc @ epoch 129: 0.8922
Global epoch 130...
Client 3 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.19 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.1957739236157967, ce=1.9811782407411986
Local test acc @ epoch 130: 0.8922
Client 0 execute local training on 23 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.23 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.2059154160525822, ce=1.990501692318989
Local test acc @ epoch 130: 0.8922
Client 1 execute local training on 6 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.25 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.1959040137605927, ce=1.9813057458764356
Local test acc @ epoch 130: 0.8922
Client 7 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.4 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.195464249597777, ce=1.9808578390078968
Local test acc @ epoch 130: 0.8922
Client 5 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.14 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.1958693719785147, ce=1.981299594721028
Local test acc @ epoch 130: 0.8922
Client 9 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.32 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.195096430428531, ce=1.980497305319469
Local test acc @ epoch 130: 0.8922
Client 4 execute local training on 20 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.2 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.2027094462595946, ce=1.9877081349681547
Local test acc @ epoch 130: 0.8922
Client 8 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.17 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.195892455927823, ce=1.9813017183504253
Local test acc @ epoch 130: 0.8922
Client 6 execute local training on 6 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.34 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.1960597390975427, ce=1.9814739811326691
Local test acc @ epoch 130: 0.8922
Client 2 execute local training on 5 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.31 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.195477557565094, ce=1.9808676209181197
Local test acc @ epoch 130: 0.8922
Global evaluate on test data...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.22 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.19728596440149, ce=1.9827117645773094
Global test acc @ epoch 130: 0.8922
Global epoch 131...
Client 1 execute local training on 6 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.36 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.196617334534269, ce=1.9820220268918132
Local test acc @ epoch 131: 0.8922
Client 2 execute local training on 5 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.11 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.1961884329078396, ce=1.981581020083155
Local test acc @ epoch 131: 0.8922
Client 0 execute local training on 23 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.25 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.206628230733609, ce=1.9911647948681277
Local test acc @ epoch 131: 0.8922
Client 9 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.06 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.1958163509128292, ce=1.9812198549096953
Local test acc @ epoch 131: 0.8922
Client 8 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.23 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.196607661083204, ce=1.982019264008949
Local test acc @ epoch 131: 0.8922
Client 7 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.22 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.196178852418147, ce=1.9815754789964635
Local test acc @ epoch 131: 0.8922
Client 6 execute local training on 6 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.2 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.196773097602599, ce=1.982190242389896
Local test acc @ epoch 131: 0.8922
Client 3 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.21 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.1964923422270957, ce=1.9818997569955275
Local test acc @ epoch 131: 0.8922
Client 4 execute local training on 20 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.06 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.203444384653634, ce=1.9883837141885303
Local test acc @ epoch 131: 0.8922
Client 5 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.99 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.1965841948986053, ce=1.9820171733063972
Local test acc @ epoch 131: 0.8922
Global evaluate on test data...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.33 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.1980541736707773, ce=1.9834141998188928
Global test acc @ epoch 131: 0.8922
Global epoch 132...
Client 5 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.35 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.197283520064223, ce=1.9827194467265445
Local test acc @ epoch 132: 0.8922
Client 3 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.37 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.1971918987024814, ce=1.9826032974443886
Local test acc @ epoch 132: 0.8922
Client 4 execute local training on 20 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.24 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.2041661449528616, ce=1.9890476162517676
Local test acc @ epoch 132: 0.8922
Client 2 execute local training on 5 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.16 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.1968949937492335, ce=1.9822907964211371
Local test acc @ epoch 132: 0.8922
Client 7 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.12 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.196883864905856, ce=1.9822841396783553
Local test acc @ epoch 132: 0.8922
Client 0 execute local training on 23 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.38 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.2073319628151187, ce=1.9918207990334529
Local test acc @ epoch 132: 0.8922
Client 6 execute local training on 6 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.15 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.197478650906764, ce=1.9828995297498835
Local test acc @ epoch 132: 0.8922
Client 9 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.23 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.196530746210606, ce=1.9819375179880172
Local test acc @ epoch 132: 0.8922
Client 1 execute local training on 6 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.12 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.1973266847636723, ce=1.9827351585174902
Local test acc @ epoch 132: 0.8922
Client 8 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.22 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.1973127676259487, ce=1.9827283902612056
Local test acc @ epoch 132: 0.8922
Global evaluate on test data...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.2 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.1988211509284623, ce=1.9841160481439641
Global test acc @ epoch 132: 0.8922
Global epoch 133...
Client 0 execute local training on 23 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.07 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.2080301481102587, ce=1.9924714547308593
Local test acc @ epoch 133: 0.8922
Client 3 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.02 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.1979400102698476, ce=1.9833091688546822
Local test acc @ epoch 133: 0.8922
Client 4 execute local training on 20 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.14 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.2048860126679095, ce=1.989711858999903
Local test acc @ epoch 133: 0.8922
Client 5 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.09 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.1980182572242315, ce=1.9834212914108293
Local test acc @ epoch 133: 0.8922
Client 2 execute local training on 5 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.26 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.1976009864325916, ce=1.9830010365326545
Local test acc @ epoch 133: 0.8922
Client 8 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.23 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.19805798727438, ce=1.983427662774846
Local test acc @ epoch 133: 0.8922
Client 6 execute local training on 6 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.17 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.198237932056462, ce=1.9836007806177618
Local test acc @ epoch 133: 0.8922
Client 1 execute local training on 6 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.1 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.1980710579167813, ce=1.9834363970231867
Local test acc @ epoch 133: 0.8922
Client 9 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.1 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.1972317312835554, ce=1.9826420225609525
Local test acc @ epoch 133: 0.8922
Client 7 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.07 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.1976117029649402, ce=1.982986905344944
Local test acc @ epoch 133: 0.8922
Global evaluate on test data...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.15 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.199577048557614, ce=1.9848085292360564
Global test acc @ epoch 133: 0.8922
Global epoch 134...
Client 9 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.21 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.197966342125464, ce=1.9833382702706386
Local test acc @ epoch 134: 0.8922
Client 5 execute local training on 4 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.31 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.1987754232292875, ce=1.9841126584523334
Local test acc @ epoch 134: 0.8922
Client 1 execute local training on 6 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
