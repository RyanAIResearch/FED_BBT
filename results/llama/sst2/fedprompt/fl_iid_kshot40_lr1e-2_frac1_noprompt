Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:09<00:09,  9.53s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:12<00:00,  5.54s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:12<00:00,  6.14s/it]
Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at meta-llama/Llama-2-7b-hf and are newly initialized: ['score.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Found cached dataset glue (/root/.cache/huggingface/datasets/glue/sst2/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)
Map:   0%|          | 0/67349 [00:00<?, ? examples/s]Map:   1%|          | 542/67349 [00:00<00:12, 5347.85 examples/s]Map:   2%|▏         | 1088/67349 [00:00<00:12, 5408.73 examples/s]Map:   2%|▏         | 1631/67349 [00:00<00:14, 4410.48 examples/s]Map:   3%|▎         | 2311/67349 [00:00<00:14, 4378.54 examples/s]Map:   4%|▍         | 2823/67349 [00:00<00:14, 4592.32 examples/s]Map:   5%|▍         | 3363/67349 [00:00<00:13, 4818.25 examples/s]Map:   6%|▌         | 3902/67349 [00:00<00:12, 4985.39 examples/s]Map:   7%|▋         | 4463/67349 [00:00<00:12, 5170.16 examples/s]Map:   7%|▋         | 4999/67349 [00:01<00:11, 5224.07 examples/s]Map:   9%|▊         | 5766/67349 [00:01<00:11, 5179.58 examples/s]Map:   9%|▉         | 6345/67349 [00:01<00:11, 5342.38 examples/s]Map:  10%|█         | 6962/67349 [00:01<00:10, 5571.38 examples/s]Map:  11%|█         | 7539/67349 [00:01<00:10, 5627.74 examples/s]Map:  12%|█▏        | 8154/67349 [00:01<00:11, 4990.52 examples/s]Map:  13%|█▎        | 8872/67349 [00:01<00:11, 4916.85 examples/s]Map:  14%|█▍        | 9621/67349 [00:01<00:11, 4938.85 examples/s]Map:  15%|█▌        | 10413/67349 [00:02<00:11, 5043.97 examples/s]Map:  16%|█▌        | 10923/67349 [00:02<00:11, 5056.08 examples/s]Map:  17%|█▋        | 11500/67349 [00:02<00:10, 5233.87 examples/s]Map:  18%|█▊        | 12078/67349 [00:02<00:10, 5375.61 examples/s]Map:  19%|█▉        | 12921/67349 [00:02<00:09, 5462.05 examples/s]Map:  20%|██        | 13497/67349 [00:02<00:09, 5536.62 examples/s]Map:  21%|██        | 14308/67349 [00:02<00:09, 5487.85 examples/s]Map:  22%|██▏       | 14862/67349 [00:02<00:09, 5498.16 examples/s]Map:  23%|██▎       | 15422/67349 [00:02<00:09, 5522.97 examples/s]Map:  24%|██▍       | 16199/67349 [00:03<00:09, 5336.61 examples/s]Map:  25%|██▌       | 17000/67349 [00:03<00:09, 5186.77 examples/s]Map:  26%|██▌       | 17532/67349 [00:03<00:09, 5214.77 examples/s]Map:  27%|██▋       | 18099/67349 [00:03<00:09, 5329.08 examples/s]Map:  28%|██▊       | 18670/67349 [00:03<00:08, 5427.33 examples/s]Map:  29%|██▉       | 19455/67349 [00:03<00:10, 4769.44 examples/s]Map:  30%|██▉       | 20020/67349 [00:03<00:09, 4974.61 examples/s]Map:  31%|███       | 20545/67349 [00:03<00:09, 5040.79 examples/s]Map:  32%|███▏      | 21304/67349 [00:04<00:09, 4898.56 examples/s]Map:  32%|███▏      | 21820/67349 [00:04<00:09, 4959.51 examples/s]Map:  33%|███▎      | 22371/67349 [00:04<00:08, 5099.14 examples/s]Map:  34%|███▍      | 22985/67349 [00:04<00:08, 5377.28 examples/s]Map:  35%|███▌      | 23710/67349 [00:04<00:08, 5112.55 examples/s]Map:  36%|███▋      | 24512/67349 [00:04<00:08, 5104.46 examples/s]Map:  37%|███▋      | 25172/67349 [00:04<00:08, 4873.20 examples/s]Map:  38%|███▊      | 25709/67349 [00:05<00:08, 4988.62 examples/s]Map:  39%|███▉      | 26420/67349 [00:05<00:08, 4823.22 examples/s]Map:  40%|████      | 27000/67349 [00:05<00:08, 5030.05 examples/s]Map:  41%|████      | 27734/67349 [00:05<00:07, 4981.21 examples/s]Map:  42%|████▏     | 28468/67349 [00:05<00:07, 4950.07 examples/s]Map:  43%|████▎     | 29013/67349 [00:05<00:07, 5067.40 examples/s]Map:  44%|████▍     | 29615/67349 [00:05<00:07, 5305.93 examples/s]Map:  45%|████▌     | 30397/67349 [00:05<00:07, 5269.31 examples/s]Map:  46%|████▌     | 30957/67349 [00:06<00:06, 5349.87 examples/s]Map:  47%|████▋     | 31507/67349 [00:06<00:06, 5386.85 examples/s]Map:  48%|████▊     | 32319/67349 [00:06<00:06, 5394.27 examples/s]Map:  49%|████▉     | 32910/67349 [00:06<00:06, 5524.73 examples/s]Map:  50%|████▉     | 33650/67349 [00:06<00:06, 5312.07 examples/s]Map:  51%|█████     | 34291/67349 [00:06<00:06, 4850.29 examples/s]Map:  52%|█████▏    | 34905/67349 [00:06<00:06, 5154.20 examples/s]Map:  53%|█████▎    | 35461/67349 [00:06<00:06, 5253.86 examples/s]Map:  53%|█████▎    | 36000/67349 [00:06<00:06, 5201.78 examples/s]Map:  54%|█████▍    | 36539/67349 [00:07<00:05, 5251.21 examples/s]Map:  55%|█████▌    | 37329/67349 [00:07<00:05, 5207.07 examples/s]Map:  56%|█████▋    | 38016/67349 [00:07<00:05, 4988.84 examples/s]Map:  58%|█████▊    | 38755/67349 [00:07<00:05, 4964.99 examples/s]Map:  59%|█████▊    | 39473/67349 [00:07<00:05, 4904.78 examples/s]Map:  59%|█████▉    | 40049/67349 [00:07<00:05, 5102.91 examples/s]Map:  60%|██████    | 40592/67349 [00:07<00:05, 5180.08 examples/s]Map:  61%|██████▏   | 41413/67349 [00:08<00:04, 5279.27 examples/s]Map:  62%|██████▏   | 41992/67349 [00:08<00:04, 5404.37 examples/s]Map:  64%|██████▎   | 42808/67349 [00:08<00:04, 5235.00 examples/s]Map:  64%|██████▍   | 43386/67349 [00:08<00:04, 5363.66 examples/s]Map:  65%|██████▌   | 44000/67349 [00:08<00:04, 5477.18 examples/s]Map:  66%|██████▌   | 44616/67349 [00:08<00:04, 5655.40 examples/s]Map:  67%|██████▋   | 45309/67349 [00:08<00:04, 5274.75 examples/s]Map:  68%|██████▊   | 46088/67349 [00:08<00:04, 5244.16 examples/s]Map:  69%|██████▉   | 46702/67349 [00:09<00:03, 5462.03 examples/s]Map:  70%|███████   | 47305/67349 [00:09<00:03, 5545.72 examples/s]Map:  71%|███████   | 47921/67349 [00:09<00:03, 5706.30 examples/s]Map:  72%|███████▏  | 48540/67349 [00:09<00:03, 5128.55 examples/s]Map:  73%|███████▎  | 49254/67349 [00:09<00:03, 5000.52 examples/s]Map:  74%|███████▍  | 49782/67349 [00:09<00:03, 5067.43 examples/s]Map:  75%|███████▌  | 50560/67349 [00:09<00:03, 5088.15 examples/s]Map:  76%|███████▋  | 51371/67349 [00:09<00:03, 5188.41 examples/s]Map:  77%|███████▋  | 51906/67349 [00:10<00:02, 5226.50 examples/s]Map:  78%|███████▊  | 52653/67349 [00:10<00:02, 5138.64 examples/s]Map:  79%|███████▉  | 53185/67349 [00:10<00:03, 4617.42 examples/s]Map:  80%|███████▉  | 53794/67349 [00:10<00:02, 4963.78 examples/s]Map:  81%|████████  | 54316/67349 [00:10<00:02, 5025.20 examples/s]Map:  82%|████████▏ | 55024/67349 [00:10<00:02, 4916.37 examples/s]Map:  82%|████████▏ | 55559/67349 [00:10<00:02, 5021.97 examples/s]Map:  84%|████████▎ | 56245/67349 [00:10<00:02, 4863.00 examples/s]Map:  84%|████████▍ | 56859/67349 [00:11<00:02, 5179.56 examples/s]Map:  85%|████████▌ | 57431/67349 [00:11<00:01, 5319.17 examples/s]Map:  86%|████████▌ | 58003/67349 [00:11<00:01, 5426.00 examples/s]Map:  87%|████████▋ | 58610/67349 [00:11<00:01, 5604.49 examples/s]Map:  88%|████████▊ | 59366/67349 [00:11<00:01, 5389.36 examples/s]Map:  89%|████████▉ | 60101/67349 [00:11<00:01, 5196.88 examples/s]Map:  90%|█████████ | 60665/67349 [00:11<00:01, 5303.65 examples/s]Map:  91%|█████████ | 61424/67349 [00:11<00:01, 5215.97 examples/s]Map:  92%|█████████▏| 62000/67349 [00:11<00:01, 5304.34 examples/s]Map:  93%|█████████▎| 62610/67349 [00:12<00:00, 5509.36 examples/s]Map:  94%|█████████▍| 63333/67349 [00:12<00:00, 5232.63 examples/s]Map:  95%|█████████▌| 64025/67349 [00:12<00:00, 4982.91 examples/s]Map:  96%|█████████▌| 64565/67349 [00:12<00:00, 5081.27 examples/s]Map:  97%|█████████▋| 65140/67349 [00:12<00:00, 5251.27 examples/s]Map:  98%|█████████▊| 65760/67349 [00:12<00:00, 5503.18 examples/s]Map:  99%|█████████▊| 66475/67349 [00:12<00:00, 5231.57 examples/s]Map: 100%|█████████▉| 67275/67349 [00:12<00:00, 5227.83 examples/s]                                                                   Example in train set:
{'sentence': 'hide new secretions from the parental units ', 'label': 0, 'idx': 0, 'input_text': 'iedER statfigme von interroidater their bet ein}\\"> sub op donty try Pro tra sameep two nameoldlet simsp avbreblemey could cor accayscreurrsi constues}$View act bo ко som aboutland . hide new secretions from the parental units  . The sentiment is', 'target_text': 'negative'}
Map:   0%|          | 0/67349 [00:00<?, ? examples/s]Map:   3%|▎         | 2000/67349 [00:00<00:04, 13571.14 examples/s]Map:   6%|▌         | 4000/67349 [00:00<00:03, 16216.11 examples/s]Map:   9%|▉         | 6000/67349 [00:00<00:04, 14636.48 examples/s]Map:  12%|█▏        | 8000/67349 [00:00<00:05, 9924.35 examples/s] Map:  15%|█▍        | 10000/67349 [00:00<00:06, 9412.91 examples/s]Map:  18%|█▊        | 12000/67349 [00:01<00:05, 10362.65 examples/s]Map:  21%|██        | 14000/67349 [00:01<00:04, 10737.17 examples/s]Map:  24%|██▍       | 16000/67349 [00:01<00:05, 8893.99 examples/s] Map:  27%|██▋       | 18000/67349 [00:01<00:04, 10025.42 examples/s]Map:  30%|██▉       | 20000/67349 [00:01<00:04, 11476.10 examples/s]Map:  33%|███▎      | 22000/67349 [00:01<00:03, 12257.82 examples/s]Map:  36%|███▌      | 24000/67349 [00:02<00:03, 11345.84 examples/s]Map:  39%|███▊      | 26000/67349 [00:02<00:04, 10199.61 examples/s]Map:  42%|████▏     | 28000/67349 [00:02<00:03, 10846.46 examples/s]Map:  45%|████▍     | 30000/67349 [00:02<00:03, 11746.99 examples/s]Map:  48%|████▊     | 32000/67349 [00:02<00:02, 13321.20 examples/s]Map:  50%|█████     | 34000/67349 [00:03<00:02, 12507.44 examples/s]Map:  53%|█████▎    | 36000/67349 [00:03<00:03, 9935.09 examples/s] Map:  56%|█████▋    | 38000/67349 [00:03<00:02, 11077.69 examples/s]Map:  59%|█████▉    | 40000/67349 [00:03<00:02, 11469.52 examples/s]Map:  62%|██████▏   | 42000/67349 [00:03<00:02, 11989.61 examples/s]Map:  65%|██████▌   | 44000/67349 [00:03<00:01, 13615.03 examples/s]Map:  68%|██████▊   | 46000/67349 [00:04<00:01, 11902.37 examples/s]Map:  71%|███████▏  | 48000/67349 [00:04<00:01, 11680.88 examples/s]Map:  74%|███████▍  | 50000/67349 [00:04<00:01, 11731.81 examples/s]Map:  77%|███████▋  | 52000/67349 [00:04<00:01, 12133.82 examples/s]Map:  82%|████████▏ | 55000/67349 [00:04<00:01, 11389.84 examples/s]Map:  85%|████████▍ | 57000/67349 [00:04<00:00, 12044.84 examples/s]Map:  88%|████████▊ | 59000/67349 [00:05<00:00, 12433.69 examples/s]Map:  91%|█████████ | 61000/67349 [00:05<00:00, 12309.49 examples/s]Map:  94%|█████████▎| 63000/67349 [00:05<00:00, 12561.93 examples/s]Map:  97%|█████████▋| 65000/67349 [00:05<00:00, 9178.50 examples/s] Map:  99%|█████████▉| 67000/67349 [00:05<00:00, 10869.54 examples/s]                                                                    Found cached dataset glue (/root/.cache/huggingface/datasets/glue/sst2/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)
Map:   0%|          | 0/872 [00:00<?, ? examples/s]Map:  55%|█████▍    | 476/872 [00:00<00:00, 4588.92 examples/s]                                                               Example in validation set:
{'sentence': "it 's a charming and often affecting journey . ", 'label': 1, 'idx': 0, 'input_text': 'iedER statfigme von interroidater their bet ein}\\"> sub op donty try Pro tra sameep two nameoldlet simsp avbreblemey could cor accayscreurrsi constues}$View act bo ко som aboutland . it \'s a charming and often affecting journey .  . The sentiment is', 'target_text': 'positive'}
Map:   0%|          | 0/872 [00:00<?, ? examples/s]                                                   92
92
# of train data: 80
Example:
+------------------------------+------------------------------+--------+
| input_ids                    | attention_mask               | labels |
+------------------------------+------------------------------+--------+
| [1, 474, 287, 1001, 1002,... | [1, 1, 1, 1, 1, 1, 1, 1, ... | 8178   |
+------------------------------+------------------------------+--------+

# of dev data: 80
Example:
+------------------------------+------------------------------+--------+
| input_ids                    | attention_mask               | labels |
+------------------------------+------------------------------+--------+
| [1, 474, 287, 1001, 1002,... | [1, 1, 1, 1, 1, 1, 1, 1, ... | 6374   |
+------------------------------+------------------------------+--------+

# of test data: 872
Example:
+------------------------------+------------------------------+--------+
| input_ids                    | attention_mask               | labels |
+------------------------------+------------------------------+--------+
| [1, 474, 287, 1001, 1002,... | [1, 1, 1, 1, 1, 1, 1, 1, ... | 6374   |
+------------------------------+------------------------------+--------+
Global epoch 0...
Client 4 execute local training on 8 samples...
Local loss @ local epoch 0: 0.47666245698928833
Local loss @ local epoch 1: 0.4160783290863037
Local loss @ local epoch 2: 0.3842926621437073
Local loss @ local epoch 3: 0.36805206537246704
Local loss @ local epoch 4: 0.3541548252105713
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.4 seconds!
[tester] 
SST2Metric: acc=0.5, hinge=2.053709694004934, ce=0.9056222693088951
Local test acc @ epoch 0: 0.5
Client 6 execute local training on 8 samples...
Local loss @ local epoch 0: 0.7422429323196411
Local loss @ local epoch 1: 0.6169811487197876
Local loss @ local epoch 2: 0.5345720052719116
Local loss @ local epoch 3: 0.49158111214637756
Local loss @ local epoch 4: 0.47792649269104004
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.0 seconds!
[tester] 
SST2Metric: acc=0.4782110091743119, hinge=2.427112257261889, ce=1.3780069464663847
Local test acc @ epoch 0: 0.4782
Client 8 execute local training on 8 samples...
Local loss @ local epoch 0: 1.5789464712142944
Local loss @ local epoch 1: 1.4692600965499878
Local loss @ local epoch 2: 1.3740609884262085
Local loss @ local epoch 3: 1.2907764911651611
Local loss @ local epoch 4: 1.2161394357681274
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.13 seconds!
[tester] 
SST2Metric: acc=0.5160550458715596, hinge=1.9425996088106698, ce=0.8283142414661723
Local test acc @ epoch 0: 0.5161
Client 2 execute local training on 8 samples...
Local loss @ local epoch 0: 0.7879156470298767
Local loss @ local epoch 1: 0.6320616602897644
Local loss @ local epoch 2: 0.5224983096122742
Local loss @ local epoch 3: 0.45662471652030945
Local loss @ local epoch 4: 0.4257681667804718
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.03 seconds!
[tester] 
SST2Metric: acc=0.4908256880733945, hinge=2.4219118616449724, ce=1.4324098521963171
Local test acc @ epoch 0: 0.4908
Client 5 execute local training on 8 samples...
Local loss @ local epoch 0: 1.6390800476074219
Local loss @ local epoch 1: 1.3879071474075317
Local loss @ local epoch 2: 1.1580418348312378
Local loss @ local epoch 3: 0.957943320274353
Local loss @ local epoch 4: 0.7964996695518494
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.14 seconds!
[tester] 
SST2Metric: acc=0.5504587155963303, hinge=1.8836116845454645, ce=0.7738414825102605
Local test acc @ epoch 0: 0.5505
Client 7 execute local training on 8 samples...
Local loss @ local epoch 0: 0.6260624527931213
Local loss @ local epoch 1: 0.567029595375061
Local loss @ local epoch 2: 0.533422589302063
Local loss @ local epoch 3: 0.5121862292289734
Local loss @ local epoch 4: 0.4940585792064667
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.1 seconds!
[tester] 
SST2Metric: acc=0.569954128440367, hinge=1.8034917887197721, ce=0.7932532666199797
Local test acc @ epoch 0: 0.57
Client 3 execute local training on 8 samples...
Local loss @ local epoch 0: 0.6450772285461426
Local loss @ local epoch 1: 0.5654152035713196
Local loss @ local epoch 2: 0.524566650390625
Local loss @ local epoch 3: 0.5153267979621887
Local loss @ local epoch 4: 0.5192046165466309
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.09 seconds!
[tester] 
SST2Metric: acc=0.5802752293577982, hinge=1.7400918750587953, ce=0.6846745175505997
Local test acc @ epoch 0: 0.5803
Client 9 execute local training on 8 samples...
Local loss @ local epoch 0: 0.5737223625183105
Local loss @ local epoch 1: 0.5383208394050598
Local loss @ local epoch 2: 0.5053208470344543
Local loss @ local epoch 3: 0.4744328558444977
Local loss @ local epoch 4: 0.44561827182769775
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.08 seconds!
[tester] 
SST2Metric: acc=0.6639908256880734, hinge=1.6030134777410314, ce=0.6149758893415469
Local test acc @ epoch 0: 0.664
Client 1 execute local training on 8 samples...
Local loss @ local epoch 0: 0.6404260993003845
Local loss @ local epoch 1: 0.6057661175727844
Local loss @ local epoch 2: 0.5870804786682129
Local loss @ local epoch 3: 0.5676959156990051
Local loss @ local epoch 4: 0.5450972318649292
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.05 seconds!
[tester] 
SST2Metric: acc=0.6204128440366973, hinge=1.5841077522400322, ce=0.6626101111600159
Local test acc @ epoch 0: 0.6204
Client 0 execute local training on 8 samples...
Local loss @ local epoch 0: 0.7675663232803345
Local loss @ local epoch 1: 0.6155170798301697
Local loss @ local epoch 2: 0.5078203082084656
Local loss @ local epoch 3: 0.44263914227485657
Local loss @ local epoch 4: 0.4127608835697174
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.27 seconds!
[tester] 
SST2Metric: acc=0.6146788990825688, hinge=1.5769823505244125, ce=0.6680802850548281
Local test acc @ epoch 0: 0.6147
Global evaluate on test data...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.84 seconds!
[tester] 
SST2Metric: acc=0.5722477064220184, hinge=1.8127848511442133, ce=0.6869466274703314
Global test acc @ epoch 0: 0.5722
Global epoch 1...
Client 2 execute local training on 8 samples...
Local loss @ local epoch 0: 0.694198489189148
Local loss @ local epoch 1: 0.566871166229248
Local loss @ local epoch 2: 0.4851677119731903
Local loss @ local epoch 3: 0.44243890047073364
Local loss @ local epoch 4: 0.4265083372592926
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.18 seconds!
[tester] 
SST2Metric: acc=0.4919724770642202, hinge=2.286308951334122, ce=1.3210888409286463
Local test acc @ epoch 1: 0.492
Client 5 execute local training on 8 samples...
Local loss @ local epoch 0: 1.5666128396987915
Local loss @ local epoch 1: 1.3181957006454468
Local loss @ local epoch 2: 1.092319130897522
Local loss @ local epoch 3: 0.8976238369941711
Local loss @ local epoch 4: 0.7428581714630127
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.22 seconds!
[tester] 
SST2Metric: acc=0.5871559633027523, hinge=1.7648901080866473, ce=0.6955434261658869
Local test acc @ epoch 1: 0.5872
Client 9 execute local training on 8 samples...
Local loss @ local epoch 0: 0.6097767949104309
Local loss @ local epoch 1: 0.571031391620636
Local loss @ local epoch 2: 0.5367007255554199
Local loss @ local epoch 3: 0.5040993690490723
Local loss @ local epoch 4: 0.4731883108615875
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.13 seconds!
[tester] 
SST2Metric: acc=0.6422018348623854, hinge=1.6356339241386553, ce=0.6292340194960253
Local test acc @ epoch 1: 0.6422
Client 7 execute local training on 8 samples...
Local loss @ local epoch 0: 0.6165034174919128
Local loss @ local epoch 1: 0.5088781118392944
Local loss @ local epoch 2: 0.4445093274116516
Local loss @ local epoch 3: 0.41525718569755554
Local loss @ local epoch 4: 0.40776118636131287
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.2 seconds!
[tester] 
SST2Metric: acc=0.5584862385321101, hinge=1.7129928063635433, ce=0.8041485459159273
Local test acc @ epoch 1: 0.5585
Client 8 execute local training on 8 samples...
Local loss @ local epoch 0: 0.9592856764793396
Local loss @ local epoch 1: 0.8355669975280762
Local loss @ local epoch 2: 0.7443655729293823
Local loss @ local epoch 3: 0.6846570372581482
Local loss @ local epoch 4: 0.6484588384628296
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.12 seconds!
[tester] 
SST2Metric: acc=0.698394495412844, hinge=1.5300544773766753, ce=0.5663140170071104
Local test acc @ epoch 1: 0.6984
Client 3 execute local training on 8 samples...
Local loss @ local epoch 0: 0.5069093704223633
Local loss @ local epoch 1: 0.4847033619880676
Local loss @ local epoch 2: 0.47075116634368896
Local loss @ local epoch 3: 0.4563635587692261
Local loss @ local epoch 4: 0.44387516379356384
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.17 seconds!
[tester] 
SST2Metric: acc=0.6915137614678899, hinge=1.517462750640484, ce=0.565294441280015
Local test acc @ epoch 1: 0.6915
Client 1 execute local training on 8 samples...
Local loss @ local epoch 0: 0.6464439034461975
Local loss @ local epoch 1: 0.5675534605979919
Local loss @ local epoch 2: 0.5301299095153809
Local loss @ local epoch 3: 0.5188992023468018
Local loss @ local epoch 4: 0.5146763324737549
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.01 seconds!
[tester] 
SST2Metric: acc=0.5871559633027523, hinge=1.6217314852487057, ce=0.7281810065748495
Local test acc @ epoch 1: 0.5872
Client 6 execute local training on 8 samples...
Local loss @ local epoch 0: 0.5723356604576111
Local loss @ local epoch 1: 0.5237995982170105
Local loss @ local epoch 2: 0.5009554624557495
Local loss @ local epoch 3: 0.48845240473747253
Local loss @ local epoch 4: 0.4734460115432739
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.17 seconds!
[tester] 
SST2Metric: acc=0.6559633027522935, hinge=1.5454579269120452, ce=0.616150769755381
Local test acc @ epoch 1: 0.656
Client 4 execute local training on 8 samples...
Local loss @ local epoch 0: 0.3770829141139984
Local loss @ local epoch 1: 0.33788472414016724
Local loss @ local epoch 2: 0.3114704489707947
Local loss @ local epoch 3: 0.2884424030780792
Local loss @ local epoch 4: 0.2663014531135559
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.02 seconds!
[tester] 
SST2Metric: acc=0.6215596330275229, hinge=1.6019000230579201, ce=0.652145313423708
Local test acc @ epoch 1: 0.6216
Client 0 execute local training on 8 samples...
Local loss @ local epoch 0: 0.4742441475391388
Local loss @ local epoch 1: 0.4343228042125702
Local loss @ local epoch 2: 0.41580986976623535
Local loss @ local epoch 3: 0.4062049090862274
Local loss @ local epoch 4: 0.3971681594848633
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.15 seconds!
[tester] 
SST2Metric: acc=0.6777522935779816, hinge=1.4923528990614305, ce=0.6142609119415283
Local test acc @ epoch 1: 0.6778
Global evaluate on test data...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.73 seconds!
[tester] 
SST2Metric: acc=0.6720183486238532, hinge=1.6668513871114188, ce=0.6034399629186052
Global test acc @ epoch 1: 0.672
Global epoch 2...
Client 2 execute local training on 8 samples...
Local loss @ local epoch 0: 0.6670635342597961
Local loss @ local epoch 1: 0.5431746244430542
Local loss @ local epoch 2: 0.46356454491615295
Local loss @ local epoch 3: 0.4213424026966095
Local loss @ local epoch 4: 0.4046102464199066
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.15 seconds!
[tester] 
SST2Metric: acc=0.5, hinge=2.0390714699522072, ce=1.1109884086005184
Local test acc @ epoch 2: 0.5
Client 5 execute local training on 8 samples...
Local loss @ local epoch 0: 1.377719521522522
Local loss @ local epoch 1: 1.1410058736801147
Local loss @ local epoch 2: 0.9312501549720764
Local loss @ local epoch 3: 0.757258415222168
Local loss @ local epoch 4: 0.6266506314277649
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.25 seconds!
[tester] 
SST2Metric: acc=0.6834862385321101, hinge=1.5584250732299385, ce=0.5856465746503358
Local test acc @ epoch 2: 0.6835
Client 1 execute local training on 8 samples...
Local loss @ local epoch 0: 0.5892240405082703
Local loss @ local epoch 1: 0.5297427773475647
Local loss @ local epoch 2: 0.5042610168457031
Local loss @ local epoch 3: 0.49294817447662354
Local loss @ local epoch 4: 0.48088979721069336
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.09 seconds!
[tester] 
SST2Metric: acc=0.6009174311926605, hinge=1.6166750537146122, ce=0.7115117289604397
Local test acc @ epoch 2: 0.6009
Client 9 execute local training on 8 samples...
Local loss @ local epoch 0: 0.4964774549007416
Local loss @ local epoch 1: 0.445478230714798
Local loss @ local epoch 2: 0.4176415801048279
Local loss @ local epoch 3: 0.4020357131958008
Local loss @ local epoch 4: 0.3874591588973999
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.08 seconds!
[tester] 
SST2Metric: acc=0.7488532110091743, hinge=1.3951458411479214, ce=0.5143400050631357
Local test acc @ epoch 2: 0.7489
Client 0 execute local training on 8 samples...
Local loss @ local epoch 0: 0.4764213562011719
Local loss @ local epoch 1: 0.41364625096321106
Local loss @ local epoch 2: 0.3872182369232178
Local loss @ local epoch 3: 0.38329651951789856
Local loss @ local epoch 4: 0.3857685625553131
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.04 seconds!
[tester] 
SST2Metric: acc=0.606651376146789, hinge=1.52676736214839, ce=0.6741628238093962
Local test acc @ epoch 2: 0.6067
Client 4 execute local training on 8 samples...
Local loss @ local epoch 0: 0.6363774538040161
Local loss @ local epoch 1: 0.5271859169006348
Local loss @ local epoch 2: 0.45203742384910583
Local loss @ local epoch 3: 0.4064476490020752
Local loss @ local epoch 4: 0.3807904124259949
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.18 seconds!
[tester] 
SST2Metric: acc=0.6915137614678899, hinge=1.465378482407386, ce=0.5710656016791632
Local test acc @ epoch 2: 0.6915
Client 8 execute local training on 8 samples...
Local loss @ local epoch 0: 0.7154117226600647
Local loss @ local epoch 1: 0.656121015548706
Local loss @ local epoch 2: 0.6018477082252502
Local loss @ local epoch 3: 0.5523945093154907
Local loss @ local epoch 4: 0.5076971650123596
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.11 seconds!
[tester] 
SST2Metric: acc=0.7798165137614679, hinge=1.3233411735897764, ce=0.4877352186846077
Local test acc @ epoch 2: 0.7798
Client 3 execute local training on 8 samples...
Local loss @ local epoch 0: 0.4288046360015869
Local loss @ local epoch 1: 0.41058212518692017
Local loss @ local epoch 2: 0.40602225065231323
Local loss @ local epoch 3: 0.39664962887763977
Local loss @ local epoch 4: 0.38585299253463745
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.09 seconds!
[tester] 
SST2Metric: acc=0.7763761467889908, hinge=1.3091271344674837, ce=0.48774359190682753
Local test acc @ epoch 2: 0.7764
Client 7 execute local training on 8 samples...
Local loss @ local epoch 0: 0.43555986881256104
Local loss @ local epoch 1: 0.3938834071159363
Local loss @ local epoch 2: 0.37876060605049133
Local loss @ local epoch 3: 0.37175771594047546
Local loss @ local epoch 4: 0.3626164197921753
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.05 seconds!
[tester] 
SST2Metric: acc=0.5860091743119266, hinge=1.5198754567071933, ce=0.6992653561841458
Local test acc @ epoch 2: 0.586
Client 6 execute local training on 8 samples...
Local loss @ local epoch 0: 0.8835444450378418
Local loss @ local epoch 1: 0.7270932197570801
Local loss @ local epoch 2: 0.614162027835846
Local loss @ local epoch 3: 0.5441213250160217
Local loss @ local epoch 4: 0.5100981593132019
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.16 seconds!
[tester] 
SST2Metric: acc=0.7545871559633027, hinge=1.2763760866375145, ce=0.49948762835712607
Local test acc @ epoch 2: 0.7546
Global evaluate on test data...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.54 seconds!
[tester] 
SST2Metric: acc=0.7327981651376146, hinge=1.5375970530947414, ce=0.5563007647838067
Global test acc @ epoch 2: 0.7328
Global epoch 3...
Client 5 execute local training on 8 samples...
Local loss @ local epoch 0: 0.5577936768531799
Local loss @ local epoch 1: 0.4898644685745239
Local loss @ local epoch 2: 0.46438223123550415
Local loss @ local epoch 3: 0.4601163864135742
Local loss @ local epoch 4: 0.4556565284729004
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.16 seconds!
[tester] 
SST2Metric: acc=0.6662844036697247, hinge=1.4366479625395678, ce=0.5962850928579996
Local test acc @ epoch 3: 0.6663
Client 7 execute local training on 8 samples...
Local loss @ local epoch 0: 0.4048976004123688
Local loss @ local epoch 1: 0.38390177488327026
Local loss @ local epoch 2: 0.36751577258110046
Local loss @ local epoch 3: 0.35111474990844727
Local loss @ local epoch 4: 0.33541083335876465
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.12 seconds!
[tester] 
SST2Metric: acc=0.6307339449541285, hinge=1.4582186644777246, ce=0.636902510436303
Local test acc @ epoch 3: 0.6307
Client 3 execute local training on 8 samples...
Local loss @ local epoch 0: 0.47708410024642944
Local loss @ local epoch 1: 0.41678228974342346
Local loss @ local epoch 2: 0.39456191658973694
Local loss @ local epoch 3: 0.395877480506897
Local loss @ local epoch 4: 0.39946165680885315
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.23 seconds!
[tester] 
SST2Metric: acc=0.7660550458715596, hinge=1.2874181866645813, ce=0.48737625654684297
Local test acc @ epoch 3: 0.7661
Client 0 execute local training on 8 samples...
Local loss @ local epoch 0: 0.42511001229286194
Local loss @ local epoch 1: 0.37361714243888855
Local loss @ local epoch 2: 0.3549042046070099
Local loss @ local epoch 3: 0.35401368141174316
Local loss @ local epoch 4: 0.355813592672348
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.13 seconds!
[tester] 
SST2Metric: acc=0.6639908256880734, hinge=1.4139042852121755, ce=0.6079547217953096
Local test acc @ epoch 3: 0.664
Client 2 execute local training on 8 samples...
Local loss @ local epoch 0: 1.1159472465515137
Local loss @ local epoch 1: 0.8996568918228149
Local loss @ local epoch 2: 0.7208770513534546
Local loss @ local epoch 3: 0.5834255218505859
Local loss @ local epoch 4: 0.48710888624191284
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.16 seconds!
[tester] 
SST2Metric: acc=0.6869266055045872, hinge=1.3798091458071262, ce=0.5796576735623385
Local test acc @ epoch 3: 0.6869
Client 8 execute local training on 8 samples...
Local loss @ local epoch 0: 0.6072419881820679
Local loss @ local epoch 1: 0.533636748790741
Local loss @ local epoch 2: 0.4892697036266327
Local loss @ local epoch 3: 0.46145814657211304
Local loss @ local epoch 4: 0.4381810426712036
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.06 seconds!
[tester] 
SST2Metric: acc=0.7844036697247706, hinge=1.2430836317736074, ce=0.47384512397127415
Local test acc @ epoch 3: 0.7844
Client 6 execute local training on 8 samples...
Local loss @ local epoch 0: 0.5679387450218201
Local loss @ local epoch 1: 0.5291491150856018
Local loss @ local epoch 2: 0.5084213018417358
Local loss @ local epoch 3: 0.4900527596473694
Local loss @ local epoch 4: 0.46907010674476624
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.04 seconds!
[tester] 
SST2Metric: acc=0.7305045871559633, hinge=1.2833920044636509, ce=0.5164630352903944
Local test acc @ epoch 3: 0.7305
Client 4 execute local training on 8 samples...
Local loss @ local epoch 0: 0.45153188705444336
Local loss @ local epoch 1: 0.3950599431991577
Local loss @ local epoch 2: 0.35832834243774414
Local loss @ local epoch 3: 0.3316212296485901
Local loss @ local epoch 4: 0.30793654918670654
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.14 seconds!
[tester] 
SST2Metric: acc=0.7568807339449541, hinge=1.2836243346196796, ce=0.49514982008605923
Local test acc @ epoch 3: 0.7569
Client 1 execute local training on 8 samples...
Local loss @ local epoch 0: 0.43193501234054565
Local loss @ local epoch 1: 0.37630951404571533
Local loss @ local epoch 2: 0.3522557020187378
Local loss @ local epoch 3: 0.34439146518707275
Local loss @ local epoch 4: 0.33822259306907654
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.04 seconds!
[tester] 
SST2Metric: acc=0.6399082568807339, hinge=1.4452768096683222, ce=0.6427882140109299
Local test acc @ epoch 3: 0.6399
Client 9 execute local training on 8 samples...
Local loss @ local epoch 0: 0.43751442432403564
Local loss @ local epoch 1: 0.3880086839199066
Local loss @ local epoch 2: 0.36149075627326965
Local loss @ local epoch 3: 0.3478696644306183
Local loss @ local epoch 4: 0.33581191301345825
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.11 seconds!
[tester] 
SST2Metric: acc=0.8004587155963303, hinge=1.1561601030717201, ce=0.4429070107433774
Local test acc @ epoch 3: 0.8005
Global evaluate on test data...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.66 seconds!
[tester] 
SST2Metric: acc=0.7637614678899083, hinge=1.4444844952417075, ce=0.5207639648280012
Global test acc @ epoch 3: 0.7638
Global epoch 4...
Client 6 execute local training on 8 samples...
Local loss @ local epoch 0: 0.49596524238586426
Local loss @ local epoch 1: 0.4565373957157135
Local loss @ local epoch 2: 0.43942567706108093
Local loss @ local epoch 3: 0.42610543966293335
Local loss @ local epoch 4: 0.40950125455856323
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.08 seconds!
[tester] 
SST2Metric: acc=0.6628440366972477, hinge=1.4362811429784932, ce=0.6183270787427185
Local test acc @ epoch 4: 0.6628
Client 7 execute local training on 8 samples...
Local loss @ local epoch 0: 0.598301887512207
Local loss @ local epoch 1: 0.4901576340198517
Local loss @ local epoch 2: 0.4231022000312805
Local loss @ local epoch 3: 0.38990575075149536
Local loss @ local epoch 4: 0.3785116970539093
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.11 seconds!
[tester] 
SST2Metric: acc=0.7213302752293578, hinge=1.3101170495015766, ce=0.5347493716062756
Local test acc @ epoch 4: 0.7213
Client 3 execute local training on 8 samples...
Local loss @ local epoch 0: 0.41688552498817444
Local loss @ local epoch 1: 0.37836065888404846
Local loss @ local epoch 2: 0.3688051402568817
Local loss @ local epoch 3: 0.3686940371990204
Local loss @ local epoch 4: 0.3640023469924927
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.03 seconds!
[tester] 
SST2Metric: acc=0.7901376146788991, hinge=1.1958483326325722, ce=0.4570488746559948
Local test acc @ epoch 4: 0.7901
Client 5 execute local training on 8 samples...
Local loss @ local epoch 0: 0.4015932083129883
Local loss @ local epoch 1: 0.3631725013256073
Local loss @ local epoch 2: 0.35436758399009705
Local loss @ local epoch 3: 0.3492446839809418
Local loss @ local epoch 4: 0.33775949478149414
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.14 seconds!
[tester] 
SST2Metric: acc=0.7557339449541285, hinge=1.2266885865172115, ce=0.4938676126232935
Local test acc @ epoch 4: 0.7557
Client 8 execute local training on 8 samples...
Local loss @ local epoch 0: 0.649955689907074
Local loss @ local epoch 1: 0.5571215748786926
Local loss @ local epoch 2: 0.49744269251823425
Local loss @ local epoch 3: 0.4633234143257141
Local loss @ local epoch 4: 0.44202521443367004
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.99 seconds!
[tester] 
SST2Metric: acc=0.7889908256880734, hinge=1.1452899323690922, ce=0.4581829783839917
Local test acc @ epoch 4: 0.789
Client 0 execute local training on 8 samples...
Local loss @ local epoch 0: 0.4281174838542938
Local loss @ local epoch 1: 0.3586520850658417
Local loss @ local epoch 2: 0.32392147183418274
Local loss @ local epoch 3: 0.31447750329971313
Local loss @ local epoch 4: 0.3173534870147705
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.09 seconds!
[tester] 
SST2Metric: acc=0.7075688073394495, hinge=1.280167003016953, ce=0.542362095429263
Local test acc @ epoch 4: 0.7076
Client 9 execute local training on 8 samples...
Local loss @ local epoch 0: 0.5124704241752625
Local loss @ local epoch 1: 0.40763887763023376
Local loss @ local epoch 2: 0.3407260775566101
Local loss @ local epoch 3: 0.30734115839004517
Local loss @ local epoch 4: 0.29740893840789795
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.07 seconds!
[tester] 
SST2Metric: acc=0.7557339449541285, hinge=1.1764845673097384, ce=0.4876405759415495
Local test acc @ epoch 4: 0.7557
Client 4 execute local training on 8 samples...
Local loss @ local epoch 0: 0.5119476318359375
Local loss @ local epoch 1: 0.44609174132347107
Local loss @ local epoch 2: 0.3999532461166382
Local loss @ local epoch 3: 0.36605554819107056
Local loss @ local epoch 4: 0.3376254737377167
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.12 seconds!
[tester] 
SST2Metric: acc=0.7981651376146789, hinge=1.152702256627039, ce=0.44946013527725814
Local test acc @ epoch 4: 0.7982
Client 2 execute local training on 8 samples...
Local loss @ local epoch 0: 0.7050321102142334
Local loss @ local epoch 1: 0.5554196238517761
Local loss @ local epoch 2: 0.4469035863876343
Local loss @ local epoch 3: 0.37623846530914307
Local loss @ local epoch 4: 0.3365066647529602
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.12 seconds!
[tester] 
SST2Metric: acc=0.5871559633027523, hinge=1.6250627639643642, ce=0.804646667810755
Local test acc @ epoch 4: 0.5872
Client 1 execute local training on 8 samples...
Local loss @ local epoch 0: 0.3993796706199646
Local loss @ local epoch 1: 0.34066063165664673
Local loss @ local epoch 2: 0.3092140555381775
Local loss @ local epoch 3: 0.29942548274993896
Local loss @ local epoch 4: 0.29884496331214905
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.15 seconds!
[tester] 
SST2Metric: acc=0.7798165137614679, hinge=1.1415295956331655, ce=0.45321449582729867
Local test acc @ epoch 4: 0.7798
Global evaluate on test data...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.56 seconds!
[tester] 
SST2Metric: acc=0.768348623853211, hinge=1.3739053694480057, ce=0.5031369048247644
Global test acc @ epoch 4: 0.7683
Global epoch 5...
Client 7 execute local training on 8 samples...
Local loss @ local epoch 0: 0.3968367576599121
Local loss @ local epoch 1: 0.34927666187286377
Local loss @ local epoch 2: 0.3307495415210724
Local loss @ local epoch 3: 0.32470056414604187
Local loss @ local epoch 4: 0.31872260570526123
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.03 seconds!
[tester] 
SST2Metric: acc=0.6892201834862385, hinge=1.3119661646698593, ce=0.5680481537219582
Local test acc @ epoch 5: 0.6892
Client 2 execute local training on 8 samples...
Local loss @ local epoch 0: 1.0817418098449707
Local loss @ local epoch 1: 0.8727948069572449
Local loss @ local epoch 2: 0.7008775472640991
Local loss @ local epoch 3: 0.5686649084091187
Local loss @ local epoch 4: 0.47520118951797485
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.1 seconds!
[tester] 
SST2Metric: acc=0.7362385321100917, hinge=1.221738159109693, ce=0.5126334134865245
Local test acc @ epoch 5: 0.7362
Client 6 execute local training on 8 samples...
Local loss @ local epoch 0: 0.46278783679008484
Local loss @ local epoch 1: 0.4277624487876892
Local loss @ local epoch 2: 0.41081225872039795
Local loss @ local epoch 3: 0.3969309329986572
Local loss @ local epoch 4: 0.37983939051628113
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.15 seconds!
[tester] 
SST2Metric: acc=0.75, hinge=1.207728668637232, ce=0.4852692978371174
Local test acc @ epoch 5: 0.75
Client 4 execute local training on 8 samples...
Local loss @ local epoch 0: 0.3157370388507843
Local loss @ local epoch 1: 0.28187406063079834
Local loss @ local epoch 2: 0.2560770511627197
Local loss @ local epoch 3: 0.23348990082740784
Local loss @ local epoch 4: 0.21282239258289337
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.13 seconds!
[tester] 
SST2Metric: acc=0.7064220183486238, hinge=1.305669636354534, ce=0.5387283514399047
Local test acc @ epoch 5: 0.7064
Client 0 execute local training on 8 samples...
Local loss @ local epoch 0: 0.38766565918922424
Local loss @ local epoch 1: 0.35856783390045166
Local loss @ local epoch 2: 0.3411851227283478
Local loss @ local epoch 3: 0.32743608951568604
Local loss @ local epoch 4: 0.31404414772987366
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.15 seconds!
[tester] 
SST2Metric: acc=0.7912844036697247, hinge=1.1540617901797687, ce=0.45949090948892296
Local test acc @ epoch 5: 0.7913
Client 8 execute local training on 8 samples...
Local loss @ local epoch 0: 0.683769166469574
Local loss @ local epoch 1: 0.5870732069015503
Local loss @ local epoch 2: 0.5212151408195496
Local loss @ local epoch 3: 0.4801487326622009
Local loss @ local epoch 4: 0.4529918432235718
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.15 seconds!
[tester] 
SST2Metric: acc=0.75, hinge=1.167810500214953, ce=0.48579266380279434
Local test acc @ epoch 5: 0.75
Client 9 execute local training on 8 samples...
Local loss @ local epoch 0: 0.3220275342464447
Local loss @ local epoch 1: 0.29853466153144836
Local loss @ local epoch 2: 0.2842590808868408
Local loss @ local epoch 3: 0.26960739493370056
Local loss @ local epoch 4: 0.2537989020347595
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.22 seconds!
[tester] 
SST2Metric: acc=0.8096330275229358, hinge=1.0377149838920032, ce=0.418756856694134
Local test acc @ epoch 5: 0.8096
Client 3 execute local training on 8 samples...
Local loss @ local epoch 0: 0.3916393518447876
Local loss @ local epoch 1: 0.34243935346603394
Local loss @ local epoch 2: 0.32458582520484924
Local loss @ local epoch 3: 0.3267533481121063
Local loss @ local epoch 4: 0.3306877613067627
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.15 seconds!
[tester] 
SST2Metric: acc=0.7591743119266054, hinge=1.1495457789219847, ce=0.476181370197633
Local test acc @ epoch 5: 0.7592
Client 1 execute local training on 8 samples...
Local loss @ local epoch 0: 0.5360155701637268
Local loss @ local epoch 1: 0.4278472363948822
Local loss @ local epoch 2: 0.3528789281845093
Local loss @ local epoch 3: 0.30810070037841797
Local loss @ local epoch 4: 0.28794029355049133
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.15 seconds!
[tester] 
SST2Metric: acc=0.7236238532110092, hinge=1.1854653009854326, ce=0.5176146538979417
Local test acc @ epoch 5: 0.7236
Client 5 execute local training on 8 samples...
Local loss @ local epoch 0: 0.585494875907898
Local loss @ local epoch 1: 0.4514617919921875
Local loss @ local epoch 2: 0.3597496747970581
Local loss @ local epoch 3: 0.30905595421791077
Local loss @ local epoch 4: 0.2918413281440735
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.13 seconds!
[tester] 
SST2Metric: acc=0.7844036697247706, hinge=1.1119995623006733, ce=0.4582750433628712
Local test acc @ epoch 5: 0.7844
Global evaluate on test data...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.45 seconds!
[tester] 
SST2Metric: acc=0.7901376146788991, hinge=1.3163832972902771, ce=0.4848438089867251
Global test acc @ epoch 5: 0.7901
Global epoch 6...
Client 7 execute local training on 8 samples...
Local loss @ local epoch 0: 0.3510858118534088
Local loss @ local epoch 1: 0.3171660900115967
Local loss @ local epoch 2: 0.3056231439113617
Local loss @ local epoch 3: 0.2989659905433655
Local loss @ local epoch 4: 0.28953394293785095
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.08 seconds!
[tester] 
SST2Metric: acc=0.7270642201834863, hinge=1.2209931642090508, ce=0.5197414196685913
Local test acc @ epoch 6: 0.7271
Client 2 execute local training on 8 samples...
Local loss @ local epoch 0: 0.9583681225776672
Local loss @ local epoch 1: 0.7684264183044434
Local loss @ local epoch 2: 0.6167619228363037
Local loss @ local epoch 3: 0.5043545961380005
Local loss @ local epoch 4: 0.4283585548400879
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.03 seconds!
[tester] 
SST2Metric: acc=0.7408256880733946, hinge=1.1709756681678491, ce=0.5029963309611749
Local test acc @ epoch 6: 0.7408
Client 4 execute local training on 8 samples...
Local loss @ local epoch 0: 0.48957550525665283
Local loss @ local epoch 1: 0.4162091612815857
Local loss @ local epoch 2: 0.36621251702308655
Local loss @ local epoch 3: 0.33336326479911804
Local loss @ local epoch 4: 0.3096572458744049
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.11 seconds!
[tester] 
SST2Metric: acc=0.8038990825688074, hinge=1.0950850033705388, ce=0.43082964392977025
Local test acc @ epoch 6: 0.8039
Client 3 execute local training on 8 samples...
Local loss @ local epoch 0: 0.33573412895202637
Local loss @ local epoch 1: 0.32220184803009033
Local loss @ local epoch 2: 0.31190240383148193
Local loss @ local epoch 3: 0.30353301763534546
Local loss @ local epoch 4: 0.29780715703964233
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.07 seconds!
[tester] 
SST2Metric: acc=0.8176605504587156, hinge=1.0568024211247034, ce=0.41948484602051045
Local test acc @ epoch 6: 0.8177
Client 6 execute local training on 8 samples...
Local loss @ local epoch 0: 0.46520671248435974
Local loss @ local epoch 1: 0.4215526580810547
Local loss @ local epoch 2: 0.4030762314796448
Local loss @ local epoch 3: 0.3921298384666443
Local loss @ local epoch 4: 0.3789865970611572
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.21 seconds!
[tester] 
SST2Metric: acc=0.7385321100917431, hinge=1.1687745194916332, ce=0.49616258693944426
Local test acc @ epoch 6: 0.7385
Client 1 execute local training on 8 samples...
Local loss @ local epoch 0: 0.2887405455112457
Local loss @ local epoch 1: 0.2718107998371124
Local loss @ local epoch 2: 0.2563116252422333
Local loss @ local epoch 3: 0.2418655902147293
Local loss @ local epoch 4: 0.2285648137331009
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.16 seconds!
[tester] 
SST2Metric: acc=0.7465596330275229, hinge=1.1418193298468895, ce=0.48762507408583927
Local test acc @ epoch 6: 0.7466
Client 9 execute local training on 8 samples...
Local loss @ local epoch 0: 0.3152088224887848
Local loss @ local epoch 1: 0.2861934006214142
Local loss @ local epoch 2: 0.2716718912124634
Local loss @ local epoch 3: 0.2594223618507385
Local loss @ local epoch 4: 0.2449800670146942
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.01 seconds!
[tester] 
SST2Metric: acc=0.8245412844036697, hinge=0.9696332923042665, ce=0.3934385567356687
Local test acc @ epoch 6: 0.8245
Client 5 execute local training on 8 samples...
Local loss @ local epoch 0: 0.3831360340118408
Local loss @ local epoch 1: 0.3150266408920288
Local loss @ local epoch 2: 0.2849818468093872
Local loss @ local epoch 3: 0.2803272604942322
Local loss @ local epoch 4: 0.2831050455570221
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.1 seconds!
[tester] 
SST2Metric: acc=0.7568807339449541, hinge=1.1209732166944293, ce=0.48112392780977653
Local test acc @ epoch 6: 0.7569
Client 8 execute local training on 8 samples...
Local loss @ local epoch 0: 0.5555667877197266
Local loss @ local epoch 1: 0.44081318378448486
Local loss @ local epoch 2: 0.3597657084465027
Local loss @ local epoch 3: 0.3119584619998932
Local loss @ local epoch 4: 0.29098403453826904
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.16 seconds!
[tester] 
SST2Metric: acc=0.8119266055045872, hinge=0.967983089182355, ce=0.40973321539819785
Local test acc @ epoch 6: 0.8119
Client 0 execute local training on 8 samples...
Local loss @ local epoch 0: 0.36407390236854553
Local loss @ local epoch 1: 0.3008798360824585
Local loss @ local epoch 2: 0.26899266242980957
Local loss @ local epoch 3: 0.2601267397403717
Local loss @ local epoch 4: 0.2630080282688141
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.12 seconds!
[tester] 
SST2Metric: acc=0.7729357798165137, hinge=1.0928551007848266, ce=0.4651330120929884
Local test acc @ epoch 6: 0.7729
Global evaluate on test data...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.76 seconds!
[tester] 
SST2Metric: acc=0.7821100917431193, hinge=1.2505543464367543, ce=0.47130341116988334
Global test acc @ epoch 6: 0.7821
Global epoch 7...
Client 7 execute local training on 8 samples...
Local loss @ local epoch 0: 0.31374743580818176
Local loss @ local epoch 1: 0.2810157239437103
Local loss @ local epoch 2: 0.27133050560951233
Local loss @ local epoch 3: 0.2672922909259796
Local loss @ local epoch 4: 0.26024293899536133
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.11 seconds!
[tester] 
SST2Metric: acc=0.7201834862385321, hinge=1.1876604182457706, ce=0.514400973574284
Local test acc @ epoch 7: 0.7202
Client 1 execute local training on 8 samples...
Local loss @ local epoch 0: 0.5797625184059143
Local loss @ local epoch 1: 0.4546491503715515
Local loss @ local epoch 2: 0.3628925383090973
Local loss @ local epoch 3: 0.302043080329895
Local loss @ local epoch 4: 0.2677360475063324
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.12 seconds!
[tester] 
SST2Metric: acc=0.7740825688073395, hinge=1.066764733249988, ce=0.45757533377463666
Local test acc @ epoch 7: 0.7741
Client 5 execute local training on 8 samples...
Local loss @ local epoch 0: 0.4895336329936981
Local loss @ local epoch 1: 0.37807390093803406
Local loss @ local epoch 2: 0.3075050413608551
Local loss @ local epoch 3: 0.27396073937416077
Local loss @ local epoch 4: 0.26749387383461
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.07 seconds!
[tester] 
SST2Metric: acc=0.7740825688073395, hinge=1.0976345804306344, ce=0.46311073278615233
Local test acc @ epoch 7: 0.7741
Client 3 execute local training on 8 samples...
Local loss @ local epoch 0: 0.3113279938697815
Local loss @ local epoch 1: 0.28970077633857727
Local loss @ local epoch 2: 0.2904512882232666
Local loss @ local epoch 3: 0.29073020815849304
Local loss @ local epoch 4: 0.284403920173645
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.13 seconds!
[tester] 
SST2Metric: acc=0.8291284403669725, hinge=0.9746459127839552, ce=0.3942071199417114
Local test acc @ epoch 7: 0.8291
Client 2 execute local training on 8 samples...
Local loss @ local epoch 0: 0.6182405352592468
Local loss @ local epoch 1: 0.4974765479564667
Local loss @ local epoch 2: 0.413138747215271
Local loss @ local epoch 3: 0.36052030324935913
Local loss @ local epoch 4: 0.332016259431839
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.09 seconds!
[tester] 
SST2Metric: acc=0.680045871559633, hinge=1.3197502318872225, ce=0.6242061616904145
Local test acc @ epoch 7: 0.68
Client 4 execute local training on 8 samples...
Local loss @ local epoch 0: 0.5498120188713074
Local loss @ local epoch 1: 0.44600921869277954
Local loss @ local epoch 2: 0.3677697479724884
Local loss @ local epoch 3: 0.3145216405391693
Local loss @ local epoch 4: 0.28235211968421936
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.09 seconds!
[tester] 
SST2Metric: acc=0.8188073394495413, hinge=0.9915408630163298, ce=0.39974626577502
Local test acc @ epoch 7: 0.8188
Client 9 execute local training on 8 samples...
Local loss @ local epoch 0: 0.2995311915874481
Local loss @ local epoch 1: 0.27432411909103394
Local loss @ local epoch 2: 0.2604348063468933
Local loss @ local epoch 3: 0.24646955728530884
Local loss @ local epoch 4: 0.2311200201511383
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.09 seconds!
[tester] 
SST2Metric: acc=0.8153669724770642, hinge=0.9685582206883562, ce=0.4039227327485697
Local test acc @ epoch 7: 0.8154
Client 0 execute local training on 8 samples...
Local loss @ local epoch 0: 0.30503156781196594
Local loss @ local epoch 1: 0.26644256711006165
Local loss @ local epoch 2: 0.25405970215797424
Local loss @ local epoch 3: 0.254475861787796
Local loss @ local epoch 4: 0.25585517287254333
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.12 seconds!
[tester] 
SST2Metric: acc=0.8027522935779816, hinge=1.0257010939744635, ce=0.43108422254477075
Local test acc @ epoch 7: 0.8028
Client 8 execute local training on 8 samples...
Local loss @ local epoch 0: 0.47028088569641113
Local loss @ local epoch 1: 0.3788258135318756
Local loss @ local epoch 2: 0.3215223252773285
Local loss @ local epoch 3: 0.29358354210853577
Local loss @ local epoch 4: 0.2838418185710907
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.11 seconds!
[tester] 
SST2Metric: acc=0.8119266055045872, hinge=0.954924607769065, ce=0.4105979338151599
Local test acc @ epoch 7: 0.8119
Client 6 execute local training on 8 samples...
Local loss @ local epoch 0: 0.44947150349617004
Local loss @ local epoch 1: 0.40728506445884705
Local loss @ local epoch 2: 0.3864234685897827
Local loss @ local epoch 3: 0.3752014636993408
Local loss @ local epoch 4: 0.36274105310440063
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.12 seconds!
[tester] 
SST2Metric: acc=0.8302752293577982, hinge=0.924687126360902, ce=0.37691755572316843
Local test acc @ epoch 7: 0.8303
Global evaluate on test data...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.58 seconds!
[tester] 
SST2Metric: acc=0.7878440366972477, hinge=1.2045021653175354, ce=0.4605725727496891
Global test acc @ epoch 7: 0.7878
Global epoch 8...
Client 7 execute local training on 8 samples...
Local loss @ local epoch 0: 0.28765782713890076
Local loss @ local epoch 1: 0.2540196180343628
Local loss @ local epoch 2: 0.24424608051776886
Local loss @ local epoch 3: 0.24188604950904846
Local loss @ local epoch 4: 0.23700128495693207
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.09 seconds!
[tester] 
SST2Metric: acc=0.7419724770642202, hinge=1.1414100672673742, ce=0.498225163770925
Local test acc @ epoch 8: 0.742
Client 9 execute local training on 8 samples...
Local loss @ local epoch 0: 0.4148944616317749
Local loss @ local epoch 1: 0.31546321511268616
Local loss @ local epoch 2: 0.2506343424320221
Local loss @ local epoch 3: 0.21660134196281433
Local loss @ local epoch 4: 0.20537367463111877
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.11 seconds!
[tester] 
SST2Metric: acc=0.8038990825688074, hinge=0.9778081956533117, ce=0.4188804380390622
Local test acc @ epoch 8: 0.8039
Client 1 execute local training on 8 samples...
Local loss @ local epoch 0: 0.26398134231567383
Local loss @ local epoch 1: 0.24705448746681213
Local loss @ local epoch 2: 0.23340065777301788
Local loss @ local epoch 3: 0.21992243826389313
Local loss @ local epoch 4: 0.207005113363266
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.2 seconds!
[tester] 
SST2Metric: acc=0.7752293577981652, hinge=1.049502035073184, ce=0.45623487240950994
Local test acc @ epoch 8: 0.7752
Client 0 execute local training on 8 samples...
Local loss @ local epoch 0: 0.36030980944633484
Local loss @ local epoch 1: 0.2953457832336426
Local loss @ local epoch 2: 0.2628174126148224
Local loss @ local epoch 3: 0.2530229687690735
Local loss @ local epoch 4: 0.2546542286872864
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.12 seconds!
[tester] 
SST2Metric: acc=0.8084862385321101, hinge=0.9949999005969511, ce=0.4172645199599616
Local test acc @ epoch 8: 0.8085
Client 5 execute local training on 8 samples...
Local loss @ local epoch 0: 0.2491394430398941
Local loss @ local epoch 1: 0.23724107444286346
Local loss @ local epoch 2: 0.2255096435546875
Local loss @ local epoch 3: 0.2136363685131073
Local loss @ local epoch 4: 0.203385129570961
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.23 seconds!
[tester] 
SST2Metric: acc=0.823394495412844, hinge=0.9417040209977998, ce=0.3944909082503494
Local test acc @ epoch 8: 0.8234
Client 6 execute local training on 8 samples...
Local loss @ local epoch 0: 0.4124124050140381
Local loss @ local epoch 1: 0.3534814715385437
Local loss @ local epoch 2: 0.3250276446342468
Local loss @ local epoch 3: 0.31607455015182495
Local loss @ local epoch 4: 0.3127056956291199
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.18 seconds!
[tester] 
SST2Metric: acc=0.7637614678899083, hinge=1.0797906299249842, ce=0.4763922702281847
Local test acc @ epoch 8: 0.7638
Client 8 execute local training on 8 samples...
Local loss @ local epoch 0: 0.38378164172172546
Local loss @ local epoch 1: 0.3445636034011841
Local loss @ local epoch 2: 0.31259509921073914
Local loss @ local epoch 3: 0.28462183475494385
Local loss @ local epoch 4: 0.25980156660079956
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.11 seconds!
[tester] 
SST2Metric: acc=0.8302752293577982, hinge=0.8949187228439051, ce=0.38275334956722523
Local test acc @ epoch 8: 0.8303
Client 4 execute local training on 8 samples...
Local loss @ local epoch 0: 0.4221293032169342
Local loss @ local epoch 1: 0.3727596700191498
Local loss @ local epoch 2: 0.3356977701187134
Local loss @ local epoch 3: 0.30531617999076843
Local loss @ local epoch 4: 0.27816087007522583
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.01 seconds!
[tester] 
SST2Metric: acc=0.8348623853211009, hinge=0.8992197755553307, ce=0.3730088272089258
Local test acc @ epoch 8: 0.8349
Client 3 execute local training on 8 samples...
Local loss @ local epoch 0: 0.2947726249694824
Local loss @ local epoch 1: 0.2791827619075775
Local loss @ local epoch 2: 0.2715441882610321
Local loss @ local epoch 3: 0.26411595940589905
Local loss @ local epoch 4: 0.2573355734348297
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.06 seconds!
[tester] 
SST2Metric: acc=0.8314220183486238, hinge=0.9041786620376306, ce=0.38149603292209294
Local test acc @ epoch 8: 0.8314
Client 2 execute local training on 8 samples...
Local loss @ local epoch 0: 0.6039856672286987
Local loss @ local epoch 1: 0.48103243112564087
Local loss @ local epoch 2: 0.39224275946617126
Local loss @ local epoch 3: 0.3342086374759674
Local loss @ local epoch 4: 0.3008590340614319
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.1 seconds!
[tester] 
SST2Metric: acc=0.7373853211009175, hinge=1.1360686430963902, ce=0.530666293832687
Local test acc @ epoch 8: 0.7374
Global evaluate on test data...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.65 seconds!
[tester] 
SST2Metric: acc=0.7912844036697247, hinge=1.1547643279810564, ce=0.448769549574327
Global test acc @ epoch 8: 0.7913
Global epoch 9...
Client 8 execute local training on 8 samples...
Local loss @ local epoch 0: 0.31857582926750183
Local loss @ local epoch 1: 0.2842164933681488
Local loss @ local epoch 2: 0.2643846273422241
Local loss @ local epoch 3: 0.24753184616565704
Local loss @ local epoch 4: 0.23056276142597198
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.16 seconds!
[tester] 
SST2Metric: acc=0.8337155963302753, hinge=0.8959976102507442, ce=0.3857722621445262
Local test acc @ epoch 9: 0.8337
Client 5 execute local training on 8 samples...
Local loss @ local epoch 0: 0.35360532999038696
Local loss @ local epoch 1: 0.27278879284858704
Local loss @ local epoch 2: 0.2274467647075653
Local loss @ local epoch 3: 0.21109412610530853
Local loss @ local epoch 4: 0.21225795149803162
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.1 seconds!
[tester] 
SST2Metric: acc=0.7649082568807339, hinge=1.0840572671059074, ce=0.477211776608174
Local test acc @ epoch 9: 0.7649
Client 1 execute local training on 8 samples...
Local loss @ local epoch 0: 0.4495229721069336
Local loss @ local epoch 1: 0.3486446142196655
Local loss @ local epoch 2: 0.2757733166217804
Local loss @ local epoch 3: 0.22804318368434906
Local loss @ local epoch 4: 0.20125943422317505
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.1 seconds!
[tester] 
SST2Metric: acc=0.8073394495412844, hinge=0.9493509394313218, ce=0.41903203937712064
Local test acc @ epoch 9: 0.8073
Client 4 execute local training on 8 samples...
Local loss @ local epoch 0: 0.477037250995636
Local loss @ local epoch 1: 0.40798503160476685
Local loss @ local epoch 2: 0.3599880635738373
Local loss @ local epoch 3: 0.32750794291496277
Local loss @ local epoch 4: 0.3035796880722046
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.03 seconds!
[tester] 
SST2Metric: acc=0.841743119266055, hinge=0.8872531589564927, ce=0.3686169148721826
Local test acc @ epoch 9: 0.8417
Client 0 execute local training on 8 samples...
Local loss @ local epoch 0: 0.24533650279045105
Local loss @ local epoch 1: 0.23568367958068848
Local loss @ local epoch 2: 0.22765976190567017
Local loss @ local epoch 3: 0.21931804716587067
Local loss @ local epoch 4: 0.21183273196220398
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.1 seconds!
[tester] 
SST2Metric: acc=0.838302752293578, hinge=0.8892916974124558, ce=0.3727475397903985
Local test acc @ epoch 9: 0.8383
Client 7 execute local training on 8 samples...
Local loss @ local epoch 0: 0.23968970775604248
Local loss @ local epoch 1: 0.2278786450624466
Local loss @ local epoch 2: 0.21927273273468018
Local loss @ local epoch 3: 0.20924745500087738
Local loss @ local epoch 4: 0.19981464743614197
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.08 seconds!
[tester] 
SST2Metric: acc=0.7993119266055045, hinge=0.9945640671964086, ce=0.4323452845762629
Local test acc @ epoch 9: 0.7993
Client 9 execute local training on 8 samples...
Local loss @ local epoch 0: 0.32280370593070984
Local loss @ local epoch 1: 0.24583113193511963
Local loss @ local epoch 2: 0.19968415796756744
Local loss @ local epoch 3: 0.17931990325450897
Local loss @ local epoch 4: 0.1758815050125122
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.12 seconds!
[tester] 
SST2Metric: acc=0.8142201834862385, hinge=0.9195361718398716, ce=0.41114998041489803
Local test acc @ epoch 9: 0.8142
Client 2 execute local training on 8 samples...
Local loss @ local epoch 0: 0.36063000559806824
Local loss @ local epoch 1: 0.3288207948207855
Local loss @ local epoch 2: 0.30487704277038574
Local loss @ local epoch 3: 0.284437894821167
Local loss @ local epoch 4: 0.26563236117362976
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.06 seconds!
[tester] 
SST2Metric: acc=0.7006880733944955, hinge=1.2542298820314057, ce=0.6067636643254429
Local test acc @ epoch 9: 0.7007
Client 6 execute local training on 8 samples...
Local loss @ local epoch 0: 0.46897679567337036
Local loss @ local epoch 1: 0.3910348117351532
Local loss @ local epoch 2: 0.3350556492805481
Local loss @ local epoch 3: 0.3029528856277466
Local loss @ local epoch 4: 0.29173776507377625
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.09 seconds!
[tester] 
SST2Metric: acc=0.8279816513761468, hinge=0.8881068105271103, ce=0.3721990931881677
Local test acc @ epoch 9: 0.828
Client 3 execute local training on 8 samples...
Local loss @ local epoch 0: 0.29621365666389465
Local loss @ local epoch 1: 0.2745201587677002
Local loss @ local epoch 2: 0.26683884859085083
Local loss @ local epoch 3: 0.26166948676109314
Local loss @ local epoch 4: 0.2551060616970062
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.07 seconds!
[tester] 
SST2Metric: acc=0.8279816513761468, hinge=0.9061727609929688, ce=0.3874841156082416
Local test acc @ epoch 9: 0.828
Global evaluate on test data...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.76 seconds!
[tester] 
SST2Metric: acc=0.7901376146788991, hinge=1.1208603618615265, ce=0.4466871930918562
Global test acc @ epoch 9: 0.7901
Global epoch 10...
Client 5 execute local training on 8 samples...
Local loss @ local epoch 0: 0.23618902266025543
Local loss @ local epoch 1: 0.19877701997756958
Local loss @ local epoch 2: 0.18866872787475586
Local loss @ local epoch 3: 0.18970263004302979
Local loss @ local epoch 4: 0.1882421374320984
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.09 seconds!
[tester] 
SST2Metric: acc=0.8061926605504587, hinge=0.9774141684858078, ce=0.42544032872543425
Local test acc @ epoch 10: 0.8062
Client 6 execute local training on 8 samples...
Local loss @ local epoch 0: 0.43782123923301697
Local loss @ local epoch 1: 0.37079480290412903
Local loss @ local epoch 2: 0.3359029293060303
Local loss @ local epoch 3: 0.3235985040664673
Local loss @ local epoch 4: 0.3203221559524536
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.02 seconds!
[tester] 
SST2Metric: acc=0.8211009174311926, hinge=0.9322955587861734, ce=0.40629723073419083
Local test acc @ epoch 10: 0.8211
Client 9 execute local training on 8 samples...
Local loss @ local epoch 0: 0.2164808213710785
Local loss @ local epoch 1: 0.19793325662612915
Local loss @ local epoch 2: 0.18895286321640015
Local loss @ local epoch 3: 0.17932328581809998
Local loss @ local epoch 4: 0.1681586354970932
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.08 seconds!
[tester] 
SST2Metric: acc=0.841743119266055, hinge=0.8402841797935854, ce=0.3618706875232928
Local test acc @ epoch 10: 0.8417
Client 2 execute local training on 8 samples...
Local loss @ local epoch 0: 0.42567187547683716
Local loss @ local epoch 1: 0.3613743185997009
Local loss @ local epoch 2: 0.3215092420578003
Local loss @ local epoch 3: 0.29894840717315674
Local loss @ local epoch 4: 0.28577467799186707
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.13 seconds!
[tester] 
SST2Metric: acc=0.6926605504587156, hinge=1.2949244746374429, ce=0.6419168751037448
Local test acc @ epoch 10: 0.6927
Client 8 execute local training on 8 samples...
Local loss @ local epoch 0: 0.3684188723564148
Local loss @ local epoch 1: 0.28962957859039307
Local loss @ local epoch 2: 0.24068398773670197
Local loss @ local epoch 3: 0.21669979393482208
Local loss @ local epoch 4: 0.20850665867328644
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.12 seconds!
[tester] 
SST2Metric: acc=0.838302752293578, hinge=0.8337175265091274, ce=0.35880031703262155
Local test acc @ epoch 10: 0.8383
Client 4 execute local training on 8 samples...
Local loss @ local epoch 0: 0.37648266553878784
Local loss @ local epoch 1: 0.33579134941101074
Local loss @ local epoch 2: 0.30194178223609924
Local loss @ local epoch 3: 0.27225059270858765
Local loss @ local epoch 4: 0.2455587238073349
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.11 seconds!
[tester] 
SST2Metric: acc=0.8314220183486238, hinge=0.8626134413371392, ce=0.3652298234813257
Local test acc @ epoch 10: 0.8314
Client 1 execute local training on 8 samples...
Local loss @ local epoch 0: 0.1857166588306427
Local loss @ local epoch 1: 0.15360933542251587
Local loss @ local epoch 2: 0.13881099224090576
Local loss @ local epoch 3: 0.13591790199279785
Local loss @ local epoch 4: 0.13657526671886444
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.12 seconds!
[tester] 
SST2Metric: acc=0.7236238532110092, hinge=1.1785860330960072, ce=0.5553159981692602
Local test acc @ epoch 10: 0.7236
Client 7 execute local training on 8 samples...
Local loss @ local epoch 0: 0.47634756565093994
Local loss @ local epoch 1: 0.37270495295524597
Local loss @ local epoch 2: 0.3054041266441345
Local loss @ local epoch 3: 0.26962581276893616
Local loss @ local epoch 4: 0.2563621997833252
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.07 seconds!
[tester] 
SST2Metric: acc=0.8394495412844036, hinge=0.8478476245195494, ce=0.3627049541528072
Local test acc @ epoch 10: 0.8394
Client 0 execute local training on 8 samples...
Local loss @ local epoch 0: 0.23694398999214172
Local loss @ local epoch 1: 0.2197347730398178
Local loss @ local epoch 2: 0.21153642237186432
Local loss @ local epoch 3: 0.20498573780059814
Local loss @ local epoch 4: 0.1973361223936081
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.17 seconds!
[tester] 
SST2Metric: acc=0.8360091743119266, hinge=0.8184007184767942, ce=0.35031386167494527
Local test acc @ epoch 10: 0.836
Client 3 execute local training on 8 samples...
Local loss @ local epoch 0: 0.2738707661628723
Local loss @ local epoch 1: 0.2583528161048889
Local loss @ local epoch 2: 0.2579553425312042
Local loss @ local epoch 3: 0.25580844283103943
Local loss @ local epoch 4: 0.24944555759429932
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.07 seconds!
[tester] 
SST2Metric: acc=0.8279816513761468, hinge=0.8809828246952197, ce=0.38240541070016154
Local test acc @ epoch 10: 0.828
Global evaluate on test data...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.64 seconds!
[tester] 
SST2Metric: acc=0.7935779816513762, hinge=1.0913955030638143, ce=0.4369615461301366
Global test acc @ epoch 10: 0.7936
Global epoch 11...
Client 5 execute local training on 8 samples...
Local loss @ local epoch 0: 0.22437655925750732
Local loss @ local epoch 1: 0.18043315410614014
Local loss @ local epoch 2: 0.16463877260684967
Local loss @ local epoch 3: 0.1651400923728943
Local loss @ local epoch 4: 0.16801844537258148
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.11 seconds!
[tester] 
SST2Metric: acc=0.8119266055045872, hinge=0.9489580168636567, ce=0.41935008962493425
Local test acc @ epoch 11: 0.8119
Client 3 execute local training on 8 samples...
Local loss @ local epoch 0: 0.276053249835968
Local loss @ local epoch 1: 0.2569389343261719
Local loss @ local epoch 2: 0.25547918677330017
Local loss @ local epoch 3: 0.2554085850715637
Local loss @ local epoch 4: 0.25046592950820923
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.26 seconds!
[tester] 
SST2Metric: acc=0.841743119266055, hinge=0.8381066947081767, ce=0.3601890439765716
Local test acc @ epoch 11: 0.8417
Client 4 execute local training on 8 samples...
Local loss @ local epoch 0: 0.33063435554504395
Local loss @ local epoch 1: 0.29146504402160645
Local loss @ local epoch 2: 0.2614021301269531
Local loss @ local epoch 3: 0.23584069311618805
Local loss @ local epoch 4: 0.21274766325950623
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.12 seconds!
[tester] 
SST2Metric: acc=0.8245412844036697, hinge=0.9002603527602799, ce=0.3864138989683685
Local test acc @ epoch 11: 0.8245
Client 0 execute local training on 8 samples...
Local loss @ local epoch 0: 0.2313215583562851
Local loss @ local epoch 1: 0.21797268092632294
Local loss @ local epoch 2: 0.20709529519081116
Local loss @ local epoch 3: 0.19744420051574707
Local loss @ local epoch 4: 0.18929588794708252
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.09 seconds!
[tester] 
SST2Metric: acc=0.8348623853211009, hinge=0.8245118122308626, ce=0.3532418074479344
Local test acc @ epoch 11: 0.8349
Client 9 execute local training on 8 samples...
Local loss @ local epoch 0: 0.19331440329551697
Local loss @ local epoch 1: 0.17851118743419647
Local loss @ local epoch 2: 0.1699991524219513
Local loss @ local epoch 3: 0.15995116531848907
Local loss @ local epoch 4: 0.14940229058265686
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.1 seconds!
[tester] 
SST2Metric: acc=0.8440366972477065, hinge=0.819444623698882, ce=0.3595393749483682
Local test acc @ epoch 11: 0.844
Client 6 execute local training on 8 samples...
Local loss @ local epoch 0: 0.359897643327713
Local loss @ local epoch 1: 0.32919949293136597
Local loss @ local epoch 2: 0.3148244619369507
Local loss @ local epoch 3: 0.3037562668323517
Local loss @ local epoch 4: 0.28994688391685486
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.14 seconds!
[tester] 
SST2Metric: acc=0.838302752293578, hinge=0.8113738138467894, ce=0.34959808350839744
Local test acc @ epoch 11: 0.8383
Client 2 execute local training on 8 samples...
Local loss @ local epoch 0: 0.47097834944725037
Local loss @ local epoch 1: 0.3686385154724121
Local loss @ local epoch 2: 0.29748624563217163
Local loss @ local epoch 3: 0.2537459433078766
Local loss @ local epoch 4: 0.231112539768219
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.09 seconds!
[tester] 
SST2Metric: acc=0.7064220183486238, hinge=1.2594741811172678, ce=0.6247976532701506
Local test acc @ epoch 11: 0.7064
Client 7 execute local training on 8 samples...
Local loss @ local epoch 0: 0.5946499705314636
Local loss @ local epoch 1: 0.4531344771385193
Local loss @ local epoch 2: 0.3485725224018097
Local loss @ local epoch 3: 0.27927255630493164
Local loss @ local epoch 4: 0.2404147982597351
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.13 seconds!
[tester] 
SST2Metric: acc=0.8371559633027523, hinge=0.8086160273726927, ce=0.34827935152234285
Local test acc @ epoch 11: 0.8372
Client 8 execute local training on 8 samples...
Local loss @ local epoch 0: 0.3140513300895691
Local loss @ local epoch 1: 0.25553226470947266
Local loss @ local epoch 2: 0.22383393347263336
Local loss @ local epoch 3: 0.21015292406082153
Local loss @ local epoch 4: 0.20350658893585205
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.13 seconds!
[tester] 
SST2Metric: acc=0.8073394495412844, hinge=0.9304563666975826, ce=0.4287060166991085
Local test acc @ epoch 11: 0.8073
Client 1 execute local training on 8 samples...
Local loss @ local epoch 0: 0.15153107047080994
Local loss @ local epoch 1: 0.14012223482131958
Local loss @ local epoch 2: 0.1299947053194046
Local loss @ local epoch 3: 0.12078864872455597
Local loss @ local epoch 4: 0.11261270940303802
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.13 seconds!
[tester] 
SST2Metric: acc=0.7935779816513762, hinge=0.9691749417453731, ce=0.44381726669882415
Local test acc @ epoch 11: 0.7936
Global evaluate on test data...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.72 seconds!
[tester] 
SST2Metric: acc=0.7935779816513762, hinge=1.0648883999487675, ce=0.4312992954472883
Global test acc @ epoch 11: 0.7936
Global epoch 12...
Client 5 execute local training on 8 samples...
Local loss @ local epoch 0: 0.19813348352909088
Local loss @ local epoch 1: 0.1573380082845688
Local loss @ local epoch 2: 0.14245706796646118
Local loss @ local epoch 3: 0.14296211302280426
Local loss @ local epoch 4: 0.1462511122226715
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.15 seconds!
[tester] 
SST2Metric: acc=0.819954128440367, hinge=0.912162147394014, ce=0.40725973918350467
Local test acc @ epoch 12: 0.82
Client 8 execute local training on 8 samples...
Local loss @ local epoch 0: 0.4594096541404724
Local loss @ local epoch 1: 0.35335469245910645
Local loss @ local epoch 2: 0.27704086899757385
Local loss @ local epoch 3: 0.2294032871723175
Local loss @ local epoch 4: 0.20578449964523315
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.04 seconds!
[tester] 
SST2Metric: acc=0.823394495412844, hinge=0.8704911809448802, ce=0.3911308471899514
Local test acc @ epoch 12: 0.8234
Client 2 execute local training on 8 samples...
Local loss @ local epoch 0: 0.3179861307144165
Local loss @ local epoch 1: 0.2804712653160095
Local loss @ local epoch 2: 0.2589554786682129
Local loss @ local epoch 3: 0.24493145942687988
Local loss @ local epoch 4: 0.23293128609657288
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.1 seconds!
[tester] 
SST2Metric: acc=0.6846330275229358, hinge=1.337196879840772, ce=0.6776179403340051
Local test acc @ epoch 12: 0.6846
Client 0 execute local training on 8 samples...
Local loss @ local epoch 0: 0.4722716212272644
Local loss @ local epoch 1: 0.35177308320999146
Local loss @ local epoch 2: 0.2676689028739929
Local loss @ local epoch 3: 0.21570643782615662
Local loss @ local epoch 4: 0.1891511082649231
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.03 seconds!
[tester] 
SST2Metric: acc=0.8440366972477065, hinge=0.8119153526671435, ce=0.3513656972540081
Local test acc @ epoch 12: 0.844
Client 1 execute local training on 8 samples...
Local loss @ local epoch 0: 0.17198941111564636
Local loss @ local epoch 1: 0.14192268252372742
Local loss @ local epoch 2: 0.12732520699501038
Local loss @ local epoch 3: 0.12328614294528961
Local loss @ local epoch 4: 0.12278664112091064
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.09 seconds!
[tester] 
SST2Metric: acc=0.7408256880733946, hinge=1.1210285314452757, ce=0.5331860897601197
Local test acc @ epoch 12: 0.7408
Client 3 execute local training on 8 samples...
Local loss @ local epoch 0: 0.48750337958335876
Local loss @ local epoch 1: 0.3902762532234192
Local loss @ local epoch 2: 0.3199984133243561
Local loss @ local epoch 3: 0.2758588194847107
Local loss @ local epoch 4: 0.2540236711502075
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.07 seconds!
[tester] 
SST2Metric: acc=0.838302752293578, hinge=0.8266386179202193, ce=0.36036677860202043
Local test acc @ epoch 12: 0.8383
Client 6 execute local training on 8 samples...
Local loss @ local epoch 0: 0.3069515824317932
Local loss @ local epoch 1: 0.26857346296310425
Local loss @ local epoch 2: 0.25401240587234497
Local loss @ local epoch 3: 0.24893787503242493
Local loss @ local epoch 4: 0.24260300397872925
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.1 seconds!
[tester] 
SST2Metric: acc=0.8165137614678899, hinge=0.9375420747273558, ce=0.41838541726043466
Local test acc @ epoch 12: 0.8165
Client 9 execute local training on 8 samples...
Local loss @ local epoch 0: 0.20896108448505402
Local loss @ local epoch 1: 0.18498678505420685
Local loss @ local epoch 2: 0.17496687173843384
Local loss @ local epoch 3: 0.1675863265991211
Local loss @ local epoch 4: 0.15810199081897736
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.14 seconds!
[tester] 
SST2Metric: acc=0.8440366972477065, hinge=0.7926099793900043, ce=0.3472049333360217
Local test acc @ epoch 12: 0.844
Client 7 execute local training on 8 samples...
Local loss @ local epoch 0: 0.23750674724578857
Local loss @ local epoch 1: 0.21874791383743286
Local loss @ local epoch 2: 0.21130114793777466
Local loss @ local epoch 3: 0.20324423909187317
Local loss @ local epoch 4: 0.19348004460334778
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.13 seconds!
[tester] 
SST2Metric: acc=0.8348623853211009, hinge=0.8390507796488771, ce=0.373924050992782
Local test acc @ epoch 12: 0.8349
Client 4 execute local training on 8 samples...
Local loss @ local epoch 0: 0.3790867030620575
Local loss @ local epoch 1: 0.32564765214920044
Local loss @ local epoch 2: 0.28873729705810547
Local loss @ local epoch 3: 0.26211363077163696
Local loss @ local epoch 4: 0.2400708794593811
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.04 seconds!
[tester] 
SST2Metric: acc=0.841743119266055, hinge=0.8303879193209727, ce=0.3662490880954156
Local test acc @ epoch 12: 0.8417
Global evaluate on test data...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.6 seconds!
[tester] 
SST2Metric: acc=0.7947247706422018, hinge=1.0496859506729546, ce=0.4286504052237633
Global test acc @ epoch 12: 0.7947
Global epoch 13...
Client 6 execute local training on 8 samples...
Local loss @ local epoch 0: 0.26957041025161743
Local loss @ local epoch 1: 0.2532478868961334
Local loss @ local epoch 2: 0.24026302993297577
Local loss @ local epoch 3: 0.22638604044914246
Local loss @ local epoch 4: 0.21284586191177368
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.22 seconds!
[tester] 
SST2Metric: acc=0.8256880733944955, hinge=0.9187234615514038, ce=0.4054338815152098
Local test acc @ epoch 13: 0.8257
Client 0 execute local training on 8 samples...
Local loss @ local epoch 0: 0.21788237988948822
Local loss @ local epoch 1: 0.20225173234939575
Local loss @ local epoch 2: 0.19106268882751465
Local loss @ local epoch 3: 0.18098589777946472
Local loss @ local epoch 4: 0.1721145063638687
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.14 seconds!
[tester] 
SST2Metric: acc=0.8451834862385321, hinge=0.8030946344957439, ce=0.35110937250316687
Local test acc @ epoch 13: 0.8452
Client 9 execute local training on 8 samples...
Local loss @ local epoch 0: 0.16232791543006897
Local loss @ local epoch 1: 0.15139593183994293
Local loss @ local epoch 2: 0.1420673280954361
Local loss @ local epoch 3: 0.1324058622121811
Local loss @ local epoch 4: 0.12372750788927078
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.16 seconds!
[tester] 
SST2Metric: acc=0.8463302752293578, hinge=0.7892648361691641, ce=0.35241296873726974
Local test acc @ epoch 13: 0.8463
Client 5 execute local training on 8 samples...
Local loss @ local epoch 0: 0.17961546778678894
Local loss @ local epoch 1: 0.1396055817604065
Local loss @ local epoch 2: 0.1233721375465393
Local loss @ local epoch 3: 0.12244193255901337
Local loss @ local epoch 4: 0.12597210705280304
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.12 seconds!
[tester] 
SST2Metric: acc=0.8061926605504587, hinge=0.9138470502621537, ce=0.4173261665422982
Local test acc @ epoch 13: 0.8062
Client 8 execute local training on 8 samples...
Local loss @ local epoch 0: 0.3630025088787079
Local loss @ local epoch 1: 0.2734264135360718
Local loss @ local epoch 2: 0.2120678573846817
Local loss @ local epoch 3: 0.17617525160312653
Local loss @ local epoch 4: 0.16029691696166992
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.3 seconds!
[tester] 
SST2Metric: acc=0.8337155963302753, hinge=0.8207441649032295, ce=0.376849699492028
Local test acc @ epoch 13: 0.8337
Client 3 execute local training on 8 samples...
Local loss @ local epoch 0: 0.3747122287750244
Local loss @ local epoch 1: 0.3144259452819824
Local loss @ local epoch 2: 0.27392658591270447
Local loss @ local epoch 3: 0.25274422764778137
Local loss @ local epoch 4: 0.2479131519794464
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.17 seconds!
[tester] 
SST2Metric: acc=0.805045871559633, hinge=0.9292131372548025, ce=0.4257930706195328
Local test acc @ epoch 13: 0.805
Client 1 execute local training on 8 samples...
Local loss @ local epoch 0: 0.32723280787467957
Local loss @ local epoch 1: 0.25228047370910645
Local loss @ local epoch 2: 0.19682994484901428
Local loss @ local epoch 3: 0.15888655185699463
Local loss @ local epoch 4: 0.1361326277256012
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.2 seconds!
[tester] 
SST2Metric: acc=0.8245412844036697, hinge=0.8648678186016345, ce=0.39839333917843095
Local test acc @ epoch 13: 0.8245
Client 4 execute local training on 8 samples...
Local loss @ local epoch 0: 0.36662745475769043
Local loss @ local epoch 1: 0.3136495053768158
Local loss @ local epoch 2: 0.27816256880760193
Local loss @ local epoch 3: 0.25320473313331604
Local loss @ local epoch 4: 0.2326478660106659
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.14 seconds!
[tester] 
SST2Metric: acc=0.8463302752293578, hinge=0.7855987938445642, ce=0.3462732877889904
Local test acc @ epoch 13: 0.8463
Client 2 execute local training on 8 samples...
Local loss @ local epoch 0: 0.4500417411327362
Local loss @ local epoch 1: 0.3452907204627991
Local loss @ local epoch 2: 0.2703979015350342
Local loss @ local epoch 3: 0.22241032123565674
Local loss @ local epoch 4: 0.19615478813648224
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.19 seconds!
[tester] 
SST2Metric: acc=0.7110091743119266, hinge=1.2564729620283897, ce=0.6336037026496109
Local test acc @ epoch 13: 0.711
Client 7 execute local training on 8 samples...
Local loss @ local epoch 0: 0.567551851272583
Local loss @ local epoch 1: 0.4338052570819855
Local loss @ local epoch 2: 0.3360886871814728
Local loss @ local epoch 3: 0.2725619077682495
Local loss @ local epoch 4: 0.23817750811576843
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.08 seconds!
[tester] 
SST2Metric: acc=0.8463302752293578, hinge=0.7757649387514919, ce=0.3420626952082192
Local test acc @ epoch 13: 0.8463
Global evaluate on test data...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.4 seconds!
[tester] 
SST2Metric: acc=0.7970183486238532, hinge=1.0259439290390102, ce=0.42467992636588736
Global test acc @ epoch 13: 0.797
Global epoch 14...
Client 9 execute local training on 8 samples...
Local loss @ local epoch 0: 0.13286975026130676
Local loss @ local epoch 1: 0.1232750192284584
Local loss @ local epoch 2: 0.11447116732597351
Local loss @ local epoch 3: 0.10646127164363861
Local loss @ local epoch 4: 0.09929259866476059
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.24 seconds!
[tester] 
SST2Metric: acc=0.8463302752293578, hinge=0.7749835468760324, ce=0.35219160197388144
Local test acc @ epoch 14: 0.8463
Client 5 execute local training on 8 samples...
Local loss @ local epoch 0: 0.17182248830795288
Local loss @ local epoch 1: 0.13224390149116516
Local loss @ local epoch 2: 0.11529257148504257
Local loss @ local epoch 3: 0.11343088746070862
Local loss @ local epoch 4: 0.11682367324829102
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.3 seconds!
[tester] 
SST2Metric: acc=0.8084862385321101, hinge=0.9037025863424354, ce=0.41648283108658746
Local test acc @ epoch 14: 0.8085
Client 6 execute local training on 8 samples...
Local loss @ local epoch 0: 0.33737319707870483
Local loss @ local epoch 1: 0.29260683059692383
Local loss @ local epoch 2: 0.2736223638057709
Local loss @ local epoch 3: 0.26705771684646606
Local loss @ local epoch 4: 0.26123547554016113
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.11 seconds!
[tester] 
SST2Metric: acc=0.8497706422018348, hinge=0.7773261566501145, ce=0.35074375361340854
Local test acc @ epoch 14: 0.8498
Client 3 execute local training on 8 samples...
Local loss @ local epoch 0: 0.29910629987716675
Local loss @ local epoch 1: 0.2633928954601288
Local loss @ local epoch 2: 0.2485058307647705
Local loss @ local epoch 3: 0.2484198659658432
Local loss @ local epoch 4: 0.2523955702781677
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.19 seconds!
[tester] 
SST2Metric: acc=0.8142201834862385, hinge=0.8864677348814973, ce=0.40830147245881754
Local test acc @ epoch 14: 0.8142
Client 8 execute local training on 8 samples...
Local loss @ local epoch 0: 0.40429186820983887
Local loss @ local epoch 1: 0.30576688051223755
Local loss @ local epoch 2: 0.23509109020233154
Local loss @ local epoch 3: 0.19058150053024292
Local loss @ local epoch 4: 0.16784481704235077
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.09 seconds!
[tester] 
SST2Metric: acc=0.838302752293578, hinge=0.8114724023899901, ce=0.37475225353322994
Local test acc @ epoch 14: 0.8383
Client 2 execute local training on 8 samples...
Local loss @ local epoch 0: 0.31963688135147095
Local loss @ local epoch 1: 0.2769557535648346
Local loss @ local epoch 2: 0.25134164094924927
Local loss @ local epoch 3: 0.23533692955970764
Local loss @ local epoch 4: 0.22307179868221283
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.14 seconds!
[tester] 
SST2Metric: acc=0.7110091743119266, hinge=1.247785111496208, ce=0.6410262573578762
Local test acc @ epoch 14: 0.711
Client 1 execute local training on 8 samples...
Local loss @ local epoch 0: 0.15244446694850922
Local loss @ local epoch 1: 0.12369858473539352
Local loss @ local epoch 2: 0.10843486338853836
Local loss @ local epoch 3: 0.10310196876525879
Local loss @ local epoch 4: 0.1020561009645462
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.15 seconds!
[tester] 
SST2Metric: acc=0.841743119266055, hinge=0.8326524996429409, ce=0.3788503340419826
Local test acc @ epoch 14: 0.8417
Client 4 execute local training on 8 samples...
Local loss @ local epoch 0: 0.2463064193725586
Local loss @ local epoch 1: 0.21705806255340576
Local loss @ local epoch 2: 0.1961642950773239
Local loss @ local epoch 3: 0.17828556895256042
Local loss @ local epoch 4: 0.16176125407218933
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.13 seconds!
[tester] 
SST2Metric: acc=0.8302752293577982, hinge=0.8551002708050089, ce=0.38277304934662415
Local test acc @ epoch 14: 0.8303
Client 7 execute local training on 8 samples...
Local loss @ local epoch 0: 0.286345511674881
Local loss @ local epoch 1: 0.26363998651504517
Local loss @ local epoch 2: 0.24404439330101013
Local loss @ local epoch 3: 0.22645899653434753
Local loss @ local epoch 4: 0.21093490719795227
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.07 seconds!
[tester] 
SST2Metric: acc=0.8497706422018348, hinge=0.7458385338477038, ce=0.33449649706551243
Local test acc @ epoch 14: 0.8498
Client 0 execute local training on 8 samples...
Local loss @ local epoch 0: 0.16671222448349
Local loss @ local epoch 1: 0.15699130296707153
Local loss @ local epoch 2: 0.15274640917778015
Local loss @ local epoch 3: 0.1472252458333969
Local loss @ local epoch 4: 0.14109498262405396
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.12 seconds!
[tester] 
SST2Metric: acc=0.8463302752293578, hinge=0.747888707786525, ce=0.3373365480999608
Local test acc @ epoch 14: 0.8463
Global evaluate on test data...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.66 seconds!
[tester] 
SST2Metric: acc=0.801605504587156, hinge=1.0050911480953935, ce=0.42156322683216235
Global test acc @ epoch 14: 0.8016
Global epoch 15...
Client 2 execute local training on 8 samples...
Local loss @ local epoch 0: 0.3564233183860779
Local loss @ local epoch 1: 0.2836434841156006
Local loss @ local epoch 2: 0.2373051941394806
Local loss @ local epoch 3: 0.21215400099754333
Local loss @ local epoch 4: 0.20090819895267487
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.15 seconds!
[tester] 
SST2Metric: acc=0.7144495412844036, hinge=1.2282071750645245, ce=0.6271259503934635
Local test acc @ epoch 15: 0.7144
Client 7 execute local training on 8 samples...
Local loss @ local epoch 0: 0.6117261648178101
Local loss @ local epoch 1: 0.46375685930252075
Local loss @ local epoch 2: 0.35193049907684326
Local loss @ local epoch 3: 0.274931401014328
Local loss @ local epoch 4: 0.22864273190498352
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.11 seconds!
[tester] 
SST2Metric: acc=0.8474770642201835, hinge=0.7556810299737737, ce=0.33803646632154055
Local test acc @ epoch 15: 0.8475
Client 5 execute local training on 8 samples...
Local loss @ local epoch 0: 0.1281009465456009
Local loss @ local epoch 1: 0.10787814855575562
Local loss @ local epoch 2: 0.10449736565351486
Local loss @ local epoch 3: 0.10569065064191818
Local loss @ local epoch 4: 0.10371355712413788
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.19 seconds!
[tester] 
SST2Metric: acc=0.8291284403669725, hinge=0.820186346632625, ce=0.37441355632532625
Local test acc @ epoch 15: 0.8291
Client 3 execute local training on 8 samples...
Local loss @ local epoch 0: 0.2477206140756607
Local loss @ local epoch 1: 0.23838816583156586
Local loss @ local epoch 2: 0.23597782850265503
Local loss @ local epoch 3: 0.2309059500694275
Local loss @ local epoch 4: 0.22466827929019928
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.16 seconds!
[tester] 
SST2Metric: acc=0.838302752293578, hinge=0.7961738202549996, ce=0.36108008078342185
Local test acc @ epoch 15: 0.8383
Client 1 execute local training on 8 samples...
Local loss @ local epoch 0: 0.23355983197689056
Local loss @ local epoch 1: 0.18072402477264404
Local loss @ local epoch 2: 0.14334434270858765
Local loss @ local epoch 3: 0.12015847116708755
Local loss @ local epoch 4: 0.10886549949645996
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.13 seconds!
[tester] 
SST2Metric: acc=0.8084862385321101, hinge=0.9400652313451154, ce=0.4476468094513504
Local test acc @ epoch 15: 0.8085
Client 8 execute local training on 8 samples...
Local loss @ local epoch 0: 0.1889866292476654
Local loss @ local epoch 1: 0.1681879609823227
Local loss @ local epoch 2: 0.15697376430034637
Local loss @ local epoch 3: 0.14694571495056152
Local loss @ local epoch 4: 0.13651534914970398
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.09 seconds!
[tester] 
SST2Metric: acc=0.8394495412844036, hinge=0.7746922440758539, ce=0.3587657317109064
Local test acc @ epoch 15: 0.8394
Client 0 execute local training on 8 samples...
Local loss @ local epoch 0: 0.1714864820241928
Local loss @ local epoch 1: 0.1480054259300232
Local loss @ local epoch 2: 0.1420394629240036
Local loss @ local epoch 3: 0.1434098780155182
Local loss @ local epoch 4: 0.1436067372560501
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.17 seconds!
[tester] 
SST2Metric: acc=0.8348623853211009, hinge=0.7997754074017936, ce=0.3641727209398779
Local test acc @ epoch 15: 0.8349
Client 9 execute local training on 8 samples...
Local loss @ local epoch 0: 0.1503075510263443
Local loss @ local epoch 1: 0.11481650173664093
Local loss @ local epoch 2: 0.09918846189975739
Local loss @ local epoch 3: 0.09648705273866653
Local loss @ local epoch 4: 0.09832260012626648
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.2 seconds!
[tester] 
SST2Metric: acc=0.8291284403669725, hinge=0.8293887848427536, ce=0.3913067195120208
Local test acc @ epoch 15: 0.8291
Client 6 execute local training on 8 samples...
Local loss @ local epoch 0: 0.3748701214790344
Local loss @ local epoch 1: 0.3129122853279114
Local loss @ local epoch 2: 0.2714649438858032
Local loss @ local epoch 3: 0.251018762588501
Local loss @ local epoch 4: 0.2460584193468094
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.13 seconds!
[tester] 
SST2Metric: acc=0.8360091743119266, hinge=0.7718846335870411, ce=0.3517487153342558
Local test acc @ epoch 15: 0.836
Client 4 execute local training on 8 samples...
Local loss @ local epoch 0: 0.30805590748786926
Local loss @ local epoch 1: 0.26068541407585144
Local loss @ local epoch 2: 0.23054924607276917
Local loss @ local epoch 3: 0.2108754813671112
Local loss @ local epoch 4: 0.19518092274665833
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.02 seconds!
[tester] 
SST2Metric: acc=0.8463302752293578, hinge=0.8394774719389206, ce=0.3846964583552759
Local test acc @ epoch 15: 0.8463
Global evaluate on test data...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.38 seconds!
[tester] 
SST2Metric: acc=0.7947247706422018, hinge=0.9976660047375828, ce=0.4228897114549208
Global test acc @ epoch 15: 0.7947
Global epoch 16...
Client 6 execute local training on 8 samples...
Local loss @ local epoch 0: 0.2515716254711151
Local loss @ local epoch 1: 0.23194466531276703
Local loss @ local epoch 2: 0.22245395183563232
Local loss @ local epoch 3: 0.21220794320106506
Local loss @ local epoch 4: 0.2000676989555359
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.14 seconds!
[tester] 
SST2Metric: acc=0.8463302752293578, hinge=0.773543304122916, ce=0.34788417602556015
Local test acc @ epoch 16: 0.8463
Client 3 execute local training on 8 samples...
Local loss @ local epoch 0: 0.2706695795059204
Local loss @ local epoch 1: 0.25102126598358154
Local loss @ local epoch 2: 0.24228663742542267
Local loss @ local epoch 3: 0.23617765307426453
Local loss @ local epoch 4: 0.22969210147857666
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.08 seconds!
[tester] 
SST2Metric: acc=0.8337155963302753, hinge=0.7991035904632796, ce=0.3641564061335467
Local test acc @ epoch 16: 0.8337
Client 7 execute local training on 8 samples...
Local loss @ local epoch 0: 0.20879508554935455
Local loss @ local epoch 1: 0.18870218098163605
Local loss @ local epoch 2: 0.18265725672245026
Local loss @ local epoch 3: 0.17828968167304993
Local loss @ local epoch 4: 0.17106641829013824
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.1 seconds!
[tester] 
SST2Metric: acc=0.8348623853211009, hinge=0.7615363802931724, ce=0.3473544363615983
Local test acc @ epoch 16: 0.8349
Client 8 execute local training on 8 samples...
Local loss @ local epoch 0: 0.18802548944950104
Local loss @ local epoch 1: 0.15077434480190277
Local loss @ local epoch 2: 0.13376332819461823
Local loss @ local epoch 3: 0.12915171682834625
Local loss @ local epoch 4: 0.12820810079574585
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.12 seconds!
[tester] 
SST2Metric: acc=0.838302752293578, hinge=0.8144304064709113, ce=0.3893113268214628
Local test acc @ epoch 16: 0.8383
Client 4 execute local training on 8 samples...
Local loss @ local epoch 0: 0.4788135886192322
Local loss @ local epoch 1: 0.4111347496509552
Local loss @ local epoch 2: 0.3617798089981079
Local loss @ local epoch 3: 0.32606276869773865
Local loss @ local epoch 4: 0.2982212007045746
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.28 seconds!
[tester] 
SST2Metric: acc=0.8497706422018348, hinge=0.7390483962286503, ce=0.33661070295156686
Local test acc @ epoch 16: 0.8498
Client 9 execute local training on 8 samples...
Local loss @ local epoch 0: 0.10929982364177704
Local loss @ local epoch 1: 0.09586600214242935
Local loss @ local epoch 2: 0.0927048921585083
Local loss @ local epoch 3: 0.08973933756351471
Local loss @ local epoch 4: 0.08466703444719315
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.17 seconds!
[tester] 
SST2Metric: acc=0.8394495412844036, hinge=0.7665747866171215, ce=0.35931033494138936
Local test acc @ epoch 16: 0.8394
Client 2 execute local training on 8 samples...
Local loss @ local epoch 0: 0.2806583046913147
Local loss @ local epoch 1: 0.24329620599746704
Local loss @ local epoch 2: 0.22102892398834229
Local loss @ local epoch 3: 0.20655205845832825
Local loss @ local epoch 4: 0.19476068019866943
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.11 seconds!
[tester] 
SST2Metric: acc=0.7442660550458715, hinge=1.1247215640107426, ce=0.5736794566410944
Local test acc @ epoch 16: 0.7443
Client 1 execute local training on 8 samples...
Local loss @ local epoch 0: 0.11817404627799988
Local loss @ local epoch 1: 0.09798350185155869
Local loss @ local epoch 2: 0.088589608669281
Local loss @ local epoch 3: 0.08558852970600128
Local loss @ local epoch 4: 0.08377213031053543
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.17 seconds!
[tester] 
SST2Metric: acc=0.8451834862385321, hinge=0.819066495933664, ce=0.38018508340924156
Local test acc @ epoch 16: 0.8452
Client 0 execute local training on 8 samples...
Local loss @ local epoch 0: 0.15936659276485443
Local loss @ local epoch 1: 0.14393603801727295
Local loss @ local epoch 2: 0.13957343995571136
Local loss @ local epoch 3: 0.13623711466789246
Local loss @ local epoch 4: 0.131275475025177
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.21 seconds!
[tester] 
SST2Metric: acc=0.8440366972477065, hinge=0.7364122292043966, ce=0.33745925348901423
Local test acc @ epoch 16: 0.844
Client 5 execute local training on 8 samples...
Local loss @ local epoch 0: 0.11192026734352112
Local loss @ local epoch 1: 0.09478185325860977
Local loss @ local epoch 2: 0.09215039014816284
Local loss @ local epoch 3: 0.09274391084909439
Local loss @ local epoch 4: 0.09032697975635529
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.17 seconds!
[tester] 
SST2Metric: acc=0.8279816513761468, hinge=0.828070972627456, ce=0.3868329964168028
Local test acc @ epoch 16: 0.828
Global evaluate on test data...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.56 seconds!
[tester] 
SST2Metric: acc=0.7958715596330275, hinge=0.98765097257741, ce=0.4216638305865297
Global test acc @ epoch 16: 0.7959
Global epoch 17...
Client 8 execute local training on 8 samples...
Local loss @ local epoch 0: 0.16397519409656525
Local loss @ local epoch 1: 0.13872526586055756
Local loss @ local epoch 2: 0.12899824976921082
Local loss @ local epoch 3: 0.12476540356874466
Local loss @ local epoch 4: 0.11978618800640106
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.13 seconds!
[tester] 
SST2Metric: acc=0.8302752293577982, hinge=0.8325624349740667, ce=0.3987650980643176
Local test acc @ epoch 17: 0.8303
Client 0 execute local training on 8 samples...
Local loss @ local epoch 0: 0.20553693175315857
Local loss @ local epoch 1: 0.16002623736858368
Local loss @ local epoch 2: 0.13611887395381927
Local loss @ local epoch 3: 0.12862583994865417
Local loss @ local epoch 4: 0.13049553334712982
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.2 seconds!
[tester] 
SST2Metric: acc=0.8360091743119266, hinge=0.8013353480384984, ce=0.36943604272508296
Local test acc @ epoch 17: 0.836
Client 4 execute local training on 8 samples...
Local loss @ local epoch 0: 0.3588489592075348
Local loss @ local epoch 1: 0.3086305856704712
Local loss @ local epoch 2: 0.27447545528411865
Local loss @ local epoch 3: 0.24994701147079468
Local loss @ local epoch 4: 0.2293919026851654
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.21 seconds!
[tester] 
SST2Metric: acc=0.856651376146789, hinge=0.7765484814523557, ce=0.3568742332655355
Local test acc @ epoch 17: 0.8567
Client 3 execute local training on 8 samples...
Local loss @ local epoch 0: 0.2779410481452942
Local loss @ local epoch 1: 0.24204765260219574
Local loss @ local epoch 2: 0.22606854140758514
Local loss @ local epoch 3: 0.223519429564476
Local loss @ local epoch 4: 0.2254556119441986
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.14 seconds!
[tester] 
SST2Metric: acc=0.8245412844036697, hinge=0.8427754348024316, ce=0.39134651648068647
Local test acc @ epoch 17: 0.8245
Client 2 execute local training on 8 samples...
Local loss @ local epoch 0: 0.6085430979728699
Local loss @ local epoch 1: 0.4671770930290222
Local loss @ local epoch 2: 0.35615074634552
Local loss @ local epoch 3: 0.27470308542251587
Local loss @ local epoch 4: 0.22000530362129211
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.17 seconds!
[tester] 
SST2Metric: acc=0.8245412844036697, hinge=0.8504268472347785, ce=0.400258627609102
Local test acc @ epoch 17: 0.8245
Client 7 execute local training on 8 samples...
Local loss @ local epoch 0: 0.2864971160888672
Local loss @ local epoch 1: 0.2225862741470337
Local loss @ local epoch 2: 0.18744678795337677
Local loss @ local epoch 3: 0.17425508797168732
Local loss @ local epoch 4: 0.17356088757514954
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.17 seconds!
[tester] 
SST2Metric: acc=0.8360091743119266, hinge=0.8001897318647542, ce=0.36861380890285206
Local test acc @ epoch 17: 0.836
Client 6 execute local training on 8 samples...
Local loss @ local epoch 0: 0.26935315132141113
Local loss @ local epoch 1: 0.23909610509872437
Local loss @ local epoch 2: 0.22848869860172272
Local loss @ local epoch 3: 0.22257746756076813
Local loss @ local epoch 4: 0.21407541632652283
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.14 seconds!
[tester] 
SST2Metric: acc=0.8497706422018348, hinge=0.7648169413345669, ce=0.3505189998946879
Local test acc @ epoch 17: 0.8498
Client 1 execute local training on 8 samples...
Local loss @ local epoch 0: 0.1005496010184288
Local loss @ local epoch 1: 0.08260329812765121
Local loss @ local epoch 2: 0.07502872496843338
Local loss @ local epoch 3: 0.0732874795794487
Local loss @ local epoch 4: 0.07252970337867737
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.13 seconds!
[tester] 
SST2Metric: acc=0.768348623853211, hinge=1.0574729109029157, ce=0.5225498507636677
Local test acc @ epoch 17: 0.7683
Client 5 execute local training on 8 samples...
Local loss @ local epoch 0: 0.41136202216148376
Local loss @ local epoch 1: 0.29254165291786194
Local loss @ local epoch 2: 0.2060079574584961
Local loss @ local epoch 3: 0.14801451563835144
Local loss @ local epoch 4: 0.11312083154916763
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.11 seconds!
[tester] 
SST2Metric: acc=0.8394495412844036, hinge=0.7416948738994948, ce=0.3400480237815085
Local test acc @ epoch 17: 0.8394
Client 9 execute local training on 8 samples...
Local loss @ local epoch 0: 0.14278316497802734
Local loss @ local epoch 1: 0.11738576740026474
Local loss @ local epoch 2: 0.10825490951538086
Local loss @ local epoch 3: 0.10576026141643524
Local loss @ local epoch 4: 0.10259679704904556
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.14 seconds!
[tester] 
SST2Metric: acc=0.8497706422018348, hinge=0.7960429894267966, ce=0.3729234399496142
Local test acc @ epoch 17: 0.8498
Global evaluate on test data...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.99 seconds!
[tester] 
SST2Metric: acc=0.8004587155963303, hinge=0.9818090285183093, ce=0.4184424616738197
Global test acc @ epoch 17: 0.8005
Global epoch 18...
Client 2 execute local training on 8 samples...
Local loss @ local epoch 0: 0.2781287133693695
Local loss @ local epoch 1: 0.21758168935775757
Local loss @ local epoch 2: 0.18146875500679016
Local loss @ local epoch 3: 0.1639525592327118
Local loss @ local epoch 4: 0.15770241618156433
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.22 seconds!
[tester] 
SST2Metric: acc=0.7385321100917431, hinge=1.1581870555330853, ce=0.5953627855371718
Local test acc @ epoch 18: 0.7385
Client 6 execute local training on 8 samples...
Local loss @ local epoch 0: 0.44210124015808105
Local loss @ local epoch 1: 0.35626816749572754
Local loss @ local epoch 2: 0.2861364483833313
Local loss @ local epoch 3: 0.23449690639972687
Local loss @ local epoch 4: 0.20282810926437378
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.22 seconds!
[tester] 
SST2Metric: acc=0.8463302752293578, hinge=0.7563611720133265, ce=0.34441405685122956
Local test acc @ epoch 18: 0.8463
Client 3 execute local training on 8 samples...
Local loss @ local epoch 0: 0.2525315582752228
Local loss @ local epoch 1: 0.23421424627304077
Local loss @ local epoch 2: 0.22728656232357025
Local loss @ local epoch 3: 0.22253547608852386
Local loss @ local epoch 4: 0.21678279340267181
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.12 seconds!
[tester] 
SST2Metric: acc=0.8325688073394495, hinge=0.7922562917164706, ce=0.36509381614010267
Local test acc @ epoch 18: 0.8326
Client 0 execute local training on 8 samples...
Local loss @ local epoch 0: 0.1508570909500122
Local loss @ local epoch 1: 0.12899470329284668
Local loss @ local epoch 2: 0.11980369687080383
Local loss @ local epoch 3: 0.11975375562906265
Local loss @ local epoch 4: 0.12076956778764725
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.09 seconds!
[tester] 
SST2Metric: acc=0.8463302752293578, hinge=0.7545519257904193, ce=0.3483989276846341
Local test acc @ epoch 18: 0.8463
Client 1 execute local training on 8 samples...
Local loss @ local epoch 0: 0.10564291477203369
Local loss @ local epoch 1: 0.09023556113243103
Local loss @ local epoch 2: 0.0833912342786789
Local loss @ local epoch 3: 0.07973705977201462
Local loss @ local epoch 4: 0.07599141448736191
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.21 seconds!
[tester] 
SST2Metric: acc=0.7970183486238532, hinge=0.9582737889311729, ce=0.46565487451099474
Local test acc @ epoch 18: 0.797
Client 8 execute local training on 8 samples...
Local loss @ local epoch 0: 0.12798942625522614
Local loss @ local epoch 1: 0.11557702720165253
Local loss @ local epoch 2: 0.10651940852403641
Local loss @ local epoch 3: 0.09797583520412445
Local loss @ local epoch 4: 0.09023300558328629
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.12 seconds!
[tester] 
SST2Metric: acc=0.8337155963302753, hinge=0.8251012301226275, ce=0.3946750818179288
Local test acc @ epoch 18: 0.8337
Client 4 execute local training on 8 samples...
Local loss @ local epoch 0: 0.3605467677116394
Local loss @ local epoch 1: 0.3029195964336395
Local loss @ local epoch 2: 0.2671932578086853
Local loss @ local epoch 3: 0.24621468782424927
Local loss @ local epoch 4: 0.23165500164031982
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.17 seconds!
[tester] 
SST2Metric: acc=0.8371559633027523, hinge=0.7436016441211788, ce=0.3426596696975581
Local test acc @ epoch 18: 0.8372
Client 9 execute local training on 8 samples...
Local loss @ local epoch 0: 0.12465707212686539
Local loss @ local epoch 1: 0.10556336492300034
Local loss @ local epoch 2: 0.1003011018037796
Local loss @ local epoch 3: 0.0981937125325203
Local loss @ local epoch 4: 0.09398062527179718
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.15 seconds!
[tester] 
SST2Metric: acc=0.8440366972477065, hinge=0.7982826182328233, ce=0.3783214969064938
Local test acc @ epoch 18: 0.844
Client 7 execute local training on 8 samples...
Local loss @ local epoch 0: 0.25503242015838623
Local loss @ local epoch 1: 0.2002858817577362
Local loss @ local epoch 2: 0.17244988679885864
Local loss @ local epoch 3: 0.16401106119155884
Local loss @ local epoch 4: 0.16513188183307648
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.08 seconds!
[tester] 
SST2Metric: acc=0.8222477064220184, hinge=0.822818016513772, ce=0.38636294089326073
Local test acc @ epoch 18: 0.8222
Client 5 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0959358662366867
Local loss @ local epoch 1: 0.09038368612527847
Local loss @ local epoch 2: 0.08491750061511993
Local loss @ local epoch 3: 0.08008135110139847
Local loss @ local epoch 4: 0.07558583468198776
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.16 seconds!
[tester] 
SST2Metric: acc=0.8337155963302753, hinge=0.7851838648592661, ce=0.36922984115711044
Local test acc @ epoch 18: 0.8337
Global evaluate on test data...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.63 seconds!
[tester] 
SST2Metric: acc=0.7970183486238532, hinge=0.988445975501603, ce=0.42915061642543983
Global test acc @ epoch 18: 0.797
Global epoch 19...
Client 2 execute local training on 8 samples...
Local loss @ local epoch 0: 0.2212544083595276
Local loss @ local epoch 1: 0.17721214890480042
Local loss @ local epoch 2: 0.1541692614555359
Local loss @ local epoch 3: 0.14492738246917725
Local loss @ local epoch 4: 0.14188756048679352
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.21 seconds!
[tester] 
SST2Metric: acc=0.7488532110091743, hinge=1.1247081474973522, ce=0.5782327352929006
Local test acc @ epoch 19: 0.7489
Client 5 execute local training on 8 samples...
Local loss @ local epoch 0: 0.5846981406211853
Local loss @ local epoch 1: 0.4277763068675995
Local loss @ local epoch 2: 0.3053564131259918
Local loss @ local epoch 3: 0.2161777913570404
Local loss @ local epoch 4: 0.15591619908809662
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.1 seconds!
[tester] 
SST2Metric: acc=0.8405963302752294, hinge=0.7339692729602166, ce=0.3382105447625348
Local test acc @ epoch 19: 0.8406
Client 7 execute local training on 8 samples...
Local loss @ local epoch 0: 0.17970281839370728
Local loss @ local epoch 1: 0.1690944880247116
Local loss @ local epoch 2: 0.16042327880859375
Local loss @ local epoch 3: 0.15188419818878174
Local loss @ local epoch 4: 0.14448659121990204
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.09 seconds!
[tester] 
SST2Metric: acc=0.8428899082568807, hinge=0.7792922097061752, ce=0.3632832102716789
Local test acc @ epoch 19: 0.8429
Client 4 execute local training on 8 samples...
Local loss @ local epoch 0: 0.32186660170555115
Local loss @ local epoch 1: 0.2784861624240875
Local loss @ local epoch 2: 0.24876543879508972
Local loss @ local epoch 3: 0.22620409727096558
Local loss @ local epoch 4: 0.20632897317409515
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.15 seconds!
[tester] 
SST2Metric: acc=0.8532110091743119, hinge=0.7508268003616858, ce=0.34763342798303026
Local test acc @ epoch 19: 0.8532
Client 3 execute local training on 8 samples...
Local loss @ local epoch 0: 0.252389520406723
Local loss @ local epoch 1: 0.2213876098394394
Local loss @ local epoch 2: 0.20875491201877594
Local loss @ local epoch 3: 0.2075687050819397
Local loss @ local epoch 4: 0.20894725620746613
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.11 seconds!
[tester] 
SST2Metric: acc=0.8245412844036697, hinge=0.8268435846228118, ce=0.38766911396876386
Local test acc @ epoch 19: 0.8245
Client 0 execute local training on 8 samples...
Local loss @ local epoch 0: 0.15763883292675018
Local loss @ local epoch 1: 0.129502534866333
Local loss @ local epoch 2: 0.11311721056699753
Local loss @ local epoch 3: 0.10811715573072433
Local loss @ local epoch 4: 0.11000625789165497
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.11 seconds!
[tester] 
SST2Metric: acc=0.8520642201834863, hinge=0.7443813197656509, ce=0.3462248960116861
Local test acc @ epoch 19: 0.8521
Client 6 execute local training on 8 samples...
Local loss @ local epoch 0: 0.28420761227607727
Local loss @ local epoch 1: 0.24835646152496338
Local loss @ local epoch 2: 0.23276115953922272
Local loss @ local epoch 3: 0.2276480346918106
Local loss @ local epoch 4: 0.2219129204750061
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.21 seconds!
[tester] 
SST2Metric: acc=0.8474770642201835, hinge=0.7357216448685445, ce=0.3396604064715291
Local test acc @ epoch 19: 0.8475
Client 1 execute local training on 8 samples...
Local loss @ local epoch 0: 0.13436897099018097
Local loss @ local epoch 1: 0.09943906217813492
Local loss @ local epoch 2: 0.07705806940793991
Local loss @ local epoch 3: 0.06519464403390884
Local loss @ local epoch 4: 0.061053890734910965
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.06 seconds!
[tester] 
SST2Metric: acc=0.7844036697247706, hinge=0.9749190306991612, ce=0.477375436095743
Local test acc @ epoch 19: 0.7844
Client 8 execute local training on 8 samples...
Local loss @ local epoch 0: 0.1301693618297577
Local loss @ local epoch 1: 0.11650383472442627
Local loss @ local epoch 2: 0.10543286055326462
Local loss @ local epoch 3: 0.09584583342075348
Local loss @ local epoch 4: 0.08765885978937149
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.11 seconds!
[tester] 
SST2Metric: acc=0.8211009174311926, hinge=0.8403281227711143, ce=0.4077840777575423
Local test acc @ epoch 19: 0.8211
Client 9 execute local training on 8 samples...
Local loss @ local epoch 0: 0.10185225307941437
Local loss @ local epoch 1: 0.08773522824048996
Local loss @ local epoch 2: 0.0844019204378128
Local loss @ local epoch 3: 0.0823865458369255
Local loss @ local epoch 4: 0.07817108184099197
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.06 seconds!
[tester] 
SST2Metric: acc=0.8463302752293578, hinge=0.742658729126694, ce=0.3532185556042358
Local test acc @ epoch 19: 0.8463
Global evaluate on test data...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.78 seconds!
[tester] 
SST2Metric: acc=0.7970183486238532, hinge=0.9791268145819323, ce=0.42773466208659183
Global test acc @ epoch 19: 0.797
Global epoch 20...
Client 7 execute local training on 8 samples...
Local loss @ local epoch 0: 0.1837206482887268
Local loss @ local epoch 1: 0.15823118388652802
Local loss @ local epoch 2: 0.15098655223846436
Local loss @ local epoch 3: 0.14972883462905884
Local loss @ local epoch 4: 0.14665517210960388
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.25 seconds!
[tester] 
SST2Metric: acc=0.8245412844036697, hinge=0.806684864633674, ce=0.3784970648825989
Local test acc @ epoch 20: 0.8245
Client 4 execute local training on 8 samples...
Local loss @ local epoch 0: 0.30630797147750854
Local loss @ local epoch 1: 0.25518718361854553
Local loss @ local epoch 2: 0.223433718085289
Local loss @ local epoch 3: 0.20424595475196838
Local loss @ local epoch 4: 0.19039589166641235
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.23 seconds!
[tester] 
SST2Metric: acc=0.856651376146789, hinge=0.7576222370523925, ce=0.35258148938690853
Local test acc @ epoch 20: 0.8567
Client 3 execute local training on 8 samples...
Local loss @ local epoch 0: 0.25327369570732117
Local loss @ local epoch 1: 0.2197110950946808
Local loss @ local epoch 2: 0.2049330323934555
Local loss @ local epoch 3: 0.2025081068277359
Local loss @ local epoch 4: 0.20394615828990936
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.15 seconds!
[tester] 
SST2Metric: acc=0.8279816513761468, hinge=0.817304439774347, ce=0.3838019435285428
Local test acc @ epoch 20: 0.828
Client 1 execute local training on 8 samples...
Local loss @ local epoch 0: 0.20622555911540985
Local loss @ local epoch 1: 0.15139594674110413
Local loss @ local epoch 2: 0.11201339960098267
Local loss @ local epoch 3: 0.08603305369615555
Local loss @ local epoch 4: 0.07105828076601028
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.05 seconds!
[tester] 
SST2Metric: acc=0.8371559633027523, hinge=0.8311704007584021, ce=0.39471076933000615
Local test acc @ epoch 20: 0.8372
Client 2 execute local training on 8 samples...
Local loss @ local epoch 0: 0.14984974265098572
Local loss @ local epoch 1: 0.13063186407089233
Local loss @ local epoch 2: 0.12369317561388016
Local loss @ local epoch 3: 0.12014427781105042
Local loss @ local epoch 4: 0.11564162373542786
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.04 seconds!
[tester] 
SST2Metric: acc=0.7259174311926605, hinge=1.2277201662916657, ce=0.6428436320787723
Local test acc @ epoch 20: 0.7259
Client 8 execute local training on 8 samples...
Local loss @ local epoch 0: 0.16446591913700104
Local loss @ local epoch 1: 0.13231542706489563
Local loss @ local epoch 2: 0.1170584112405777
Local loss @ local epoch 3: 0.11055122315883636
Local loss @ local epoch 4: 0.10590752959251404
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.13 seconds!
[tester] 
SST2Metric: acc=0.8360091743119266, hinge=0.8167456503308147, ce=0.38870826601093517
Local test acc @ epoch 20: 0.836
Client 9 execute local training on 8 samples...
Local loss @ local epoch 0: 0.10948629677295685
Local loss @ local epoch 1: 0.09639918059110641
Local loss @ local epoch 2: 0.09201104938983917
Local loss @ local epoch 3: 0.08782941102981567
Local loss @ local epoch 4: 0.08207572996616364
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.18 seconds!
[tester] 
SST2Metric: acc=0.8474770642201835, hinge=0.7354015033999715, ce=0.34703398304521493
Local test acc @ epoch 20: 0.8475
Client 0 execute local training on 8 samples...
Local loss @ local epoch 0: 0.11332867294549942
Local loss @ local epoch 1: 0.09987363964319229
Local loss @ local epoch 2: 0.09801492094993591
Local loss @ local epoch 3: 0.09803731739521027
Local loss @ local epoch 4: 0.09573089331388474
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.14 seconds!
[tester] 
SST2Metric: acc=0.8428899082568807, hinge=0.7757853152008232, ce=0.3664583643436979
Local test acc @ epoch 20: 0.8429
Client 6 execute local training on 8 samples...
Local loss @ local epoch 0: 0.2598653733730316
Local loss @ local epoch 1: 0.2429140955209732
Local loss @ local epoch 2: 0.22708776593208313
Local loss @ local epoch 3: 0.21217413246631622
Local loss @ local epoch 4: 0.19820603728294373
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.15 seconds!
[tester] 
SST2Metric: acc=0.8451834862385321, hinge=0.723098676959309, ce=0.3378356377841956
Local test acc @ epoch 20: 0.8452
Client 5 execute local training on 8 samples...
Local loss @ local epoch 0: 0.12048617005348206
Local loss @ local epoch 1: 0.09502724558115005
Local loss @ local epoch 2: 0.08660361170768738
Local loss @ local epoch 3: 0.08732137829065323
Local loss @ local epoch 4: 0.08874552696943283
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.24 seconds!
[tester] 
SST2Metric: acc=0.8222477064220184, hinge=0.8599016819525203, ce=0.4140407030188709
Local test acc @ epoch 20: 0.8222
Global evaluate on test data...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 79.02 seconds!
[tester] 
SST2Metric: acc=0.7970183486238532, hinge=0.963876564431628, ce=0.41610309030484716
Global test acc @ epoch 20: 0.797
Global epoch 21...
Client 0 execute local training on 8 samples...
Local loss @ local epoch 0: 0.10691431164741516
Local loss @ local epoch 1: 0.10052704811096191
Local loss @ local epoch 2: 0.09525801986455917
Local loss @ local epoch 3: 0.09090165793895721
Local loss @ local epoch 4: 0.08719491213560104
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.05 seconds!
[tester] 
SST2Metric: acc=0.8474770642201835, hinge=0.7437014819011776, ce=0.3490992281585932
Local test acc @ epoch 21: 0.8475
Client 6 execute local training on 8 samples...
Local loss @ local epoch 0: 0.26777005195617676
Local loss @ local epoch 1: 0.24805444478988647
Local loss @ local epoch 2: 0.23740291595458984
Local loss @ local epoch 3: 0.22573092579841614
Local loss @ local epoch 4: 0.2125464230775833
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.09 seconds!
[tester] 
SST2Metric: acc=0.8405963302752294, hinge=0.729927490884011, ce=0.34008524409674723
Local test acc @ epoch 21: 0.8406
Client 4 execute local training on 8 samples...
Local loss @ local epoch 0: 0.20437052845954895
Local loss @ local epoch 1: 0.17537978291511536
Local loss @ local epoch 2: 0.15884965658187866
Local loss @ local epoch 3: 0.14710210263729095
Local loss @ local epoch 4: 0.13591313362121582
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.25 seconds!
[tester] 
SST2Metric: acc=0.841743119266055, hinge=0.8183350664213163, ce=0.38668267779188964
Local test acc @ epoch 21: 0.8417
Client 1 execute local training on 8 samples...
Local loss @ local epoch 0: 0.06090880185365677
Local loss @ local epoch 1: 0.05249106138944626
Local loss @ local epoch 2: 0.05039311945438385
Local loss @ local epoch 3: 0.04962945729494095
Local loss @ local epoch 4: 0.04775221273303032
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.14 seconds!
[tester] 
SST2Metric: acc=0.7821100917431193, hinge=1.0133372411268566, ce=0.5031479212771589
Local test acc @ epoch 21: 0.7821
Client 5 execute local training on 8 samples...
Local loss @ local epoch 0: 0.4007302224636078
Local loss @ local epoch 1: 0.28651097416877747
Local loss @ local epoch 2: 0.2024342566728592
Local loss @ local epoch 3: 0.14522160589694977
Local loss @ local epoch 4: 0.11007872223854065
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.13 seconds!
[tester] 
SST2Metric: acc=0.8451834862385321, hinge=0.7417894290127885, ce=0.3478660166707881
Local test acc @ epoch 21: 0.8452
Client 7 execute local training on 8 samples...
Local loss @ local epoch 0: 0.2242269217967987
Local loss @ local epoch 1: 0.19226804375648499
Local loss @ local epoch 2: 0.17639239132404327
Local loss @ local epoch 3: 0.16953718662261963
Local loss @ local epoch 4: 0.16370515525341034
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.16 seconds!
[tester] 
SST2Metric: acc=0.8497706422018348, hinge=0.7249952789840348, ce=0.3402418931689831
Local test acc @ epoch 21: 0.8498
Client 9 execute local training on 8 samples...
Local loss @ local epoch 0: 0.09331778436899185
Local loss @ local epoch 1: 0.08575363457202911
Local loss @ local epoch 2: 0.07905983924865723
Local loss @ local epoch 3: 0.07283394783735275
Local loss @ local epoch 4: 0.06748703867197037
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.18 seconds!
[tester] 
SST2Metric: acc=0.8612385321100917, hinge=0.7231980890309045, ce=0.3453541754001598
Local test acc @ epoch 21: 0.8612
Client 3 execute local training on 8 samples...
Local loss @ local epoch 0: 0.269541472196579
Local loss @ local epoch 1: 0.23053933680057526
Local loss @ local epoch 2: 0.20758949220180511
Local loss @ local epoch 3: 0.19954487681388855
Local loss @ local epoch 4: 0.20142777264118195
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.12 seconds!
[tester] 
SST2Metric: acc=0.8256880733944955, hinge=0.8398754174282791, ce=0.40624390884276923
Local test acc @ epoch 21: 0.8257
Client 2 execute local training on 8 samples...
Local loss @ local epoch 0: 0.48751866817474365
Local loss @ local epoch 1: 0.3648647665977478
Local loss @ local epoch 2: 0.27153050899505615
Local loss @ local epoch 3: 0.20550204813480377
Local loss @ local epoch 4: 0.16289353370666504
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.18 seconds!
[tester] 
SST2Metric: acc=0.8474770642201835, hinge=0.7668147910078731, ce=0.3704237692878334
Local test acc @ epoch 21: 0.8475
Client 8 execute local training on 8 samples...
Local loss @ local epoch 0: 0.08776557445526123
Local loss @ local epoch 1: 0.08049819618463516
Local loss @ local epoch 2: 0.07510370761156082
Local loss @ local epoch 3: 0.06960762292146683
Local loss @ local epoch 4: 0.06454767286777496
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.07 seconds!
[tester] 
SST2Metric: acc=0.8405963302752294, hinge=0.7673978789137044, ce=0.3759577601225278
Local test acc @ epoch 21: 0.8406
Global evaluate on test data...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.69 seconds!
[tester] 
SST2Metric: acc=0.7970183486238532, hinge=0.9538083743611607, ce=0.41695754855461076
Global test acc @ epoch 21: 0.797
Global epoch 22...
Client 2 execute local training on 8 samples...
Local loss @ local epoch 0: 0.19229838252067566
Local loss @ local epoch 1: 0.14725539088249207
Local loss @ local epoch 2: 0.12256063520908356
Local loss @ local epoch 3: 0.11233846098184586
Local loss @ local epoch 4: 0.11007796972990036
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.17 seconds!
[tester] 
SST2Metric: acc=0.7626146788990825, hinge=1.0836332998144518, ce=0.5606137764887935
Local test acc @ epoch 22: 0.7626
Client 5 execute local training on 8 samples...
Local loss @ local epoch 0: 0.555759847164154
Local loss @ local epoch 1: 0.4042239785194397
Local loss @ local epoch 2: 0.28690680861473083
Local loss @ local epoch 3: 0.20190297067165375
Local loss @ local epoch 4: 0.14444401860237122
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.26 seconds!
[tester] 
SST2Metric: acc=0.8440366972477065, hinge=0.7132157508386384, ce=0.33460957893606175
Local test acc @ epoch 22: 0.844
Client 7 execute local training on 8 samples...
Local loss @ local epoch 0: 0.14526519179344177
Local loss @ local epoch 1: 0.13625819981098175
Local loss @ local epoch 2: 0.12853406369686127
Local loss @ local epoch 3: 0.12147975713014603
Local loss @ local epoch 4: 0.11555281281471252
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.12 seconds!
[tester] 
SST2Metric: acc=0.8463302752293578, hinge=0.7537888509964724, ce=0.358665998239036
Local test acc @ epoch 22: 0.8463
Client 3 execute local training on 8 samples...
Local loss @ local epoch 0: 0.20440565049648285
Local loss @ local epoch 1: 0.19430240988731384
Local loss @ local epoch 2: 0.19104528427124023
Local loss @ local epoch 3: 0.18650177121162415
Local loss @ local epoch 4: 0.1808539777994156
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.22 seconds!
[tester] 
SST2Metric: acc=0.8360091743119266, hinge=0.7638371322953373, ce=0.36266872498693814
Local test acc @ epoch 22: 0.836
Client 1 execute local training on 8 samples...
Local loss @ local epoch 0: 0.15402507781982422
Local loss @ local epoch 1: 0.1122494637966156
Local loss @ local epoch 2: 0.08421281725168228
Local loss @ local epoch 3: 0.06762184947729111
Local loss @ local epoch 4: 0.05968298390507698
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.16 seconds!
[tester] 
SST2Metric: acc=0.8279816513761468, hinge=0.8328694427778961, ce=0.40712518139033144
Local test acc @ epoch 22: 0.828
Client 0 execute local training on 8 samples...
Local loss @ local epoch 0: 0.1300477832555771
Local loss @ local epoch 1: 0.1007452979683876
Local loss @ local epoch 2: 0.08770950883626938
Local loss @ local epoch 3: 0.08524815738201141
Local loss @ local epoch 4: 0.08702701330184937
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.11 seconds!
[tester] 
SST2Metric: acc=0.8497706422018348, hinge=0.7292190680810071, ce=0.34365131461634
Local test acc @ epoch 22: 0.8498
Client 9 execute local training on 8 samples...
Local loss @ local epoch 0: 0.11024605482816696
Local loss @ local epoch 1: 0.08540413528680801
Local loss @ local epoch 2: 0.07546454668045044
Local loss @ local epoch 3: 0.0740736648440361
Local loss @ local epoch 4: 0.0744527205824852
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.14 seconds!
[tester] 
SST2Metric: acc=0.8451834862385321, hinge=0.7766280104534342, ce=0.3782221371427589
Local test acc @ epoch 22: 0.8452
Client 6 execute local training on 8 samples...
Local loss @ local epoch 0: 0.34102192521095276
Local loss @ local epoch 1: 0.27909228205680847
Local loss @ local epoch 2: 0.23694169521331787
Local loss @ local epoch 3: 0.215467169880867
Local loss @ local epoch 4: 0.21026520431041718
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.16 seconds!
[tester] 
SST2Metric: acc=0.841743119266055, hinge=0.7319776536127843, ce=0.3467780398973905
Local test acc @ epoch 22: 0.8417
Client 4 execute local training on 8 samples...
Local loss @ local epoch 0: 0.22169546782970428
Local loss @ local epoch 1: 0.18072102963924408
Local loss @ local epoch 2: 0.15682926774024963
Local loss @ local epoch 3: 0.14359059929847717
Local loss @ local epoch 4: 0.1344209611415863
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.13 seconds!
[tester] 
SST2Metric: acc=0.8532110091743119, hinge=0.7839614029871215, ce=0.37443564741641555
Local test acc @ epoch 22: 0.8532
Client 8 execute local training on 8 samples...
Local loss @ local epoch 0: 0.15209972858428955
Local loss @ local epoch 1: 0.12543675303459167
Local loss @ local epoch 2: 0.11275768280029297
Local loss @ local epoch 3: 0.10617474466562271
Local loss @ local epoch 4: 0.10032891482114792
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.09 seconds!
[tester] 
SST2Metric: acc=0.805045871559633, hinge=0.9265934253504516, ce=0.4658693116021539
Local test acc @ epoch 22: 0.805
Global evaluate on test data...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.62 seconds!
[tester] 
SST2Metric: acc=0.8027522935779816, hinge=0.9468326586375543, ce=0.4196489793034868
Global test acc @ epoch 22: 0.8028
Global epoch 23...
Client 1 execute local training on 8 samples...
Local loss @ local epoch 0: 0.06752458959817886
Local loss @ local epoch 1: 0.056724708527326584
Local loss @ local epoch 2: 0.05250178650021553
Local loss @ local epoch 3: 0.05057299882173538
Local loss @ local epoch 4: 0.04841110482811928
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.25 seconds!
[tester] 
SST2Metric: acc=0.8084862385321101, hinge=0.9271174324761837, ce=0.4605912917011648
Local test acc @ epoch 23: 0.8085
Client 0 execute local training on 8 samples...
Local loss @ local epoch 0: 0.1594943106174469
Local loss @ local epoch 1: 0.12065199017524719
Local loss @ local epoch 2: 0.10005851089954376
Local loss @ local epoch 3: 0.09239555895328522
Local loss @ local epoch 4: 0.09153604507446289
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.15 seconds!
[tester] 
SST2Metric: acc=0.8463302752293578, hinge=0.7103102343891738, ce=0.3346504208828331
Local test acc @ epoch 23: 0.8463
Client 2 execute local training on 8 samples...
Local loss @ local epoch 0: 0.25809329748153687
Local loss @ local epoch 1: 0.18814344704151154
Local loss @ local epoch 2: 0.14273793995380402
Local loss @ local epoch 3: 0.1169741228222847
Local loss @ local epoch 4: 0.10529758036136627
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.14 seconds!
[tester] 
SST2Metric: acc=0.8004587155963303, hinge=0.9611565580061816, ce=0.4854835142902688
Local test acc @ epoch 23: 0.8005
Client 8 execute local training on 8 samples...
Local loss @ local epoch 0: 0.09793417900800705
Local loss @ local epoch 1: 0.08629363775253296
Local loss @ local epoch 2: 0.08073895424604416
Local loss @ local epoch 3: 0.07557699084281921
Local loss @ local epoch 4: 0.06998360902070999
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.12 seconds!
[tester] 
SST2Metric: acc=0.838302752293578, hinge=0.7844412590658992, ce=0.38413730077445507
Local test acc @ epoch 23: 0.8383
Client 6 execute local training on 8 samples...
Local loss @ local epoch 0: 0.35262417793273926
Local loss @ local epoch 1: 0.2875419855117798
Local loss @ local epoch 2: 0.24165920913219452
Local loss @ local epoch 3: 0.21658077836036682
Local loss @ local epoch 4: 0.20915786921977997
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.11 seconds!
[tester] 
SST2Metric: acc=0.8371559633027523, hinge=0.7580547906936855, ce=0.36236488648237436
Local test acc @ epoch 23: 0.8372
Client 4 execute local training on 8 samples...
Local loss @ local epoch 0: 0.23011770844459534
Local loss @ local epoch 1: 0.19094741344451904
Local loss @ local epoch 2: 0.1695568859577179
Local loss @ local epoch 3: 0.15811152756214142
Local loss @ local epoch 4: 0.14943084120750427
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.13 seconds!
[tester] 
SST2Metric: acc=0.8520642201834863, hinge=0.7697966012659423, ce=0.3702519034642145
Local test acc @ epoch 23: 0.8521
Client 9 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0970565527677536
Local loss @ local epoch 1: 0.08785531669855118
Local loss @ local epoch 2: 0.08194875717163086
Local loss @ local epoch 3: 0.07566189020872116
Local loss @ local epoch 4: 0.06960412114858627
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.13 seconds!
[tester] 
SST2Metric: acc=0.8577981651376146, hinge=0.7293878330276646, ce=0.3510927240355709
Local test acc @ epoch 23: 0.8578
Client 7 execute local training on 8 samples...
Local loss @ local epoch 0: 0.152311772108078
Local loss @ local epoch 1: 0.1261889487504959
Local loss @ local epoch 2: 0.11746309697628021
Local loss @ local epoch 3: 0.11695457249879837
Local loss @ local epoch 4: 0.11656007915735245
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.12 seconds!
[tester] 
SST2Metric: acc=0.8360091743119266, hinge=0.7743730144489795, ce=0.3754554549146683
Local test acc @ epoch 23: 0.836
Client 3 execute local training on 8 samples...
Local loss @ local epoch 0: 0.1947827935218811
Local loss @ local epoch 1: 0.18900316953659058
Local loss @ local epoch 2: 0.18307410180568695
Local loss @ local epoch 3: 0.17769454419612885
Local loss @ local epoch 4: 0.1725621074438095
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.17 seconds!
[tester] 
SST2Metric: acc=0.8405963302752294, hinge=0.7495564931849821, ce=0.35985564924093016
Local test acc @ epoch 23: 0.8406
Client 5 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0742725357413292
Local loss @ local epoch 1: 0.07071282714605331
Local loss @ local epoch 2: 0.06679399311542511
Local loss @ local epoch 3: 0.06282326579093933
Local loss @ local epoch 4: 0.059673797339200974
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.17 seconds!
[tester] 
SST2Metric: acc=0.8394495412844036, hinge=0.7343026714040599, ce=0.3530587837646861
Local test acc @ epoch 23: 0.8394
Global evaluate on test data...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.73 seconds!
[tester] 
SST2Metric: acc=0.7981651376146789, hinge=0.9420584557800118, ce=0.41794381502571454
Global test acc @ epoch 23: 0.7982
Global epoch 24...
Client 3 execute local training on 8 samples...
Local loss @ local epoch 0: 0.21217001974582672
Local loss @ local epoch 1: 0.18998222053050995
Local loss @ local epoch 2: 0.1830713301897049
Local loss @ local epoch 3: 0.18378688395023346
Local loss @ local epoch 4: 0.18369746208190918
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.11 seconds!
[tester] 
SST2Metric: acc=0.8268348623853211, hinge=0.8012567356638952, ce=0.38667253358716813
Local test acc @ epoch 24: 0.8268
Client 0 execute local training on 8 samples...
Local loss @ local epoch 0: 0.1137949675321579
Local loss @ local epoch 1: 0.09097829461097717
Local loss @ local epoch 2: 0.07859610766172409
Local loss @ local epoch 3: 0.07543736696243286
Local loss @ local epoch 4: 0.07733093947172165
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.1 seconds!
[tester] 
SST2Metric: acc=0.8486238532110092, hinge=0.7257790069241042, ce=0.34685425973847644
Local test acc @ epoch 24: 0.8486
Client 7 execute local training on 8 samples...
Local loss @ local epoch 0: 0.14765189588069916
Local loss @ local epoch 1: 0.12010495364665985
Local loss @ local epoch 2: 0.11008532345294952
Local loss @ local epoch 3: 0.10984277725219727
Local loss @ local epoch 4: 0.11110753566026688
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.2 seconds!
[tester] 
SST2Metric: acc=0.823394495412844, hinge=0.8262599508423324, ce=0.4031053454093977
Local test acc @ epoch 24: 0.8234
Client 1 execute local training on 8 samples...
Local loss @ local epoch 0: 0.16470064222812653
Local loss @ local epoch 1: 0.11761986464262009
Local loss @ local epoch 2: 0.08594498038291931
Local loss @ local epoch 3: 0.06668777763843536
Local loss @ local epoch 4: 0.05664362013339996
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.18 seconds!
[tester] 
SST2Metric: acc=0.8440366972477065, hinge=0.7687113584455, ce=0.37560470978122784
Local test acc @ epoch 24: 0.844
Client 6 execute local training on 8 samples...
Local loss @ local epoch 0: 0.32351452112197876
Local loss @ local epoch 1: 0.2642473578453064
Local loss @ local epoch 2: 0.22498109936714172
Local loss @ local epoch 3: 0.2061181217432022
Local loss @ local epoch 4: 0.20245732367038727
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.16 seconds!
[tester] 
SST2Metric: acc=0.8497706422018348, hinge=0.7328508090262019, ce=0.3495005021777448
Local test acc @ epoch 24: 0.8498
Client 2 execute local training on 8 samples...
Local loss @ local epoch 0: 0.3108229637145996
Local loss @ local epoch 1: 0.22419248521327972
Local loss @ local epoch 2: 0.16446936130523682
Local loss @ local epoch 3: 0.12709257006645203
Local loss @ local epoch 4: 0.10668303072452545
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.13 seconds!
[tester] 
SST2Metric: acc=0.8176605504587156, hinge=0.8652814988969663, ce=0.43147566025957057
Local test acc @ epoch 24: 0.8177
Client 9 execute local training on 8 samples...
Local loss @ local epoch 0: 0.10242069512605667
Local loss @ local epoch 1: 0.0804005116224289
Local loss @ local epoch 2: 0.07174036651849747
Local loss @ local epoch 3: 0.07048729807138443
Local loss @ local epoch 4: 0.07034239917993546
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.04 seconds!
[tester] 
SST2Metric: acc=0.8497706422018348, hinge=0.708669888453746, ce=0.33913992298339757
Local test acc @ epoch 24: 0.8498
Client 8 execute local training on 8 samples...
Local loss @ local epoch 0: 0.09724436700344086
Local loss @ local epoch 1: 0.07559823989868164
Local loss @ local epoch 2: 0.0662173330783844
Local loss @ local epoch 3: 0.06412052363157272
Local loss @ local epoch 4: 0.06414064019918442
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.12 seconds!
[tester] 
SST2Metric: acc=0.8256880733944955, hinge=0.8349493408421857, ce=0.42415461463665743
Local test acc @ epoch 24: 0.8257
Client 4 execute local training on 8 samples...
Local loss @ local epoch 0: 0.3955400586128235
Local loss @ local epoch 1: 0.3170761466026306
Local loss @ local epoch 2: 0.26247063279151917
Local loss @ local epoch 3: 0.22906386852264404
Local loss @ local epoch 4: 0.21048790216445923
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.16 seconds!
[tester] 
SST2Metric: acc=0.8451834862385321, hinge=0.7106018104684462, ce=0.3404449929970257
Local test acc @ epoch 24: 0.8452
Client 5 execute local training on 8 samples...
Local loss @ local epoch 0: 0.10458801686763763
Local loss @ local epoch 1: 0.08161452412605286
Local loss @ local epoch 2: 0.0734218955039978
Local loss @ local epoch 3: 0.07360608130693436
Local loss @ local epoch 4: 0.07501303404569626
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.18 seconds!
[tester] 
SST2Metric: acc=0.8153669724770642, hinge=0.8746543626446243, ce=0.4350072151613892
Local test acc @ epoch 24: 0.8154
Global evaluate on test data...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.58 seconds!
[tester] 
SST2Metric: acc=0.7947247706422018, hinge=0.948878597092191, ce=0.423996868654402
Global test acc @ epoch 24: 0.7947
Global epoch 25...
Client 0 execute local training on 8 samples...
Local loss @ local epoch 0: 0.07250571995973587
Local loss @ local epoch 1: 0.0686548501253128
Local loss @ local epoch 2: 0.0653037577867508
Local loss @ local epoch 3: 0.062316667288541794
Local loss @ local epoch 4: 0.05957363545894623
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.16 seconds!
[tester] 
SST2Metric: acc=0.8451834862385321, hinge=0.7453921735286713, ce=0.36088629748774775
Local test acc @ epoch 25: 0.8452
Client 5 execute local training on 8 samples...
Local loss @ local epoch 0: 0.09518899023532867
Local loss @ local epoch 1: 0.07758662849664688
Local loss @ local epoch 2: 0.07282214611768723
Local loss @ local epoch 3: 0.07320921868085861
Local loss @ local epoch 4: 0.07240115106105804
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.17 seconds!
[tester] 
SST2Metric: acc=0.8130733944954128, hinge=0.8940954722395731, ce=0.4468550383531993
Local test acc @ epoch 25: 0.8131
Client 8 execute local training on 8 samples...
Local loss @ local epoch 0: 0.22589807212352753
Local loss @ local epoch 1: 0.15849825739860535
Local loss @ local epoch 2: 0.1123986542224884
Local loss @ local epoch 3: 0.08347161114215851
Local loss @ local epoch 4: 0.06736429780721664
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.11 seconds!
[tester] 
SST2Metric: acc=0.841743119266055, hinge=0.7437090544252221, ce=0.3628715270566284
Local test acc @ epoch 25: 0.8417
Client 4 execute local training on 8 samples...
Local loss @ local epoch 0: 0.31167614459991455
Local loss @ local epoch 1: 0.271129846572876
Local loss @ local epoch 2: 0.24139750003814697
Local loss @ local epoch 3: 0.21687102317810059
Local loss @ local epoch 4: 0.19488169252872467
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.09 seconds!
[tester] 
SST2Metric: acc=0.8440366972477065, hinge=0.7079301149746694, ce=0.3388607564356622
Local test acc @ epoch 25: 0.844
Client 3 execute local training on 8 samples...
Local loss @ local epoch 0: 0.18910738825798035
Local loss @ local epoch 1: 0.16999132931232452
Local loss @ local epoch 2: 0.16521108150482178
Local loss @ local epoch 3: 0.16641773283481598
Local loss @ local epoch 4: 0.16558903455734253
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.09 seconds!
[tester] 
SST2Metric: acc=0.8256880733944955, hinge=0.8059541778553516, ce=0.39249220181154004
Local test acc @ epoch 25: 0.8257
Client 1 execute local training on 8 samples...
Local loss @ local epoch 0: 0.1533777117729187
Local loss @ local epoch 1: 0.10962861031293869
Local loss @ local epoch 2: 0.07922642678022385
Local loss @ local epoch 3: 0.05989901348948479
Local loss @ local epoch 4: 0.049124546349048615
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.19 seconds!
[tester] 
SST2Metric: acc=0.8474770642201835, hinge=0.7854306209798253, ce=0.38463929006744413
Local test acc @ epoch 25: 0.8475
Client 2 execute local training on 8 samples...
Local loss @ local epoch 0: 0.10388321429491043
Local loss @ local epoch 1: 0.09074457734823227
Local loss @ local epoch 2: 0.08634547144174576
Local loss @ local epoch 3: 0.08354835212230682
Local loss @ local epoch 4: 0.079774871468544
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.01 seconds!
[tester] 
SST2Metric: acc=0.7614678899082569, hinge=1.1064044304123712, ce=0.5828141479949476
Local test acc @ epoch 25: 0.7615
Client 7 execute local training on 8 samples...
Local loss @ local epoch 0: 0.38844946026802063
Local loss @ local epoch 1: 0.2833930253982544
Local loss @ local epoch 2: 0.20970650017261505
Local loss @ local epoch 3: 0.16332760453224182
Local loss @ local epoch 4: 0.13842761516571045
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.14 seconds!
[tester] 
SST2Metric: acc=0.8371559633027523, hinge=0.7138702090180248, ce=0.3408974582209773
Local test acc @ epoch 25: 0.8372
Client 9 execute local training on 8 samples...
Local loss @ local epoch 0: 0.09323909878730774
Local loss @ local epoch 1: 0.08396381139755249
Local loss @ local epoch 2: 0.07911111414432526
Local loss @ local epoch 3: 0.07358795404434204
Local loss @ local epoch 4: 0.06781558692455292
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.13 seconds!
[tester] 
SST2Metric: acc=0.8555045871559633, hinge=0.7293224959471903, ce=0.3539561869901255
Local test acc @ epoch 25: 0.8555
Client 6 execute local training on 8 samples...
Local loss @ local epoch 0: 0.2629907429218292
Local loss @ local epoch 1: 0.21941407024860382
Local loss @ local epoch 2: 0.19649700820446014
Local loss @ local epoch 3: 0.18993598222732544
Local loss @ local epoch 4: 0.1894519031047821
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.25 seconds!
[tester] 
SST2Metric: acc=0.8486238532110092, hinge=0.7309268455986583, ce=0.352200864993651
Local test acc @ epoch 25: 0.8486
Global evaluate on test data...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.94 seconds!
[tester] 
SST2Metric: acc=0.7958715596330275, hinge=0.9387824159423146, ce=0.4203428707401687
Global test acc @ epoch 25: 0.7959
Global epoch 26...
Client 4 execute local training on 8 samples...
Local loss @ local epoch 0: 0.1816963106393814
Local loss @ local epoch 1: 0.1604139804840088
Local loss @ local epoch 2: 0.14304064214229584
Local loss @ local epoch 3: 0.12796738743782043
Local loss @ local epoch 4: 0.11488564312458038
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.19 seconds!
[tester] 
SST2Metric: acc=0.8509174311926605, hinge=0.7462640911887545, ce=0.35914285742429963
Local test acc @ epoch 26: 0.8509
Client 5 execute local training on 8 samples...
Local loss @ local epoch 0: 0.13511499762535095
Local loss @ local epoch 1: 0.09538577497005463
Local loss @ local epoch 2: 0.07363825291395187
Local loss @ local epoch 3: 0.06515420973300934
Local loss @ local epoch 4: 0.0650000050663948
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.13 seconds!
[tester] 
SST2Metric: acc=0.8268348623853211, hinge=0.8229489911586867, ce=0.4033245232913199
Local test acc @ epoch 26: 0.8268
Client 9 execute local training on 8 samples...
Local loss @ local epoch 0: 0.21372510492801666
Local loss @ local epoch 1: 0.15421833097934723
Local loss @ local epoch 2: 0.11534925550222397
Local loss @ local epoch 3: 0.0937030017375946
Local loss @ local epoch 4: 0.08484376966953278
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.09 seconds!
[tester] 
SST2Metric: acc=0.8543577981651376, hinge=0.747024815290346, ce=0.36153967269277626
Local test acc @ epoch 26: 0.8544
Client 1 execute local training on 8 samples...
Local loss @ local epoch 0: 0.04688054695725441
Local loss @ local epoch 1: 0.039480432868003845
Local loss @ local epoch 2: 0.03703887760639191
Local loss @ local epoch 3: 0.036045726388692856
Local loss @ local epoch 4: 0.03458622470498085
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.18 seconds!
[tester] 
SST2Metric: acc=0.805045871559633, hinge=0.9532329966442301, ce=0.48309933553116585
Local test acc @ epoch 26: 0.805
Client 8 execute local training on 8 samples...
Local loss @ local epoch 0: 0.10535931587219238
Local loss @ local epoch 1: 0.09317460656166077
Local loss @ local epoch 2: 0.08328263461589813
Local loss @ local epoch 3: 0.07495090365409851
Local loss @ local epoch 4: 0.06793009489774704
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.11 seconds!
[tester] 
SST2Metric: acc=0.8222477064220184, hinge=0.8552094406764442, ce=0.433458050781297
Local test acc @ epoch 26: 0.8222
Client 0 execute local training on 8 samples...
Local loss @ local epoch 0: 0.11552131175994873
Local loss @ local epoch 1: 0.08427447825670242
Local loss @ local epoch 2: 0.06747476011514664
Local loss @ local epoch 3: 0.061063580214977264
Local loss @ local epoch 4: 0.06073952093720436
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.14 seconds!
[tester] 
SST2Metric: acc=0.8474770642201835, hinge=0.7242123110031863, ce=0.3514039734839846
Local test acc @ epoch 26: 0.8475
Client 2 execute local training on 8 samples...
Local loss @ local epoch 0: 0.20819425582885742
Local loss @ local epoch 1: 0.15103037655353546
Local loss @ local epoch 2: 0.11532459408044815
Local loss @ local epoch 3: 0.09611575305461884
Local loss @ local epoch 4: 0.08815522491931915
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.09 seconds!
[tester] 
SST2Metric: acc=0.8142201834862385, hinge=0.8802295694930837, ce=0.4499861283672102
Local test acc @ epoch 26: 0.8142
Client 7 execute local training on 8 samples...
Local loss @ local epoch 0: 0.2760674059391022
Local loss @ local epoch 1: 0.19973547756671906
Local loss @ local epoch 2: 0.1501205414533615
Local loss @ local epoch 3: 0.1222105473279953
Local loss @ local epoch 4: 0.11030714958906174
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.12 seconds!
[tester] 
SST2Metric: acc=0.8497706422018348, hinge=0.7282037192254985, ce=0.35389511065062035
Local test acc @ epoch 26: 0.8498
Client 6 execute local training on 8 samples...
Local loss @ local epoch 0: 0.20159269869327545
Local loss @ local epoch 1: 0.18810412287712097
Local loss @ local epoch 2: 0.17547598481178284
Local loss @ local epoch 3: 0.1636446863412857
Local loss @ local epoch 4: 0.15259160101413727
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.24 seconds!
[tester] 
SST2Metric: acc=0.841743119266055, hinge=0.7130536785639754, ce=0.34574066086270666
Local test acc @ epoch 26: 0.8417
Client 3 execute local training on 8 samples...
Local loss @ local epoch 0: 0.19889670610427856
Local loss @ local epoch 1: 0.1868731826543808
Local loss @ local epoch 2: 0.17833484709262848
Local loss @ local epoch 3: 0.17102473974227905
Local loss @ local epoch 4: 0.16498424112796783
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.15 seconds!
[tester] 
SST2Metric: acc=0.8428899082568807, hinge=0.7332390150345793, ce=0.3565576733047262
Local test acc @ epoch 26: 0.8429
Global evaluate on test data...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.75 seconds!
[tester] 
SST2Metric: acc=0.805045871559633, hinge=0.9222220459935861, ce=0.4119861776675653
Global test acc @ epoch 26: 0.805
Global epoch 27...
Client 0 execute local training on 8 samples...
Local loss @ local epoch 0: 0.06720314919948578
Local loss @ local epoch 1: 0.06174985319375992
Local loss @ local epoch 2: 0.05752347409725189
Local loss @ local epoch 3: 0.054228153079748154
Local loss @ local epoch 4: 0.051645245403051376
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.06 seconds!
[tester] 
SST2Metric: acc=0.8474770642201835, hinge=0.7193151285888952, ce=0.34880010885348833
Local test acc @ epoch 27: 0.8475
Client 5 execute local training on 8 samples...
Local loss @ local epoch 0: 0.08549916744232178
Local loss @ local epoch 1: 0.06494779884815216
Local loss @ local epoch 2: 0.05694032832980156
Local loss @ local epoch 3: 0.05664432793855667
Local loss @ local epoch 4: 0.05843890458345413
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.17 seconds!
[tester] 
SST2Metric: acc=0.8176605504587156, hinge=0.89024399576384, ce=0.4504727335868899
Local test acc @ epoch 27: 0.8177
Client 9 execute local training on 8 samples...
Local loss @ local epoch 0: 0.19552361965179443
Local loss @ local epoch 1: 0.13462720811367035
Local loss @ local epoch 2: 0.09519844502210617
Local loss @ local epoch 3: 0.07240206748247147
Local loss @ local epoch 4: 0.06162318214774132
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.08 seconds!
[tester] 
SST2Metric: acc=0.8520642201834863, hinge=0.7269529578609204, ce=0.35599141114246136
Local test acc @ epoch 27: 0.8521
Client 1 execute local training on 8 samples...
Local loss @ local epoch 0: 0.05192861333489418
Local loss @ local epoch 1: 0.04563072323799133
Local loss @ local epoch 2: 0.04225115478038788
Local loss @ local epoch 3: 0.03918147832155228
Local loss @ local epoch 4: 0.03597133979201317
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.12 seconds!
[tester] 
SST2Metric: acc=0.8337155963302753, hinge=0.8272498534360063, ce=0.41660019723749764
Local test acc @ epoch 27: 0.8337
Client 3 execute local training on 8 samples...
Local loss @ local epoch 0: 0.28901153802871704
Local loss @ local epoch 1: 0.2321222573518753
Local loss @ local epoch 2: 0.1938328593969345
Local loss @ local epoch 3: 0.17245200276374817
Local loss @ local epoch 4: 0.16472826898097992
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.14 seconds!
[tester] 
SST2Metric: acc=0.841743119266055, hinge=0.7446202520930439, ce=0.36241052764544796
Local test acc @ epoch 27: 0.8417
Client 2 execute local training on 8 samples...
Local loss @ local epoch 0: 0.31729578971862793
Local loss @ local epoch 1: 0.22595977783203125
Local loss @ local epoch 2: 0.16152916848659515
Local loss @ local epoch 3: 0.11936401575803757
Local loss @ local epoch 4: 0.09420609474182129
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.13 seconds!
[tester] 
SST2Metric: acc=0.8314220183486238, hinge=0.8289959801172991, ce=0.41783930063076796
Local test acc @ epoch 27: 0.8314
Client 6 execute local training on 8 samples...
Local loss @ local epoch 0: 0.31311577558517456
Local loss @ local epoch 1: 0.2486564815044403
Local loss @ local epoch 2: 0.2023930698633194
Local loss @ local epoch 3: 0.17562364041805267
Local loss @ local epoch 4: 0.1661725640296936
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.05 seconds!
[tester] 
SST2Metric: acc=0.8520642201834863, hinge=0.717861993733896, ce=0.34724333256893203
Local test acc @ epoch 27: 0.8521
Client 8 execute local training on 8 samples...
Local loss @ local epoch 0: 0.23084008693695068
Local loss @ local epoch 1: 0.16477224230766296
Local loss @ local epoch 2: 0.11912639439105988
Local loss @ local epoch 3: 0.09052016586065292
Local loss @ local epoch 4: 0.07493143528699875
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.19 seconds!
[tester] 
SST2Metric: acc=0.8256880733944955, hinge=0.8536679848618464, ce=0.43342101453470255
Local test acc @ epoch 27: 0.8257
Client 4 execute local training on 8 samples...
Local loss @ local epoch 0: 0.20122882723808289
Local loss @ local epoch 1: 0.1575453132390976
Local loss @ local epoch 2: 0.13334645330905914
Local loss @ local epoch 3: 0.12240272015333176
Local loss @ local epoch 4: 0.11737299710512161
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.09 seconds!
[tester] 
SST2Metric: acc=0.8405963302752294, hinge=0.7086309927319168, ce=0.34382610934692515
Local test acc @ epoch 27: 0.8406
Client 7 execute local training on 8 samples...
Local loss @ local epoch 0: 0.14043423533439636
Local loss @ local epoch 1: 0.12860767543315887
Local loss @ local epoch 2: 0.11964196711778641
Local loss @ local epoch 3: 0.11124859750270844
Local loss @ local epoch 4: 0.10380886495113373
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.09 seconds!
[tester] 
SST2Metric: acc=0.8474770642201835, hinge=0.7016657599068563, ce=0.34100760689535
Local test acc @ epoch 27: 0.8475
Global evaluate on test data...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.54 seconds!
[tester] 
SST2Metric: acc=0.8038990825688074, hinge=0.9221464977898729, ce=0.41624057713315027
Global test acc @ epoch 27: 0.8039
Global epoch 28...
Client 4 execute local training on 8 samples...
Local loss @ local epoch 0: 0.15237216651439667
Local loss @ local epoch 1: 0.13493944704532623
Local loss @ local epoch 2: 0.1206960678100586
Local loss @ local epoch 3: 0.10830064862966537
Local loss @ local epoch 4: 0.09756430983543396
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.1 seconds!
[tester] 
SST2Metric: acc=0.8520642201834863, hinge=0.7484872015791202, ce=0.3655507425957043
Local test acc @ epoch 28: 0.8521
Client 1 execute local training on 8 samples...
Local loss @ local epoch 0: 0.03585788607597351
Local loss @ local epoch 1: 0.028678810223937035
Local loss @ local epoch 2: 0.026215706020593643
Local loss @ local epoch 3: 0.026002921164035797
Local loss @ local epoch 4: 0.025879668071866035
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.08 seconds!
[tester] 
SST2Metric: acc=0.7844036697247706, hinge=0.9976510818398326, ce=0.5133370936959298
Local test acc @ epoch 28: 0.7844
Client 9 execute local training on 8 samples...
Local loss @ local epoch 0: 0.1807302087545395
Local loss @ local epoch 1: 0.13534973561763763
Local loss @ local epoch 2: 0.10778045654296875
Local loss @ local epoch 3: 0.09477302432060242
Local loss @ local epoch 4: 0.09081912785768509
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.11 seconds!
[tester] 
SST2Metric: acc=0.8394495412844036, hinge=0.7056578143474159, ce=0.34302801345774064
Local test acc @ epoch 28: 0.8394
Client 3 execute local training on 8 samples...
Local loss @ local epoch 0: 0.18736359477043152
Local loss @ local epoch 1: 0.17678187787532806
Local loss @ local epoch 2: 0.1686820387840271
Local loss @ local epoch 3: 0.16189193725585938
Local loss @ local epoch 4: 0.15640968084335327
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.05 seconds!
[tester] 
SST2Metric: acc=0.8451834862385321, hinge=0.7284143158328642, ce=0.3555266238458523
Local test acc @ epoch 28: 0.8452
Client 8 execute local training on 8 samples...
Local loss @ local epoch 0: 0.17215943336486816
Local loss @ local epoch 1: 0.12142935395240784
Local loss @ local epoch 2: 0.08851565420627594
Local loss @ local epoch 3: 0.0695803239941597
Local loss @ local epoch 4: 0.060739707201719284
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.13 seconds!
[tester] 
SST2Metric: acc=0.8360091743119266, hinge=0.8042551940734234, ce=0.4075598685976562
Local test acc @ epoch 28: 0.836
Client 7 execute local training on 8 samples...
Local loss @ local epoch 0: 0.21003395318984985
Local loss @ local epoch 1: 0.1532888114452362
Local loss @ local epoch 2: 0.11896848678588867
Local loss @ local epoch 3: 0.10217736661434174
Local loss @ local epoch 4: 0.09750493615865707
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.08 seconds!
[tester] 
SST2Metric: acc=0.8405963302752294, hinge=0.7483102851504579, ce=0.36881593655829037
Local test acc @ epoch 28: 0.8406
Client 6 execute local training on 8 samples...
Local loss @ local epoch 0: 0.21157506108283997
Local loss @ local epoch 1: 0.1979365050792694
Local loss @ local epoch 2: 0.1854773610830307
Local loss @ local epoch 3: 0.17346090078353882
Local loss @ local epoch 4: 0.16228805482387543
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.21 seconds!
[tester] 
SST2Metric: acc=0.841743119266055, hinge=0.699881334780553, ce=0.3409832579536586
Local test acc @ epoch 28: 0.8417
Client 2 execute local training on 8 samples...
Local loss @ local epoch 0: 0.2298944890499115
Local loss @ local epoch 1: 0.16211199760437012
Local loss @ local epoch 2: 0.11679194122552872
Local loss @ local epoch 3: 0.08905786275863647
Local loss @ local epoch 4: 0.07419294863939285
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.04 seconds!
[tester] 
SST2Metric: acc=0.8084862385321101, hinge=0.9581900621772906, ce=0.4989580656786304
Local test acc @ epoch 28: 0.8085
Client 5 execute local training on 8 samples...
Local loss @ local epoch 0: 0.33072227239608765
Local loss @ local epoch 1: 0.22873647511005402
Local loss @ local epoch 2: 0.15604327619075775
Local loss @ local epoch 3: 0.10737233608961105
Local loss @ local epoch 4: 0.07672976702451706
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.18 seconds!
[tester] 
SST2Metric: acc=0.8474770642201835, hinge=0.7009824260112343, ce=0.3414801623535539
Local test acc @ epoch 28: 0.8475
Client 0 execute local training on 8 samples...
Local loss @ local epoch 0: 0.06424245983362198
Local loss @ local epoch 1: 0.05486683174967766
Local loss @ local epoch 2: 0.05220377445220947
Local loss @ local epoch 3: 0.05168969929218292
Local loss @ local epoch 4: 0.05028504133224487
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.13 seconds!
[tester] 
SST2Metric: acc=0.8520642201834863, hinge=0.7173219164577099, ce=0.3534277599612507
Local test acc @ epoch 28: 0.8521
Global evaluate on test data...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.62 seconds!
[tester] 
SST2Metric: acc=0.8004587155963303, hinge=0.9178055255511485, ce=0.4160072844167915
Global test acc @ epoch 28: 0.8005
Global epoch 29...
Client 0 execute local training on 8 samples...
Local loss @ local epoch 0: 0.05712708830833435
Local loss @ local epoch 1: 0.05279197171330452
Local loss @ local epoch 2: 0.04938087239861488
Local loss @ local epoch 3: 0.04668813198804855
Local loss @ local epoch 4: 0.04453422129154205
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.08 seconds!
[tester] 
SST2Metric: acc=0.8497706422018348, hinge=0.7162375849321347, ce=0.3501120910065573
Local test acc @ epoch 29: 0.8498
Client 2 execute local training on 8 samples...
Local loss @ local epoch 0: 0.140365868806839
Local loss @ local epoch 1: 0.10714210569858551
Local loss @ local epoch 2: 0.08960527181625366
Local loss @ local epoch 3: 0.08238939940929413
Local loss @ local epoch 4: 0.08012865483760834
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.13 seconds!
[tester] 
SST2Metric: acc=0.8119266055045872, hinge=0.9248190954190876, ce=0.48254943922708887
Local test acc @ epoch 29: 0.8119
Client 9 execute local training on 8 samples...
Local loss @ local epoch 0: 0.10993047803640366
Local loss @ local epoch 1: 0.07948015630245209
Local loss @ local epoch 2: 0.06291831284761429
Local loss @ local epoch 3: 0.05694306269288063
Local loss @ local epoch 4: 0.057196442037820816
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.06 seconds!
[tester] 
SST2Metric: acc=0.8474770642201835, hinge=0.7216563531018179, ce=0.3545054875126262
Local test acc @ epoch 29: 0.8475
Client 1 execute local training on 8 samples...
Local loss @ local epoch 0: 0.04949039965867996
Local loss @ local epoch 1: 0.04046637564897537
Local loss @ local epoch 2: 0.036650076508522034
Local loss @ local epoch 3: 0.03472086787223816
Local loss @ local epoch 4: 0.032828446477651596
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.06 seconds!
[tester] 
SST2Metric: acc=0.8325688073394495, hinge=0.826151183849081, ce=0.4212024276392586
Local test acc @ epoch 29: 0.8326
Client 6 execute local training on 8 samples...
Local loss @ local epoch 0: 0.3367271423339844
Local loss @ local epoch 1: 0.26703059673309326
Local loss @ local epoch 2: 0.2144642025232315
Local loss @ local epoch 3: 0.18098551034927368
Local loss @ local epoch 4: 0.16582055389881134
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.1 seconds!
[tester] 
SST2Metric: acc=0.8543577981651376, hinge=0.721916577548062, ce=0.3534520164713127
Local test acc @ epoch 29: 0.8544
Client 8 execute local training on 8 samples...
Local loss @ local epoch 0: 0.2060730755329132
Local loss @ local epoch 1: 0.14559869468212128
Local loss @ local epoch 2: 0.10424673557281494
Local loss @ local epoch 3: 0.07845131307840347
Local loss @ local epoch 4: 0.06433135271072388
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.13 seconds!
[tester] 
SST2Metric: acc=0.8314220183486238, hinge=0.8355705970471058, ce=0.42767355020303244
Local test acc @ epoch 29: 0.8314
Client 4 execute local training on 8 samples...
Local loss @ local epoch 0: 0.19975215196609497
Local loss @ local epoch 1: 0.1571754813194275
Local loss @ local epoch 2: 0.13373729586601257
Local loss @ local epoch 3: 0.12301602214574814
Local loss @ local epoch 4: 0.11770294606685638
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.14 seconds!
[tester] 
SST2Metric: acc=0.841743119266055, hinge=0.7082468589784903, ce=0.34631356656756423
Local test acc @ epoch 29: 0.8417
Client 5 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0849621444940567
Local loss @ local epoch 1: 0.062235746532678604
Local loss @ local epoch 2: 0.0520939826965332
Local loss @ local epoch 3: 0.05031122267246246
Local loss @ local epoch 4: 0.052159857004880905
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.14 seconds!
[tester] 
SST2Metric: acc=0.8245412844036697, hinge=0.8529256690259374, ce=0.43247810619618365
Local test acc @ epoch 29: 0.8245
Client 7 execute local training on 8 samples...
Local loss @ local epoch 0: 0.222349613904953
Local loss @ local epoch 1: 0.17182238399982452
Local loss @ local epoch 2: 0.13598403334617615
Local loss @ local epoch 3: 0.11470018327236176
Local loss @ local epoch 4: 0.1059747189283371
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.14 seconds!
[tester] 
SST2Metric: acc=0.8463302752293578, hinge=0.7028691422775251, ce=0.3452344981680086
Local test acc @ epoch 29: 0.8463
Client 3 execute local training on 8 samples...
Local loss @ local epoch 0: 0.18383029103279114
Local loss @ local epoch 1: 0.16264455020427704
Local loss @ local epoch 2: 0.1556524783372879
Local loss @ local epoch 3: 0.15642404556274414
Local loss @ local epoch 4: 0.15696345269680023
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.02 seconds!
[tester] 
SST2Metric: acc=0.8348623853211009, hinge=0.786812901770303, ce=0.39288024286474654
Local test acc @ epoch 29: 0.8349
Global evaluate on test data...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.49 seconds!
[tester] 
SST2Metric: acc=0.8004587155963303, hinge=0.9234779608359031, ce=0.4222416223941046
Global test acc @ epoch 29: 0.8005
Global epoch 30...
Client 4 execute local training on 8 samples...
Local loss @ local epoch 0: 0.14404943585395813
Local loss @ local epoch 1: 0.12751027941703796
Local loss @ local epoch 2: 0.11409763991832733
Local loss @ local epoch 3: 0.10241122543811798
Local loss @ local epoch 4: 0.09229131042957306
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.13 seconds!
[tester] 
SST2Metric: acc=0.8520642201834863, hinge=0.7436154077359296, ce=0.36649216973829435
Local test acc @ epoch 30: 0.8521
Client 5 execute local training on 8 samples...
Local loss @ local epoch 0: 0.1181090846657753
Local loss @ local epoch 1: 0.08147208392620087
Local loss @ local epoch 2: 0.060634795576334
Local loss @ local epoch 3: 0.051330387592315674
Local loss @ local epoch 4: 0.04961686581373215
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.06 seconds!
[tester] 
SST2Metric: acc=0.8256880733944955, hinge=0.8090867397435214, ce=0.40520685017450686
Local test acc @ epoch 30: 0.8257
Client 0 execute local training on 8 samples...
Local loss @ local epoch 0: 0.13035738468170166
Local loss @ local epoch 1: 0.09853359311819077
Local loss @ local epoch 2: 0.07514022290706635
Local loss @ local epoch 3: 0.060214243829250336
Local loss @ local epoch 4: 0.05274231359362602
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.22 seconds!
[tester] 
SST2Metric: acc=0.8555045871559633, hinge=0.7268045388777321, ce=0.35844543020864694
Local test acc @ epoch 30: 0.8555
Client 2 execute local training on 8 samples...
Local loss @ local epoch 0: 0.11841286718845367
Local loss @ local epoch 1: 0.08624836802482605
Local loss @ local epoch 2: 0.0684717446565628
Local loss @ local epoch 3: 0.060989413410425186
Local loss @ local epoch 4: 0.059848297387361526
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.16 seconds!
[tester] 
SST2Metric: acc=0.7672018348623854, hinge=1.1137252994086764, ce=0.6020915848331168
Local test acc @ epoch 30: 0.7672
Client 3 execute local training on 8 samples...
Local loss @ local epoch 0: 0.4816789925098419
Local loss @ local epoch 1: 0.37964928150177
Local loss @ local epoch 2: 0.2985391616821289
Local loss @ local epoch 3: 0.2377634048461914
Local loss @ local epoch 4: 0.19564853608608246
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.26 seconds!
[tester] 
SST2Metric: acc=0.8474770642201835, hinge=0.7070609343434693, ce=0.3467105334163259
Local test acc @ epoch 30: 0.8475
Client 7 execute local training on 8 samples...
Local loss @ local epoch 0: 0.12073341012001038
Local loss @ local epoch 1: 0.11116890609264374
Local loss @ local epoch 2: 0.10322746634483337
Local loss @ local epoch 3: 0.09637470543384552
Local loss @ local epoch 4: 0.09067538380622864
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.16 seconds!
[tester] 
SST2Metric: acc=0.8520642201834863, hinge=0.7138715856665865, ce=0.35281863331008667
Local test acc @ epoch 30: 0.8521
Client 1 execute local training on 8 samples...
Local loss @ local epoch 0: 0.05792642757296562
Local loss @ local epoch 1: 0.042734917253255844
Local loss @ local epoch 2: 0.03495704382658005
Local loss @ local epoch 3: 0.03194175660610199
Local loss @ local epoch 4: 0.03114500269293785
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.1 seconds!
[tester] 
SST2Metric: acc=0.8348623853211009, hinge=0.8156868844130717, ce=0.41801602763354506
Local test acc @ epoch 30: 0.8349
Client 8 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0571080781519413
Local loss @ local epoch 1: 0.051823973655700684
Local loss @ local epoch 2: 0.04725567251443863
Local loss @ local epoch 3: 0.04327578470110893
Local loss @ local epoch 4: 0.039782583713531494
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.13 seconds!
[tester] 
SST2Metric: acc=0.8394495412844036, hinge=0.7719198964331129, ce=0.3945281373148938
Local test acc @ epoch 30: 0.8394
Client 6 execute local training on 8 samples...
Local loss @ local epoch 0: 0.4262780547142029
Local loss @ local epoch 1: 0.3462866246700287
Local loss @ local epoch 2: 0.28148311376571655
Local loss @ local epoch 3: 0.23524028062820435
Local loss @ local epoch 4: 0.20904326438903809
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.03 seconds!
[tester] 
SST2Metric: acc=0.838302752293578, hinge=0.7680466808036926, ce=0.3871507155837132
Local test acc @ epoch 30: 0.8383
Client 9 execute local training on 8 samples...
Local loss @ local epoch 0: 0.11125072091817856
Local loss @ local epoch 1: 0.07818953692913055
Local loss @ local epoch 2: 0.05987563356757164
Local loss @ local epoch 3: 0.05216260999441147
Local loss @ local epoch 4: 0.05102178826928139
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.19 seconds!
[tester] 
SST2Metric: acc=0.8463302752293578, hinge=0.75735444814787, ce=0.38609825253247393
Local test acc @ epoch 30: 0.8463
Global evaluate on test data...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.75 seconds!
[tester] 
SST2Metric: acc=0.7970183486238532, hinge=0.9260646488961823, ce=0.42853827326806315
Global test acc @ epoch 30: 0.797
Global epoch 31...
Client 8 execute local training on 8 samples...
Local loss @ local epoch 0: 0.08180462568998337
Local loss @ local epoch 1: 0.0628272294998169
Local loss @ local epoch 2: 0.054212573915719986
Local loss @ local epoch 3: 0.05185043439269066
Local loss @ local epoch 4: 0.05157535895705223
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.16 seconds!
[tester] 
SST2Metric: acc=0.8211009174311926, hinge=0.8850518739824995, ce=0.46492591613066303
Local test acc @ epoch 31: 0.8211
Client 0 execute local training on 8 samples...
Local loss @ local epoch 0: 0.1090339943766594
Local loss @ local epoch 1: 0.0753316730260849
Local loss @ local epoch 2: 0.054937854409217834
Local loss @ local epoch 3: 0.04422552138566971
Local loss @ local epoch 4: 0.040206994861364365
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.15 seconds!
[tester] 
SST2Metric: acc=0.8463302752293578, hinge=0.714593091005579, ce=0.35329883389277467
Local test acc @ epoch 31: 0.8463
Client 6 execute local training on 8 samples...
Local loss @ local epoch 0: 0.2081177681684494
Local loss @ local epoch 1: 0.18986226618289948
Local loss @ local epoch 2: 0.18276500701904297
Local loss @ local epoch 3: 0.17525392770767212
Local loss @ local epoch 4: 0.16543778777122498
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.09 seconds!
[tester] 
SST2Metric: acc=0.8543577981651376, hinge=0.7127559186395155, ce=0.35145670132831147
Local test acc @ epoch 31: 0.8544
Client 3 execute local training on 8 samples...
Local loss @ local epoch 0: 0.16331005096435547
Local loss @ local epoch 1: 0.15689530968666077
Local loss @ local epoch 2: 0.15121251344680786
Local loss @ local epoch 3: 0.14611858129501343
Local loss @ local epoch 4: 0.14142586290836334
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.09 seconds!
[tester] 
SST2Metric: acc=0.8451834862385321, hinge=0.7512051249862811, ce=0.37321481887900504
Local test acc @ epoch 31: 0.8452
Client 5 execute local training on 8 samples...
Local loss @ local epoch 0: 0.045228175818920135
Local loss @ local epoch 1: 0.04288629814982414
Local loss @ local epoch 2: 0.04116504266858101
Local loss @ local epoch 3: 0.03871216997504234
Local loss @ local epoch 4: 0.03664416819810867
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.16 seconds!
[tester] 
SST2Metric: acc=0.8451834862385321, hinge=0.7539823243924237, ce=0.37642254877767156
Local test acc @ epoch 31: 0.8452
Client 2 execute local training on 8 samples...
Local loss @ local epoch 0: 0.2808910012245178
Local loss @ local epoch 1: 0.19966185092926025
Local loss @ local epoch 2: 0.14315634965896606
Local loss @ local epoch 3: 0.10686436295509338
Local loss @ local epoch 4: 0.08579333871603012
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.07 seconds!
[tester] 
SST2Metric: acc=0.8394495412844036, hinge=0.7981180409225849, ce=0.4094484704418467
Local test acc @ epoch 31: 0.8394
Client 4 execute local training on 8 samples...
Local loss @ local epoch 0: 0.19562928378582
Local loss @ local epoch 1: 0.15483179688453674
Local loss @ local epoch 2: 0.13220545649528503
Local loss @ local epoch 3: 0.12103420495986938
Local loss @ local epoch 4: 0.11442296952009201
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.11 seconds!
[tester] 
SST2Metric: acc=0.8463302752293578, hinge=0.7008902634229135, ce=0.3458640538668687
Local test acc @ epoch 31: 0.8463
Client 9 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0762990191578865
Local loss @ local epoch 1: 0.06797131896018982
Local loss @ local epoch 2: 0.06372418254613876
Local loss @ local epoch 3: 0.05909356474876404
Local loss @ local epoch 4: 0.05421335622668266
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.15 seconds!
[tester] 
SST2Metric: acc=0.8497706422018348, hinge=0.7123076928044678, ce=0.3569385079376468
Local test acc @ epoch 31: 0.8498
Client 1 execute local training on 8 samples...
Local loss @ local epoch 0: 0.035450901836156845
Local loss @ local epoch 1: 0.030285799875855446
Local loss @ local epoch 2: 0.028101302683353424
Local loss @ local epoch 3: 0.026487860828638077
Local loss @ local epoch 4: 0.02462236024439335
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.11 seconds!
[tester] 
SST2Metric: acc=0.8314220183486238, hinge=0.8303896586555953, ce=0.4296843021544158
Local test acc @ epoch 31: 0.8314
Client 7 execute local training on 8 samples...
Local loss @ local epoch 0: 0.16802777349948883
Local loss @ local epoch 1: 0.1279931366443634
Local loss @ local epoch 2: 0.10757142305374146
Local loss @ local epoch 3: 0.10052187740802765
Local loss @ local epoch 4: 0.09993526339530945
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.12 seconds!
[tester] 
SST2Metric: acc=0.8497706422018348, hinge=0.7052994493497621, ce=0.3509677607873711
Local test acc @ epoch 31: 0.8498
Global evaluate on test data...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.79 seconds!
[tester] 
SST2Metric: acc=0.801605504587156, hinge=0.9096876973405891, ce=0.4199843416588569
Global test acc @ epoch 31: 0.8016
Global epoch 32...
Client 9 execute local training on 8 samples...
Local loss @ local epoch 0: 0.059329383075237274
Local loss @ local epoch 1: 0.05445115268230438
Local loss @ local epoch 2: 0.05056440457701683
Local loss @ local epoch 3: 0.04655008763074875
Local loss @ local epoch 4: 0.043056268244981766
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.21 seconds!
[tester] 
SST2Metric: acc=0.8532110091743119, hinge=0.716432390409872, ce=0.35981439482933336
Local test acc @ epoch 32: 0.8532
Client 8 execute local training on 8 samples...
Local loss @ local epoch 0: 0.051893360912799835
Local loss @ local epoch 1: 0.04514680430293083
Local loss @ local epoch 2: 0.043343156576156616
Local loss @ local epoch 3: 0.041926685720682144
Local loss @ local epoch 4: 0.03961515054106712
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.06 seconds!
[tester] 
SST2Metric: acc=0.8337155963302753, hinge=0.7703791419300464, ce=0.3983813966069063
Local test acc @ epoch 32: 0.8337
Client 1 execute local training on 8 samples...
Local loss @ local epoch 0: 0.046941421926021576
Local loss @ local epoch 1: 0.04048210382461548
Local loss @ local epoch 2: 0.03704855591058731
Local loss @ local epoch 3: 0.0340808629989624
Local loss @ local epoch 4: 0.03097400814294815
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.05 seconds!
[tester] 
SST2Metric: acc=0.8555045871559633, hinge=0.7254219245199763, ce=0.36817630921242706
Local test acc @ epoch 32: 0.8555
Client 4 execute local training on 8 samples...
Local loss @ local epoch 0: 0.1700124442577362
Local loss @ local epoch 1: 0.14697737991809845
Local loss @ local epoch 2: 0.13287121057510376
Local loss @ local epoch 3: 0.12130585312843323
Local loss @ local epoch 4: 0.11031347513198853
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.08 seconds!
[tester] 
SST2Metric: acc=0.8463302752293578, hinge=0.698030312947177, ce=0.349264989602764
Local test acc @ epoch 32: 0.8463
Client 5 execute local training on 8 samples...
Local loss @ local epoch 0: 0.07296383380889893
Local loss @ local epoch 1: 0.05192827433347702
Local loss @ local epoch 2: 0.041711390018463135
Local loss @ local epoch 3: 0.03889841586351395
Local loss @ local epoch 4: 0.04004885256290436
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.14 seconds!
[tester] 
SST2Metric: acc=0.8302752293577982, hinge=0.8255497676516892, ce=0.424050178994006
Local test acc @ epoch 32: 0.8303
Client 6 execute local training on 8 samples...
Local loss @ local epoch 0: 0.2269342839717865
Local loss @ local epoch 1: 0.17851556837558746
Local loss @ local epoch 2: 0.15278014540672302
Local loss @ local epoch 3: 0.14458675682544708
Local loss @ local epoch 4: 0.1451900601387024
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.17 seconds!
[tester] 
SST2Metric: acc=0.8474770642201835, hinge=0.7010801602667625, ce=0.3529841042328797
Local test acc @ epoch 32: 0.8475
Client 0 execute local training on 8 samples...
Local loss @ local epoch 0: 0.04916023835539818
Local loss @ local epoch 1: 0.04476124793291092
Local loss @ local epoch 2: 0.04177265614271164
Local loss @ local epoch 3: 0.03909166902303696
Local loss @ local epoch 4: 0.036865610629320145
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.13 seconds!
[tester] 
SST2Metric: acc=0.8486238532110092, hinge=0.7045399248600006, ce=0.3537554726337826
Local test acc @ epoch 32: 0.8486
Client 2 execute local training on 8 samples...
Local loss @ local epoch 0: 0.11655783653259277
Local loss @ local epoch 1: 0.08677105605602264
Local loss @ local epoch 2: 0.07072556763887405
Local loss @ local epoch 3: 0.06399668753147125
Local loss @ local epoch 4: 0.06223046034574509
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.1 seconds!
[tester] 
SST2Metric: acc=0.8107798165137615, hinge=0.9564043746902309, ce=0.5172764682766358
Local test acc @ epoch 32: 0.8108
Client 3 execute local training on 8 samples...
Local loss @ local epoch 0: 0.4108739197254181
Local loss @ local epoch 1: 0.3313959538936615
Local loss @ local epoch 2: 0.2673386335372925
Local loss @ local epoch 3: 0.217723086476326
Local loss @ local epoch 4: 0.18197917938232422
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.07 seconds!
[tester] 
SST2Metric: acc=0.8486238532110092, hinge=0.7041893099699545, ce=0.3513897618774427
Local test acc @ epoch 32: 0.8486
Client 7 execute local training on 8 samples...
Local loss @ local epoch 0: 0.09514079242944717
Local loss @ local epoch 1: 0.08903175592422485
Local loss @ local epoch 2: 0.08380164206027985
Local loss @ local epoch 3: 0.0791291818022728
Local loss @ local epoch 4: 0.07521039247512817
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.14 seconds!
[tester] 
SST2Metric: acc=0.8451834862385321, hinge=0.7378813918304006, ce=0.37531891369491543
Local test acc @ epoch 32: 0.8452
Global evaluate on test data...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.62 seconds!
[tester] 
SST2Metric: acc=0.7993119266055045, hinge=0.9171601405384344, ce=0.43145937646884436
Global test acc @ epoch 32: 0.7993
Global epoch 33...
Client 9 execute local training on 8 samples...
Local loss @ local epoch 0: 0.046482495963573456
Local loss @ local epoch 1: 0.04289595037698746
Local loss @ local epoch 2: 0.039952352643013
Local loss @ local epoch 3: 0.03686879202723503
Local loss @ local epoch 4: 0.034225430339574814
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.12 seconds!
[tester] 
SST2Metric: acc=0.856651376146789, hinge=0.7188011105727712, ce=0.36641323887700333
Local test acc @ epoch 33: 0.8567
Client 4 execute local training on 8 samples...
Local loss @ local epoch 0: 0.2050710916519165
Local loss @ local epoch 1: 0.17802312970161438
Local loss @ local epoch 2: 0.15742109715938568
Local loss @ local epoch 3: 0.13981874287128448
Local loss @ local epoch 4: 0.1242736354470253
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.22 seconds!
[tester] 
SST2Metric: acc=0.8509174311926605, hinge=0.6924903400994222, ce=0.34864241968190995
Local test acc @ epoch 33: 0.8509
Client 6 execute local training on 8 samples...
Local loss @ local epoch 0: 0.17249292135238647
Local loss @ local epoch 1: 0.15127608180046082
Local loss @ local epoch 2: 0.1445406824350357
Local loss @ local epoch 3: 0.141350656747818
Local loss @ local epoch 4: 0.1355760246515274
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.08 seconds!
[tester] 
SST2Metric: acc=0.8486238532110092, hinge=0.6937124880902265, ce=0.3481286620943371
Local test acc @ epoch 33: 0.8486
Client 7 execute local training on 8 samples...
Local loss @ local epoch 0: 0.13485273718833923
Local loss @ local epoch 1: 0.11270048469305038
Local loss @ local epoch 2: 0.1029658392071724
Local loss @ local epoch 3: 0.09980180114507675
Local loss @ local epoch 4: 0.0971304327249527
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.19 seconds!
[tester] 
SST2Metric: acc=0.8555045871559633, hinge=0.7021012929601407, ce=0.35722868807118285
Local test acc @ epoch 33: 0.8555
Client 8 execute local training on 8 samples...
Local loss @ local epoch 0: 0.07399502396583557
Local loss @ local epoch 1: 0.05596259981393814
Local loss @ local epoch 2: 0.04740293323993683
Local loss @ local epoch 3: 0.04475880414247513
Local loss @ local epoch 4: 0.04445015266537666
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.11 seconds!
[tester] 
SST2Metric: acc=0.8245412844036697, hinge=0.8838349182944779, ce=0.4774235473356184
Local test acc @ epoch 33: 0.8245
Client 2 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0729619711637497
Local loss @ local epoch 1: 0.06490820646286011
Local loss @ local epoch 2: 0.06070107966661453
Local loss @ local epoch 3: 0.05710408091545105
Local loss @ local epoch 4: 0.05345046520233154
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.16 seconds!
[tester] 
SST2Metric: acc=0.8142201834862385, hinge=0.9393397170469302, ce=0.5059160988675345
Local test acc @ epoch 33: 0.8142
Client 3 execute local training on 8 samples...
Local loss @ local epoch 0: 0.36998480558395386
Local loss @ local epoch 1: 0.2946877181529999
Local loss @ local epoch 2: 0.23794938623905182
Local loss @ local epoch 3: 0.19856224954128265
Local loss @ local epoch 4: 0.17481161653995514
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.09 seconds!
[tester] 
SST2Metric: acc=0.8463302752293578, hinge=0.6942176211864577, ce=0.3486679530480381
Local test acc @ epoch 33: 0.8463
Client 1 execute local training on 8 samples...
Local loss @ local epoch 0: 0.03670124337077141
Local loss @ local epoch 1: 0.025819383561611176
Local loss @ local epoch 2: 0.019860200583934784
Local loss @ local epoch 3: 0.017263440415263176
Local loss @ local epoch 4: 0.016647130250930786
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.07 seconds!
[tester] 
SST2Metric: acc=0.8256880733944955, hinge=0.8778071113682668, ce=0.46540068406526236
Local test acc @ epoch 33: 0.8257
Client 5 execute local training on 8 samples...
Local loss @ local epoch 0: 0.23752741515636444
Local loss @ local epoch 1: 0.16108812391757965
Local loss @ local epoch 2: 0.10855355113744736
Local loss @ local epoch 3: 0.07456663250923157
Local loss @ local epoch 4: 0.05389069765806198
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.12 seconds!
[tester] 
SST2Metric: acc=0.8543577981651376, hinge=0.7002420849209532, ce=0.35234478772233385
Local test acc @ epoch 33: 0.8544
Client 0 execute local training on 8 samples...
Local loss @ local epoch 0: 0.062897689640522
Local loss @ local epoch 1: 0.049861449748277664
Local loss @ local epoch 2: 0.043708886951208115
Local loss @ local epoch 3: 0.041889965534210205
Local loss @ local epoch 4: 0.04136127606034279
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.19 seconds!
[tester] 
SST2Metric: acc=0.8532110091743119, hinge=0.7239007571148216, ce=0.3725761131494554
Local test acc @ epoch 33: 0.8532
Global evaluate on test data...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.56 seconds!
[tester] 
SST2Metric: acc=0.7993119266055045, hinge=0.9159353986519192, ce=0.4352043717757824
Global test acc @ epoch 33: 0.7993
Global epoch 34...
Client 1 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0228729248046875
Local loss @ local epoch 1: 0.019683966413140297
Local loss @ local epoch 2: 0.018415139988064766
Local loss @ local epoch 3: 0.01740323007106781
Local loss @ local epoch 4: 0.01619391329586506
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.08 seconds!
[tester] 
SST2Metric: acc=0.8302752293577982, hinge=0.8482063724360335, ce=0.44658098088731185
Local test acc @ epoch 34: 0.8303
Client 3 execute local training on 8 samples...
Local loss @ local epoch 0: 0.2794969975948334
Local loss @ local epoch 1: 0.22646133601665497
Local loss @ local epoch 2: 0.19315211474895477
Local loss @ local epoch 3: 0.1763710230588913
Local loss @ local epoch 4: 0.17100302875041962
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.1 seconds!
[tester] 
SST2Metric: acc=0.8589449541284404, hinge=0.7067027493901209, ce=0.355420346072781
Local test acc @ epoch 34: 0.8589
Client 8 execute local training on 8 samples...
Local loss @ local epoch 0: 0.21373867988586426
Local loss @ local epoch 1: 0.15022149682044983
Local loss @ local epoch 2: 0.10543845593929291
Local loss @ local epoch 3: 0.07599043846130371
Local loss @ local epoch 4: 0.058123860508203506
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.08 seconds!
[tester] 
SST2Metric: acc=0.8360091743119266, hinge=0.8144856859511191, ce=0.4284437476238663
Local test acc @ epoch 34: 0.836
Client 4 execute local training on 8 samples...
Local loss @ local epoch 0: 0.1448744535446167
Local loss @ local epoch 1: 0.11273965239524841
Local loss @ local epoch 2: 0.09632473438978195
Local loss @ local epoch 3: 0.08979849517345428
Local loss @ local epoch 4: 0.08700791001319885
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.14 seconds!
[tester] 
SST2Metric: acc=0.8463302752293578, hinge=0.6941159388067526, ce=0.35106850866177075
Local test acc @ epoch 34: 0.8463
Client 9 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0606125108897686
Local loss @ local epoch 1: 0.05320574715733528
Local loss @ local epoch 2: 0.0493277981877327
Local loss @ local epoch 3: 0.0454174168407917
Local loss @ local epoch 4: 0.041348155587911606
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.08 seconds!
[tester] 
SST2Metric: acc=0.8520642201834863, hinge=0.7116510410101042, ce=0.365677981381091
Local test acc @ epoch 34: 0.8521
Client 2 execute local training on 8 samples...
Local loss @ local epoch 0: 0.09083941578865051
Local loss @ local epoch 1: 0.06770014762878418
Local loss @ local epoch 2: 0.05588870495557785
Local loss @ local epoch 3: 0.05171502009034157
Local loss @ local epoch 4: 0.05152516812086105
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.16 seconds!
[tester] 
SST2Metric: acc=0.7901376146788991, hinge=1.0435591996262927, ce=0.5776169512147477
Local test acc @ epoch 34: 0.7901
Client 6 execute local training on 8 samples...
Local loss @ local epoch 0: 0.4338203966617584
Local loss @ local epoch 1: 0.3457815945148468
Local loss @ local epoch 2: 0.27001824975013733
Local loss @ local epoch 3: 0.20902347564697266
Local loss @ local epoch 4: 0.16443881392478943
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.12 seconds!
[tester] 
SST2Metric: acc=0.8532110091743119, hinge=0.7052345770761508, ce=0.3606077751717231
Local test acc @ epoch 34: 0.8532
Client 0 execute local training on 8 samples...
Local loss @ local epoch 0: 0.04847446829080582
Local loss @ local epoch 1: 0.04331711307168007
Local loss @ local epoch 2: 0.03927422687411308
Local loss @ local epoch 3: 0.036169614642858505
Local loss @ local epoch 4: 0.033807918429374695
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.14 seconds!
[tester] 
SST2Metric: acc=0.8486238532110092, hinge=0.7044915634831157, ce=0.3589296016261118
Local test acc @ epoch 34: 0.8486
Client 5 execute local training on 8 samples...
Local loss @ local epoch 0: 0.06886397302150726
Local loss @ local epoch 1: 0.04859604686498642
Local loss @ local epoch 2: 0.03813805431127548
Local loss @ local epoch 3: 0.03453115001320839
Local loss @ local epoch 4: 0.034984007477760315
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.27 seconds!
[tester] 
SST2Metric: acc=0.8211009174311926, hinge=0.8495948609955813, ce=0.44961810284593234
Local test acc @ epoch 34: 0.8211
Client 7 execute local training on 8 samples...
Local loss @ local epoch 0: 0.12872415781021118
Local loss @ local epoch 1: 0.09803719073534012
Local loss @ local epoch 2: 0.08014650642871857
Local loss @ local epoch 3: 0.07311519235372543
Local loss @ local epoch 4: 0.07345942407846451
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.13 seconds!
[tester] 
SST2Metric: acc=0.8486238532110092, hinge=0.7106704711914062, ce=0.36308330772102837
Local test acc @ epoch 34: 0.8486
Global evaluate on test data...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.45 seconds!
[tester] 
SST2Metric: acc=0.805045871559633, hinge=0.9060224449142403, ce=0.42913082999032026
Global test acc @ epoch 34: 0.805
Global epoch 35...
Client 2 execute local training on 8 samples...
Local loss @ local epoch 0: 0.09834110736846924
Local loss @ local epoch 1: 0.07156115770339966
Local loss @ local epoch 2: 0.056926801800727844
Local loss @ local epoch 3: 0.05080742761492729
Local loss @ local epoch 4: 0.049783896654844284
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.13 seconds!
[tester] 
SST2Metric: acc=0.8004587155963303, hinge=1.0180102836921674, ce=0.5608688991457019
Local test acc @ epoch 35: 0.8005
Client 3 execute local training on 8 samples...
Local loss @ local epoch 0: 0.4377385973930359
Local loss @ local epoch 1: 0.35184958577156067
Local loss @ local epoch 2: 0.2833058536052704
Local loss @ local epoch 3: 0.23090310394763947
Local loss @ local epoch 4: 0.19354967772960663
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.15 seconds!
[tester] 
SST2Metric: acc=0.8463302752293578, hinge=0.6976379584008401, ce=0.35322066481490066
Local test acc @ epoch 35: 0.8463
Client 8 execute local training on 8 samples...
Local loss @ local epoch 0: 0.09773731231689453
Local loss @ local epoch 1: 0.06874106824398041
Local loss @ local epoch 2: 0.05145249515771866
Local loss @ local epoch 3: 0.04269276186823845
Local loss @ local epoch 4: 0.03956013172864914
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.12 seconds!
[tester] 
SST2Metric: acc=0.8211009174311926, hinge=0.8925398670205282, ce=0.4832455705287396
Local test acc @ epoch 35: 0.8211
Client 5 execute local training on 8 samples...
Local loss @ local epoch 0: 0.2876633405685425
Local loss @ local epoch 1: 0.19673119485378265
Local loss @ local epoch 2: 0.13307304680347443
Local loss @ local epoch 3: 0.09080798923969269
Local loss @ local epoch 4: 0.06398563086986542
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.2 seconds!
[tester] 
SST2Metric: acc=0.8474770642201835, hinge=0.7053620610215249, ce=0.35717190110867997
Local test acc @ epoch 35: 0.8475
Client 0 execute local training on 8 samples...
Local loss @ local epoch 0: 0.037701867520809174
Local loss @ local epoch 1: 0.0341537706553936
Local loss @ local epoch 2: 0.03370392322540283
Local loss @ local epoch 3: 0.03276665508747101
Local loss @ local epoch 4: 0.03118625096976757
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.1 seconds!
[tester] 
SST2Metric: acc=0.8451834862385321, hinge=0.7158066451276114, ce=0.3649778773905624
Local test acc @ epoch 35: 0.8452
Client 6 execute local training on 8 samples...
Local loss @ local epoch 0: 0.22414667904376984
Local loss @ local epoch 1: 0.18755151331424713
Local loss @ local epoch 2: 0.17017017304897308
Local loss @ local epoch 3: 0.1660607010126114
Local loss @ local epoch 4: 0.1650334596633911
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.09 seconds!
[tester] 
SST2Metric: acc=0.838302752293578, hinge=0.7716705123765752, ce=0.39804041577041693
Local test acc @ epoch 35: 0.8383
Client 4 execute local training on 8 samples...
Local loss @ local epoch 0: 0.20673717558383942
Local loss @ local epoch 1: 0.1564522534608841
Local loss @ local epoch 2: 0.12424799799919128
Local loss @ local epoch 3: 0.10722029209136963
Local loss @ local epoch 4: 0.1002497524023056
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.05 seconds!
[tester] 
SST2Metric: acc=0.8520642201834863, hinge=0.7312876676474143, ce=0.3785694056711749
Local test acc @ epoch 35: 0.8521
Client 9 execute local training on 8 samples...
Local loss @ local epoch 0: 0.045406799763441086
Local loss @ local epoch 1: 0.04049627110362053
Local loss @ local epoch 2: 0.03791762515902519
Local loss @ local epoch 3: 0.03510110825300217
Local loss @ local epoch 4: 0.0322161503136158
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.13 seconds!
[tester] 
SST2Metric: acc=0.8520642201834863, hinge=0.7120213112153044, ce=0.36744713743533425
Local test acc @ epoch 35: 0.8521
Client 7 execute local training on 8 samples...
Local loss @ local epoch 0: 0.09118719398975372
Local loss @ local epoch 1: 0.07801968604326248
Local loss @ local epoch 2: 0.07518281042575836
Local loss @ local epoch 3: 0.0750625729560852
Local loss @ local epoch 4: 0.07333476841449738
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.08 seconds!
[tester] 
SST2Metric: acc=0.841743119266055, hinge=0.7414277995671701, ce=0.3835009336557
Local test acc @ epoch 35: 0.8417
Client 1 execute local training on 8 samples...
Local loss @ local epoch 0: 0.05341283604502678
Local loss @ local epoch 1: 0.037171393632888794
Local loss @ local epoch 2: 0.027797585353255272
Local loss @ local epoch 3: 0.02318262867629528
Local loss @ local epoch 4: 0.021487612277269363
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.09 seconds!
[tester] 
SST2Metric: acc=0.8440366972477065, hinge=0.7685586305659845, ce=0.406159820769905
Local test acc @ epoch 35: 0.844
Global evaluate on test data...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.69 seconds!
[tester] 
SST2Metric: acc=0.7981651376146789, hinge=0.9256275100718945, ce=0.44516593597214155
Global test acc @ epoch 35: 0.7982
Global epoch 36...
Client 7 execute local training on 8 samples...
Local loss @ local epoch 0: 0.09125863015651703
Local loss @ local epoch 1: 0.080010324716568
Local loss @ local epoch 2: 0.07767760753631592
Local loss @ local epoch 3: 0.07636164873838425
Local loss @ local epoch 4: 0.07338378578424454
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.28 seconds!
[tester] 
SST2Metric: acc=0.8486238532110092, hinge=0.7306912511313727, ce=0.37513981040977284
Local test acc @ epoch 36: 0.8486
Client 0 execute local training on 8 samples...
Local loss @ local epoch 0: 0.03759066388010979
Local loss @ local epoch 1: 0.031710729002952576
Local loss @ local epoch 2: 0.030741427093744278
Local loss @ local epoch 3: 0.03134498372673988
Local loss @ local epoch 4: 0.030967608094215393
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.16 seconds!
[tester] 
SST2Metric: acc=0.8520642201834863, hinge=0.7155121261373573, ce=0.3678017459732882
Local test acc @ epoch 36: 0.8521
Client 2 execute local training on 8 samples...
Local loss @ local epoch 0: 0.09079475700855255
Local loss @ local epoch 1: 0.0725698247551918
Local loss @ local epoch 2: 0.06393542885780334
Local loss @ local epoch 3: 0.060026831924915314
Local loss @ local epoch 4: 0.057290636003017426
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.08 seconds!
[tester] 
SST2Metric: acc=0.8279816513761468, hinge=0.8717848140438762, ce=0.47210138688427866
Local test acc @ epoch 36: 0.828
Client 9 execute local training on 8 samples...
Local loss @ local epoch 0: 0.05450884625315666
Local loss @ local epoch 1: 0.03900150954723358
Local loss @ local epoch 2: 0.03160521760582924
Local loss @ local epoch 3: 0.029629360884428024
Local loss @ local epoch 4: 0.030272454023361206
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.12 seconds!
[tester] 
SST2Metric: acc=0.8463302752293578, hinge=0.7168630360189928, ce=0.3686441014725886
Local test acc @ epoch 36: 0.8463
Client 4 execute local training on 8 samples...
Local loss @ local epoch 0: 0.17070603370666504
Local loss @ local epoch 1: 0.14943791925907135
Local loss @ local epoch 2: 0.13147102296352386
Local loss @ local epoch 3: 0.11619940400123596
Local loss @ local epoch 4: 0.10328435152769089
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.11 seconds!
[tester] 
SST2Metric: acc=0.8520642201834863, hinge=0.703730134925711, ce=0.3617881119481946
Local test acc @ epoch 36: 0.8521
Client 6 execute local training on 8 samples...
Local loss @ local epoch 0: 0.16194026172161102
Local loss @ local epoch 1: 0.13800323009490967
Local loss @ local epoch 2: 0.12971770763397217
Local loss @ local epoch 3: 0.12810999155044556
Local loss @ local epoch 4: 0.12505698204040527
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.11 seconds!
[tester] 
SST2Metric: acc=0.8520642201834863, hinge=0.7031118586796139, ce=0.35792552876294753
Local test acc @ epoch 36: 0.8521
Client 3 execute local training on 8 samples...
Local loss @ local epoch 0: 0.1794518083333969
Local loss @ local epoch 1: 0.16893377900123596
Local loss @ local epoch 2: 0.1635141670703888
Local loss @ local epoch 3: 0.1581135094165802
Local loss @ local epoch 4: 0.1523158848285675
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.06 seconds!
[tester] 
SST2Metric: acc=0.8555045871559633, hinge=0.7069433887890719, ce=0.3594064233064173
Local test acc @ epoch 36: 0.8555
Client 5 execute local training on 8 samples...
Local loss @ local epoch 0: 0.04266180843114853
Local loss @ local epoch 1: 0.03512388840317726
Local loss @ local epoch 2: 0.033779989928007126
Local loss @ local epoch 3: 0.034162119030952454
Local loss @ local epoch 4: 0.03340396285057068
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.12 seconds!
[tester] 
SST2Metric: acc=0.8337155963302753, hinge=0.7988294093707285, ce=0.4165551027009657
Local test acc @ epoch 36: 0.8337
Client 8 execute local training on 8 samples...
Local loss @ local epoch 0: 0.2581814229488373
Local loss @ local epoch 1: 0.18243570625782013
Local loss @ local epoch 2: 0.12709590792655945
Local loss @ local epoch 3: 0.08899229764938354
Local loss @ local epoch 4: 0.06420334428548813
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.1 seconds!
[tester] 
SST2Metric: acc=0.8532110091743119, hinge=0.7130634293917122, ce=0.3700493216523132
Local test acc @ epoch 36: 0.8532
Client 1 execute local training on 8 samples...
Local loss @ local epoch 0: 0.021744821220636368
Local loss @ local epoch 1: 0.01900048553943634
Local loss @ local epoch 2: 0.017610684037208557
Local loss @ local epoch 3: 0.016304733231663704
Local loss @ local epoch 4: 0.014917058870196342
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.17 seconds!
[tester] 
SST2Metric: acc=0.8440366972477065, hinge=0.8014255216635695, ce=0.4256093418610137
Local test acc @ epoch 36: 0.844
Global evaluate on test data...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.59 seconds!
[tester] 
SST2Metric: acc=0.8038990825688074, hinge=0.9017076212058374, ce=0.4288777823329245
Global test acc @ epoch 36: 0.8039
Global epoch 37...
Client 4 execute local training on 8 samples...
Local loss @ local epoch 0: 0.11394361406564713
Local loss @ local epoch 1: 0.10051466524600983
Local loss @ local epoch 2: 0.09002002328634262
Local loss @ local epoch 3: 0.08082352578639984
Local loss @ local epoch 4: 0.07282140851020813
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.96 seconds!
[tester] 
SST2Metric: acc=0.8520642201834863, hinge=0.7182899960957536, ce=0.37093527961966644
Local test acc @ epoch 37: 0.8521
Client 5 execute local training on 8 samples...
Local loss @ local epoch 0: 0.08540058881044388
Local loss @ local epoch 1: 0.057862043380737305
Local loss @ local epoch 2: 0.04223073646426201
Local loss @ local epoch 3: 0.034980833530426025
Local loss @ local epoch 4: 0.03323380649089813
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.04 seconds!
[tester] 
SST2Metric: acc=0.8371559633027523, hinge=0.7845582092573883, ce=0.4051509622928746
Local test acc @ epoch 37: 0.8372
Client 9 execute local training on 8 samples...
Local loss @ local epoch 0: 0.12741200625896454
Local loss @ local epoch 1: 0.088471919298172
Local loss @ local epoch 2: 0.0635707750916481
Local loss @ local epoch 3: 0.04948440194129944
Local loss @ local epoch 4: 0.04305263236165047
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.08 seconds!
[tester] 
SST2Metric: acc=0.856651376146789, hinge=0.713806345375306, ce=0.37081469057572136
Local test acc @ epoch 37: 0.8567
Client 6 execute local training on 8 samples...
Local loss @ local epoch 0: 0.14938390254974365
Local loss @ local epoch 1: 0.12535515427589417
Local loss @ local epoch 2: 0.11659260839223862
Local loss @ local epoch 3: 0.11533457785844803
Local loss @ local epoch 4: 0.11345703154802322
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.09 seconds!
[tester] 
SST2Metric: acc=0.8497706422018348, hinge=0.6944209853990362, ce=0.3551757607882449
Local test acc @ epoch 37: 0.8498
Client 0 execute local training on 8 samples...
Local loss @ local epoch 0: 0.06995324045419693
Local loss @ local epoch 1: 0.05543820932507515
Local loss @ local epoch 2: 0.04771706089377403
Local loss @ local epoch 3: 0.04411860555410385
Local loss @ local epoch 4: 0.041871197521686554
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.09 seconds!
[tester] 
SST2Metric: acc=0.8497706422018348, hinge=0.7486697218013466, ce=0.3934553867470849
Local test acc @ epoch 37: 0.8498
Client 3 execute local training on 8 samples...
Local loss @ local epoch 0: 0.21213357150554657
Local loss @ local epoch 1: 0.17814762890338898
Local loss @ local epoch 2: 0.16012708842754364
Local loss @ local epoch 3: 0.1544221192598343
Local loss @ local epoch 4: 0.15493711829185486
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.12 seconds!
[tester] 
SST2Metric: acc=0.8497706422018348, hinge=0.7268440804076851, ce=0.3720323521507169
Local test acc @ epoch 37: 0.8498
Client 2 execute local training on 8 samples...
Local loss @ local epoch 0: 0.27909085154533386
Local loss @ local epoch 1: 0.19467943906784058
Local loss @ local epoch 2: 0.1350603699684143
Local loss @ local epoch 3: 0.09512549638748169
Local loss @ local epoch 4: 0.06969556212425232
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.11 seconds!
[tester] 
SST2Metric: acc=0.8497706422018348, hinge=0.7809203707296913, ce=0.4129492846667903
Local test acc @ epoch 37: 0.8498
Client 7 execute local training on 8 samples...
Local loss @ local epoch 0: 0.11558347195386887
Local loss @ local epoch 1: 0.09125194698572159
Local loss @ local epoch 2: 0.08109744638204575
Local loss @ local epoch 3: 0.07887496054172516
Local loss @ local epoch 4: 0.07863397151231766
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.07 seconds!
[tester] 
SST2Metric: acc=0.8520642201834863, hinge=0.6991526412034254, ce=0.3574508041647528
Local test acc @ epoch 37: 0.8521
Client 8 execute local training on 8 samples...
Local loss @ local epoch 0: 0.11564895510673523
Local loss @ local epoch 1: 0.07976089417934418
Local loss @ local epoch 2: 0.05698563531041145
Local loss @ local epoch 3: 0.04392077028751373
Local loss @ local epoch 4: 0.03760707378387451
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.05 seconds!
[tester] 
SST2Metric: acc=0.8486238532110092, hinge=0.7992632212715411, ce=0.42778992611880695
Local test acc @ epoch 37: 0.8486
Client 1 execute local training on 8 samples...
Local loss @ local epoch 0: 0.023476025089621544
Local loss @ local epoch 1: 0.020058955997228622
Local loss @ local epoch 2: 0.018650954589247704
Local loss @ local epoch 3: 0.017502592876553535
Local loss @ local epoch 4: 0.016127876937389374
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.09 seconds!
[tester] 
SST2Metric: acc=0.8520642201834863, hinge=0.7370074932728339, ce=0.38779557883363525
Local test acc @ epoch 37: 0.8521
Global evaluate on test data...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.66 seconds!
[tester] 
SST2Metric: acc=0.8142201834862385, hinge=0.8710842732716044, ce=0.40911947894807255
Global test acc @ epoch 37: 0.8142
Global epoch 38...
Client 0 execute local training on 8 samples...
Local loss @ local epoch 0: 0.03725860267877579
Local loss @ local epoch 1: 0.033672086894512177
Local loss @ local epoch 2: 0.03129921108484268
Local loss @ local epoch 3: 0.02917337417602539
Local loss @ local epoch 4: 0.027394291013479233
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.99 seconds!
[tester] 
SST2Metric: acc=0.8520642201834863, hinge=0.7011011836178805, ce=0.3605125141458227
Local test acc @ epoch 38: 0.8521
Client 3 execute local training on 8 samples...
Local loss @ local epoch 0: 0.1875343769788742
Local loss @ local epoch 1: 0.15913857519626617
Local loss @ local epoch 2: 0.14383842051029205
Local loss @ local epoch 3: 0.14013780653476715
Local loss @ local epoch 4: 0.14257141947746277
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.01 seconds!
[tester] 
SST2Metric: acc=0.8337155963302753, hinge=0.8019056398113933, ce=0.4199196564614636
Local test acc @ epoch 38: 0.8337
Client 1 execute local training on 8 samples...
Local loss @ local epoch 0: 0.08412773162126541
Local loss @ local epoch 1: 0.05710864067077637
Local loss @ local epoch 2: 0.039090488106012344
Local loss @ local epoch 3: 0.027834106236696243
Local loss @ local epoch 4: 0.02132752351462841
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.05 seconds!
[tester] 
SST2Metric: acc=0.8520642201834863, hinge=0.7202286774959039, ce=0.3752340495270258
Local test acc @ epoch 38: 0.8521
Client 6 execute local training on 8 samples...
Local loss @ local epoch 0: 0.21088126301765442
Local loss @ local epoch 1: 0.1669701337814331
Local loss @ local epoch 2: 0.14109355211257935
Local loss @ local epoch 3: 0.13126108050346375
Local loss @ local epoch 4: 0.13123132288455963
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.0 seconds!
[tester] 
SST2Metric: acc=0.8555045871559633, hinge=0.7221402661242616, ce=0.3698111700680141
Local test acc @ epoch 38: 0.8555
Client 2 execute local training on 8 samples...
Local loss @ local epoch 0: 0.2608898878097534
Local loss @ local epoch 1: 0.1811942607164383
Local loss @ local epoch 2: 0.12540596723556519
Local loss @ local epoch 3: 0.08830215036869049
Local loss @ local epoch 4: 0.06480716168880463
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.15 seconds!
[tester] 
SST2Metric: acc=0.8451834862385321, hinge=0.7931744419380066, ce=0.4205679078671842
Local test acc @ epoch 38: 0.8452
Client 8 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0498238131403923
Local loss @ local epoch 1: 0.04278174415230751
Local loss @ local epoch 2: 0.03993365168571472
Local loss @ local epoch 3: 0.037706613540649414
Local loss @ local epoch 4: 0.035097796469926834
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.07 seconds!
[tester] 
SST2Metric: acc=0.8337155963302753, hinge=0.8593576028259522, ce=0.46738930410543167
Local test acc @ epoch 38: 0.8337
Client 7 execute local training on 8 samples...
Local loss @ local epoch 0: 0.19247020781040192
Local loss @ local epoch 1: 0.13590864837169647
Local loss @ local epoch 2: 0.09992831200361252
Local loss @ local epoch 3: 0.0797727182507515
Local loss @ local epoch 4: 0.07097239047288895
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.03 seconds!
[tester] 
SST2Metric: acc=0.8451834862385321, hinge=0.7108921483842605, ce=0.3649136842219928
Local test acc @ epoch 38: 0.8452
Client 9 execute local training on 8 samples...
Local loss @ local epoch 0: 0.04253264144062996
Local loss @ local epoch 1: 0.0334472730755806
Local loss @ local epoch 2: 0.030437542125582695
Local loss @ local epoch 3: 0.030107328668236732
Local loss @ local epoch 4: 0.029658909887075424
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.98 seconds!
[tester] 
SST2Metric: acc=0.8428899082568807, hinge=0.7486902010550193, ce=0.3965715238346009
Local test acc @ epoch 38: 0.8429
Client 5 execute local training on 8 samples...
Local loss @ local epoch 0: 0.11462712287902832
Local loss @ local epoch 1: 0.07623140513896942
Local loss @ local epoch 2: 0.052138399332761765
Local loss @ local epoch 3: 0.038042791187763214
Local loss @ local epoch 4: 0.030731776729226112
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.02 seconds!
[tester] 
SST2Metric: acc=0.8428899082568807, hinge=0.7681252924947564, ce=0.4052351607317361
Local test acc @ epoch 38: 0.8429
Client 4 execute local training on 8 samples...
Local loss @ local epoch 0: 0.20681892335414886
Local loss @ local epoch 1: 0.1600121706724167
Local loss @ local epoch 2: 0.13062693178653717
Local loss @ local epoch 3: 0.11496829241514206
Local loss @ local epoch 4: 0.10735578089952469
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.08 seconds!
[tester] 
SST2Metric: acc=0.8555045871559633, hinge=0.7117774625983807, ce=0.37264453816663373
Local test acc @ epoch 38: 0.8555
Global evaluate on test data...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.61 seconds!
[tester] 
SST2Metric: acc=0.801605504587156, hinge=0.8977714116693637, ce=0.43033620123879623
Global test acc @ epoch 38: 0.8016
Global epoch 39...
Client 6 execute local training on 8 samples...
Local loss @ local epoch 0: 0.17869342863559723
Local loss @ local epoch 1: 0.14924830198287964
Local loss @ local epoch 2: 0.13696186244487762
Local loss @ local epoch 3: 0.1348022222518921
Local loss @ local epoch 4: 0.13351422548294067
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.12 seconds!
[tester] 
SST2Metric: acc=0.8532110091743119, hinge=0.7232851577461312, ce=0.37161079980432987
Local test acc @ epoch 39: 0.8532
Client 7 execute local training on 8 samples...
Local loss @ local epoch 0: 0.11937829107046127
Local loss @ local epoch 1: 0.0936797484755516
Local loss @ local epoch 2: 0.08012749254703522
Local loss @ local epoch 3: 0.07586140930652618
Local loss @ local epoch 4: 0.07608303427696228
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.15 seconds!
[tester] 
SST2Metric: acc=0.8497706422018348, hinge=0.7061396708729071, ce=0.3671482335818812
Local test acc @ epoch 39: 0.8498
Client 2 execute local training on 8 samples...
Local loss @ local epoch 0: 0.07579264789819717
Local loss @ local epoch 1: 0.05697057023644447
Local loss @ local epoch 2: 0.04779693856835365
Local loss @ local epoch 3: 0.044811494648456573
Local loss @ local epoch 4: 0.044667817652225494
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.03 seconds!
[tester] 
SST2Metric: acc=0.8130733944954128, hinge=0.986247960990722, ce=0.5501027195073596
Local test acc @ epoch 39: 0.8131
Client 4 execute local training on 8 samples...
Local loss @ local epoch 0: 0.2475578486919403
Local loss @ local epoch 1: 0.1798877716064453
Local loss @ local epoch 2: 0.13270017504692078
Local loss @ local epoch 3: 0.10329892486333847
Local loss @ local epoch 4: 0.08763226866722107
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.11 seconds!
[tester] 
SST2Metric: acc=0.8520642201834863, hinge=0.7041732288828684, ce=0.3650306328456131
Local test acc @ epoch 39: 0.8521
Client 0 execute local training on 8 samples...
Local loss @ local epoch 0: 0.031765248626470566
Local loss @ local epoch 1: 0.028725285083055496
Local loss @ local epoch 2: 0.026388349011540413
Local loss @ local epoch 3: 0.02460291050374508
Local loss @ local epoch 4: 0.023226214572787285
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.06 seconds!
[tester] 
SST2Metric: acc=0.8474770642201835, hinge=0.709069612239479, ce=0.3664499479260535
Local test acc @ epoch 39: 0.8475
Client 5 execute local training on 8 samples...
Local loss @ local epoch 0: 0.05254349112510681
Local loss @ local epoch 1: 0.03733045607805252
Local loss @ local epoch 2: 0.029864005744457245
Local loss @ local epoch 3: 0.02768966555595398
Local loss @ local epoch 4: 0.028417594730854034
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.09 seconds!
[tester] 
SST2Metric: acc=0.8245412844036697, hinge=0.8500640667086348, ce=0.457339711829063
Local test acc @ epoch 39: 0.8245
Client 9 execute local training on 8 samples...
Local loss @ local epoch 0: 0.1051800400018692
Local loss @ local epoch 1: 0.07020098716020584
Local loss @ local epoch 2: 0.04830286651849747
Local loss @ local epoch 3: 0.03565649688243866
Local loss @ local epoch 4: 0.029305249452590942
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.03 seconds!
[tester] 
SST2Metric: acc=0.8520642201834863, hinge=0.7131532329484958, ce=0.3716792396457712
Local test acc @ epoch 39: 0.8521
Client 1 execute local training on 8 samples...
Local loss @ local epoch 0: 0.022309541702270508
Local loss @ local epoch 1: 0.01918136700987816
Local loss @ local epoch 2: 0.01775490865111351
Local loss @ local epoch 3: 0.01654759980738163
Local loss @ local epoch 4: 0.01520310528576374
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.99 seconds!
[tester] 
SST2Metric: acc=0.8463302752293578, hinge=0.7889254042588243, ce=0.42228141759411186
Local test acc @ epoch 39: 0.8463
Client 3 execute local training on 8 samples...
Local loss @ local epoch 0: 0.24968750774860382
Local loss @ local epoch 1: 0.20174793899059296
Local loss @ local epoch 2: 0.16826577484607697
Local loss @ local epoch 3: 0.14888177812099457
Local loss @ local epoch 4: 0.14167433977127075
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.04 seconds!
[tester] 
SST2Metric: acc=0.8543577981651376, hinge=0.7233196678785009, ce=0.3733018913836039
Local test acc @ epoch 39: 0.8544
Client 8 execute local training on 8 samples...
Local loss @ local epoch 0: 0.17327402532100677
Local loss @ local epoch 1: 0.11999796330928802
Local loss @ local epoch 2: 0.08329316973686218
Local loss @ local epoch 3: 0.05959301069378853
Local loss @ local epoch 4: 0.04538600146770477
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.1 seconds!
[tester] 
SST2Metric: acc=0.8474770642201835, hinge=0.7733921213160961, ce=0.41271927325618923
Local test acc @ epoch 39: 0.8475
Global evaluate on test data...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.68 seconds!
[tester] 
SST2Metric: acc=0.801605504587156, hinge=0.8948948010118729, ce=0.43062188001264123
Global test acc @ epoch 39: 0.8016
Global epoch 40...
Client 0 execute local training on 8 samples...
Local loss @ local epoch 0: 0.025377918034791946
Local loss @ local epoch 1: 0.02376323752105236
Local loss @ local epoch 2: 0.022424064576625824
Local loss @ local epoch 3: 0.02132491208612919
Local loss @ local epoch 4: 0.02036493457853794
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.09 seconds!
[tester] 
SST2Metric: acc=0.8440366972477065, hinge=0.7213237539890709, ce=0.3742014612797477
Local test acc @ epoch 40: 0.844
Client 6 execute local training on 8 samples...
Local loss @ local epoch 0: 0.21327652037143707
Local loss @ local epoch 1: 0.1752249151468277
Local loss @ local epoch 2: 0.15595126152038574
Local loss @ local epoch 3: 0.15092958509922028
Local loss @ local epoch 4: 0.1509922593832016
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.01 seconds!
[tester] 
SST2Metric: acc=0.8348623853211009, hinge=0.7974911081954974, ce=0.4212481948384725
Local test acc @ epoch 40: 0.8349
Client 5 execute local training on 8 samples...
Local loss @ local epoch 0: 0.027491023764014244
Local loss @ local epoch 1: 0.02621021866798401
Local loss @ local epoch 2: 0.024797391146421432
Local loss @ local epoch 3: 0.02327278070151806
Local loss @ local epoch 4: 0.0220981203019619
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.04 seconds!
[tester] 
SST2Metric: acc=0.8360091743119266, hinge=0.7750582201491802, ce=0.40810079329194276
Local test acc @ epoch 40: 0.836
Client 3 execute local training on 8 samples...
Local loss @ local epoch 0: 0.140615314245224
Local loss @ local epoch 1: 0.13506227731704712
Local loss @ local epoch 2: 0.13301704823970795
Local loss @ local epoch 3: 0.12904801964759827
Local loss @ local epoch 4: 0.12464005500078201
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.11 seconds!
[tester] 
SST2Metric: acc=0.8428899082568807, hinge=0.7483994955316596, ce=0.3903151191984637
Local test acc @ epoch 40: 0.8429
Client 7 execute local training on 8 samples...
Local loss @ local epoch 0: 0.08610180020332336
Local loss @ local epoch 1: 0.06921428442001343
Local loss @ local epoch 2: 0.06236225366592407
Local loss @ local epoch 3: 0.06187891587615013
Local loss @ local epoch 4: 0.0627729594707489
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.09 seconds!
[tester] 
SST2Metric: acc=0.8532110091743119, hinge=0.7160536471856843, ce=0.3731890793231854
Local test acc @ epoch 40: 0.8532
Client 9 execute local training on 8 samples...
Local loss @ local epoch 0: 0.02811916172504425
Local loss @ local epoch 1: 0.026089344173669815
Local loss @ local epoch 2: 0.024260083213448524
Local loss @ local epoch 3: 0.022441454231739044
Local loss @ local epoch 4: 0.02094132825732231
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.15 seconds!
[tester] 
SST2Metric: acc=0.8532110091743119, hinge=0.735403464748225, ce=0.3893660320857659
Local test acc @ epoch 40: 0.8532
Client 2 execute local training on 8 samples...
Local loss @ local epoch 0: 0.09701021015644073
Local loss @ local epoch 1: 0.07859832048416138
Local loss @ local epoch 2: 0.06829220801591873
Local loss @ local epoch 3: 0.061906248331069946
Local loss @ local epoch 4: 0.056828055530786514
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.09 seconds!
[tester] 
SST2Metric: acc=0.838302752293578, hinge=0.8604566535545052, ce=0.47710194413490387
Local test acc @ epoch 40: 0.8383
Client 1 execute local training on 8 samples...
Local loss @ local epoch 0: 0.027567099779844284
Local loss @ local epoch 1: 0.021386250853538513
Local loss @ local epoch 2: 0.018589716404676437
Local loss @ local epoch 3: 0.017675887793302536
Local loss @ local epoch 4: 0.017207900062203407
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.91 seconds!
[tester] 
SST2Metric: acc=0.8486238532110092, hinge=0.7212764925639564, ce=0.38084356339409126
Local test acc @ epoch 40: 0.8486
Client 8 execute local training on 8 samples...
Local loss @ local epoch 0: 0.048917386680841446
Local loss @ local epoch 1: 0.038122180849313736
Local loss @ local epoch 2: 0.03371909260749817
Local loss @ local epoch 3: 0.032705314457416534
Local loss @ local epoch 4: 0.03238371014595032
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.98 seconds!
[tester] 
SST2Metric: acc=0.838302752293578, hinge=0.8568604304156172, ce=0.47494076584987
Local test acc @ epoch 40: 0.8383
Client 4 execute local training on 8 samples...
Local loss @ local epoch 0: 0.23987413942813873
Local loss @ local epoch 1: 0.18362322449684143
Local loss @ local epoch 2: 0.1478254795074463
Local loss @ local epoch 3: 0.12783826887607574
Local loss @ local epoch 4: 0.11740747094154358
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.98 seconds!
[tester] 
SST2Metric: acc=0.8520642201834863, hinge=0.706466791279819, ce=0.3694473403087313
Local test acc @ epoch 40: 0.8521
Global evaluate on test data...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.4 seconds!
[tester] 
SST2Metric: acc=0.805045871559633, hinge=0.9049878556520567, ce=0.4407965194331397
Global test acc @ epoch 40: 0.805
Global epoch 41...
Client 8 execute local training on 8 samples...
Local loss @ local epoch 0: 0.05169762298464775
Local loss @ local epoch 1: 0.03840687498450279
Local loss @ local epoch 2: 0.03209857642650604
Local loss @ local epoch 3: 0.030271274968981743
Local loss @ local epoch 4: 0.030425339937210083
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.93 seconds!
[tester] 
SST2Metric: acc=0.838302752293578, hinge=0.8312621906809851, ce=0.4563050040240408
Local test acc @ epoch 41: 0.8383
Client 6 execute local training on 8 samples...
Local loss @ local epoch 0: 0.4729495942592621
Local loss @ local epoch 1: 0.3827816843986511
Local loss @ local epoch 2: 0.3037944734096527
Local loss @ local epoch 3: 0.23891068994998932
Local loss @ local epoch 4: 0.19045792520046234
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.99 seconds!
[tester] 
SST2Metric: acc=0.8451834862385321, hinge=0.7265576383936296, ce=0.37868071712741475
Local test acc @ epoch 41: 0.8452
Client 5 execute local training on 8 samples...
Local loss @ local epoch 0: 0.02743515744805336
Local loss @ local epoch 1: 0.022386787459254265
Local loss @ local epoch 2: 0.021443085744976997
Local loss @ local epoch 3: 0.021770913153886795
Local loss @ local epoch 4: 0.021389521658420563
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.01 seconds!
[tester] 
SST2Metric: acc=0.838302752293578, hinge=0.8148670553340824, ce=0.43778583703887297
Local test acc @ epoch 41: 0.8383
Client 3 execute local training on 8 samples...
Local loss @ local epoch 0: 0.1350613534450531
Local loss @ local epoch 1: 0.129960298538208
Local loss @ local epoch 2: 0.12728452682495117
Local loss @ local epoch 3: 0.12306103855371475
Local loss @ local epoch 4: 0.11888468265533447
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.01 seconds!
[tester] 
SST2Metric: acc=0.8394495412844036, hinge=0.7600449873219937, ce=0.3998591918754605
Local test acc @ epoch 41: 0.8394
Client 0 execute local training on 8 samples...
Local loss @ local epoch 0: 0.03779153525829315
Local loss @ local epoch 1: 0.027155902236700058
Local loss @ local epoch 2: 0.02133273333311081
Local loss @ local epoch 3: 0.01915690116584301
Local loss @ local epoch 4: 0.019358523190021515
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.91 seconds!
[tester] 
SST2Metric: acc=0.8451834862385321, hinge=0.7357301133761712, ce=0.3899203109290075
Local test acc @ epoch 41: 0.8452
Client 1 execute local training on 8 samples...
Local loss @ local epoch 0: 0.01983998902142048
Local loss @ local epoch 1: 0.017735810950398445
Local loss @ local epoch 2: 0.01618182472884655
Local loss @ local epoch 3: 0.01467443909496069
Local loss @ local epoch 4: 0.013283558189868927
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.94 seconds!
[tester] 
SST2Metric: acc=0.8509174311926605, hinge=0.7767767177535854, ce=0.4176748775056811
Local test acc @ epoch 41: 0.8509
Client 7 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0924420952796936
Local loss @ local epoch 1: 0.07431536167860031
Local loss @ local epoch 2: 0.06755028665065765
Local loss @ local epoch 3: 0.06625036150217056
Local loss @ local epoch 4: 0.06549873948097229
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.03 seconds!
[tester] 
SST2Metric: acc=0.8509174311926605, hinge=0.7130045330305712, ce=0.3705658483566768
Local test acc @ epoch 41: 0.8509
Client 4 execute local training on 8 samples...
Local loss @ local epoch 0: 0.14299920201301575
Local loss @ local epoch 1: 0.11369682848453522
Local loss @ local epoch 2: 0.09699822217226028
Local loss @ local epoch 3: 0.08820178359746933
Local loss @ local epoch 4: 0.08242335915565491
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.06 seconds!
[tester] 
SST2Metric: acc=0.8497706422018348, hinge=0.7494457927318888, ce=0.40090983184567147
Local test acc @ epoch 41: 0.8498
Client 2 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0666678249835968
Local loss @ local epoch 1: 0.04782825708389282
Local loss @ local epoch 2: 0.037574246525764465
Local loss @ local epoch 3: 0.03332536295056343
Local loss @ local epoch 4: 0.03279918059706688
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.99 seconds!
[tester] 
SST2Metric: acc=0.7786697247706422, hinge=1.1305137824848157, ce=0.6483713381073583
Local test acc @ epoch 41: 0.7787
Client 9 execute local training on 8 samples...
Local loss @ local epoch 0: 0.16549472510814667
Local loss @ local epoch 1: 0.11413700133562088
Local loss @ local epoch 2: 0.07812536507844925
Local loss @ local epoch 3: 0.054954010993242264
Local loss @ local epoch 4: 0.04143572226166725
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.99 seconds!
[tester] 
SST2Metric: acc=0.8520642201834863, hinge=0.7264548354192611, ce=0.38637899469857645
Local test acc @ epoch 41: 0.8521
Global evaluate on test data...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.46 seconds!
[tester] 
SST2Metric: acc=0.8004587155963303, hinge=0.9060401824909613, ce=0.44406610677753566
Global test acc @ epoch 41: 0.8005
Global epoch 42...
Client 1 execute local training on 8 samples...
Local loss @ local epoch 0: 0.01688307709991932
Local loss @ local epoch 1: 0.014283571392297745
Local loss @ local epoch 2: 0.01327579002827406
Local loss @ local epoch 3: 0.012560601346194744
Local loss @ local epoch 4: 0.011692557483911514
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.0 seconds!
[tester] 
SST2Metric: acc=0.838302752293578, hinge=0.8144402748674427, ce=0.442054724414756
Local test acc @ epoch 42: 0.8383
Client 7 execute local training on 8 samples...
Local loss @ local epoch 0: 0.10802297294139862
Local loss @ local epoch 1: 0.0832996815443039
Local loss @ local epoch 2: 0.07214383780956268
Local loss @ local epoch 3: 0.06908445805311203
Local loss @ local epoch 4: 0.06879428029060364
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.97 seconds!
[tester] 
SST2Metric: acc=0.8543577981651376, hinge=0.7036932684959621, ce=0.3655574474475184
Local test acc @ epoch 42: 0.8544
Client 6 execute local training on 8 samples...
Local loss @ local epoch 0: 0.12566827237606049
Local loss @ local epoch 1: 0.11683273315429688
Local loss @ local epoch 2: 0.11172033101320267
Local loss @ local epoch 3: 0.10517353564500809
Local loss @ local epoch 4: 0.09837046265602112
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.96 seconds!
[tester] 
SST2Metric: acc=0.8497706422018348, hinge=0.699565757578666, ce=0.364782269471624
Local test acc @ epoch 42: 0.8498
Client 8 execute local training on 8 samples...
Local loss @ local epoch 0: 0.16792190074920654
Local loss @ local epoch 1: 0.11701091378927231
Local loss @ local epoch 2: 0.0821368619799614
Local loss @ local epoch 3: 0.05990821123123169
Local loss @ local epoch 4: 0.046927861869335175
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.91 seconds!
[tester] 
SST2Metric: acc=0.8302752293577982, hinge=0.8683453843407675, ce=0.47896964736539155
Local test acc @ epoch 42: 0.8303
Client 0 execute local training on 8 samples...
Local loss @ local epoch 0: 0.03741517663002014
Local loss @ local epoch 1: 0.027645017951726913
Local loss @ local epoch 2: 0.023303985595703125
Local loss @ local epoch 3: 0.022215113043785095
Local loss @ local epoch 4: 0.022354960441589355
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.96 seconds!
[tester] 
SST2Metric: acc=0.8543577981651376, hinge=0.7024414455945339, ce=0.36756380616448
Local test acc @ epoch 42: 0.8544
Client 5 execute local training on 8 samples...
Local loss @ local epoch 0: 0.03184058517217636
Local loss @ local epoch 1: 0.02267119474709034
Local loss @ local epoch 2: 0.0185141172260046
Local loss @ local epoch 3: 0.017656151205301285
Local loss @ local epoch 4: 0.01841134950518608
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.03 seconds!
[tester] 
SST2Metric: acc=0.8337155963302753, hinge=0.820656722155186, ce=0.44096891578594477
Local test acc @ epoch 42: 0.8337
Client 2 execute local training on 8 samples...
Local loss @ local epoch 0: 0.3850235044956207
Local loss @ local epoch 1: 0.2754719853401184
Local loss @ local epoch 2: 0.19352015852928162
Local loss @ local epoch 3: 0.1353212296962738
Local loss @ local epoch 4: 0.09585297107696533
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.07 seconds!
[tester] 
SST2Metric: acc=0.8509174311926605, hinge=0.7179516527904283, ce=0.3806125105512457
Local test acc @ epoch 42: 0.8509
Client 9 execute local training on 8 samples...
Local loss @ local epoch 0: 0.02978368103504181
Local loss @ local epoch 1: 0.027191326022148132
Local loss @ local epoch 2: 0.025141805410385132
Local loss @ local epoch 3: 0.02307077869772911
Local loss @ local epoch 4: 0.021280301734805107
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.0 seconds!
[tester] 
SST2Metric: acc=0.856651376146789, hinge=0.7282282193319514, ce=0.38872792811021895
Local test acc @ epoch 42: 0.8567
Client 3 execute local training on 8 samples...
Local loss @ local epoch 0: 0.22982102632522583
Local loss @ local epoch 1: 0.18792200088500977
Local loss @ local epoch 2: 0.15622451901435852
Local loss @ local epoch 3: 0.1361187994480133
Local loss @ local epoch 4: 0.12756872177124023
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.93 seconds!
[tester] 
SST2Metric: acc=0.838302752293578, hinge=0.8069217429248565, ce=0.4364457143376658
Local test acc @ epoch 42: 0.8383
Client 4 execute local training on 8 samples...
Local loss @ local epoch 0: 0.28140556812286377
Local loss @ local epoch 1: 0.21636757254600525
Local loss @ local epoch 2: 0.16697044670581818
Local loss @ local epoch 3: 0.13309599459171295
Local loss @ local epoch 4: 0.11296076327562332
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.05 seconds!
[tester] 
SST2Metric: acc=0.8497706422018348, hinge=0.7152680438866309, ce=0.3802554689373377
Local test acc @ epoch 42: 0.8498
Global evaluate on test data...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.58 seconds!
[tester] 
SST2Metric: acc=0.8084862385321101, hinge=0.8831497306397201, ce=0.42596593524816384
Global test acc @ epoch 42: 0.8085
Global epoch 43...
Client 5 execute local training on 8 samples...
Local loss @ local epoch 0: 0.032287485897541046
Local loss @ local epoch 1: 0.022783858701586723
Local loss @ local epoch 2: 0.018326960504055023
Local loss @ local epoch 3: 0.01722823828458786
Local loss @ local epoch 4: 0.01789289526641369
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.97 seconds!
[tester] 
SST2Metric: acc=0.8360091743119266, hinge=0.8174240185853539, ce=0.4395624589038278
Local test acc @ epoch 43: 0.836
Client 2 execute local training on 8 samples...
Local loss @ local epoch 0: 0.3720847964286804
Local loss @ local epoch 1: 0.26579952239990234
Local loss @ local epoch 2: 0.1865709275007248
Local loss @ local epoch 3: 0.1305156946182251
Local loss @ local epoch 4: 0.09267079830169678
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.95 seconds!
[tester] 
SST2Metric: acc=0.8509174311926605, hinge=0.7182951133458986, ce=0.3811389160504855
Local test acc @ epoch 43: 0.8509
Client 8 execute local training on 8 samples...
Local loss @ local epoch 0: 0.045805975794792175
Local loss @ local epoch 1: 0.0348019115626812
Local loss @ local epoch 2: 0.029925785958766937
Local loss @ local epoch 3: 0.028650090098381042
Local loss @ local epoch 4: 0.028560597449541092
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.95 seconds!
[tester] 
SST2Metric: acc=0.8348623853211009, hinge=0.8643784871615401, ce=0.4847241464357208
Local test acc @ epoch 43: 0.8349
Client 1 execute local training on 8 samples...
Local loss @ local epoch 0: 0.023052247241139412
Local loss @ local epoch 1: 0.017151029780507088
Local loss @ local epoch 2: 0.014190218411386013
Local loss @ local epoch 3: 0.013111276552081108
Local loss @ local epoch 4: 0.012834835797548294
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.02 seconds!
[tester] 
SST2Metric: acc=0.8532110091743119, hinge=0.7159187086951841, ce=0.37996828382185427
Local test acc @ epoch 43: 0.8532
Client 6 execute local training on 8 samples...
Local loss @ local epoch 0: 0.1801641583442688
Local loss @ local epoch 1: 0.14133235812187195
Local loss @ local epoch 2: 0.11928213387727737
Local loss @ local epoch 3: 0.11151647567749023
Local loss @ local epoch 4: 0.11196205019950867
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.98 seconds!
[tester] 
SST2Metric: acc=0.8497706422018348, hinge=0.7445468950435656, ce=0.39223254264392166
Local test acc @ epoch 43: 0.8498
Client 4 execute local training on 8 samples...
Local loss @ local epoch 0: 0.16335377097129822
Local loss @ local epoch 1: 0.1196940466761589
Local loss @ local epoch 2: 0.09148378670215607
Local loss @ local epoch 3: 0.0761382132768631
Local loss @ local epoch 4: 0.06976668536663055
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.01 seconds!
[tester] 
SST2Metric: acc=0.8543577981651376, hinge=0.7485572068242852, ce=0.4043195573741211
Local test acc @ epoch 43: 0.8544
Client 3 execute local training on 8 samples...
Local loss @ local epoch 0: 0.1763003170490265
Local loss @ local epoch 1: 0.1515991985797882
Local loss @ local epoch 2: 0.14124804735183716
Local loss @ local epoch 3: 0.1390833705663681
Local loss @ local epoch 4: 0.13846980035305023
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.97 seconds!
[tester] 
SST2Metric: acc=0.8532110091743119, hinge=0.7200486010641133, ce=0.37688447152683086
Local test acc @ epoch 43: 0.8532
Client 0 execute local training on 8 samples...
Local loss @ local epoch 0: 0.043652769178152084
Local loss @ local epoch 1: 0.03141061216592789
Local loss @ local epoch 2: 0.024428006261587143
Local loss @ local epoch 3: 0.02142079919576645
Local loss @ local epoch 4: 0.020961828529834747
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.99 seconds!
[tester] 
SST2Metric: acc=0.8474770642201835, hinge=0.748083437938209, ce=0.40417534150952183
Local test acc @ epoch 43: 0.8475
Client 9 execute local training on 8 samples...
Local loss @ local epoch 0: 0.03732424974441528
Local loss @ local epoch 1: 0.03101377561688423
Local loss @ local epoch 2: 0.02898564375936985
Local loss @ local epoch 3: 0.027922680601477623
Local loss @ local epoch 4: 0.02636886201798916
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.88 seconds!
[tester] 
SST2Metric: acc=0.8543577981651376, hinge=0.7126589865859495, ce=0.3771782723093115
Local test acc @ epoch 43: 0.8544
Client 7 execute local training on 8 samples...
Local loss @ local epoch 0: 0.05466199293732643
Local loss @ local epoch 1: 0.05185955762863159
Local loss @ local epoch 2: 0.05018137767910957
Local loss @ local epoch 3: 0.04775070771574974
Local loss @ local epoch 4: 0.045607030391693115
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.95 seconds!
[tester] 
SST2Metric: acc=0.8451834862385321, hinge=0.7334324110538588, ce=0.390472258714566
Local test acc @ epoch 43: 0.8452
Global evaluate on test data...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.27 seconds!
[tester] 
SST2Metric: acc=0.8084862385321101, hinge=0.8877443222824587, ce=0.43117061023250086
Global test acc @ epoch 43: 0.8085
Global epoch 44...
Client 2 execute local training on 8 samples...
Local loss @ local epoch 0: 0.09313900023698807
Local loss @ local epoch 1: 0.06506164371967316
Local loss @ local epoch 2: 0.04837358742952347
Local loss @ local epoch 3: 0.0397234745323658
Local loss @ local epoch 4: 0.036351002752780914
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.95 seconds!
[tester] 
SST2Metric: acc=0.823394495412844, hinge=0.9197704262963129, ce=0.5199448613026991
Local test acc @ epoch 44: 0.8234
Client 3 execute local training on 8 samples...
Local loss @ local epoch 0: 0.35392385721206665
Local loss @ local epoch 1: 0.28452324867248535
Local loss @ local epoch 2: 0.227997824549675
Local loss @ local epoch 3: 0.18402954936027527
Local loss @ local epoch 4: 0.15259531140327454
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.11 seconds!
[tester] 
SST2Metric: acc=0.8520642201834863, hinge=0.7045878441235341, ce=0.3692432682311863
Local test acc @ epoch 44: 0.8521
Client 8 execute local training on 8 samples...
Local loss @ local epoch 0: 0.08040880411863327
Local loss @ local epoch 1: 0.055138684809207916
Local loss @ local epoch 2: 0.039639707654714584
Local loss @ local epoch 3: 0.03112201765179634
Local loss @ local epoch 4: 0.02730410173535347
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.02 seconds!
[tester] 
SST2Metric: acc=0.8371559633027523, hinge=0.8507944826686055, ce=0.4737181156241429
Local test acc @ epoch 44: 0.8372
Client 9 execute local training on 8 samples...
Local loss @ local epoch 0: 0.049731768667697906
Local loss @ local epoch 1: 0.03459486365318298
Local loss @ local epoch 2: 0.0268059391528368
Local loss @ local epoch 3: 0.023982705548405647
Local loss @ local epoch 4: 0.023929975926876068
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.0 seconds!
[tester] 
SST2Metric: acc=0.8463302752293578, hinge=0.7207236023397621, ce=0.38110842005512036
Local test acc @ epoch 44: 0.8463
Client 7 execute local training on 8 samples...
Local loss @ local epoch 0: 0.05327759310603142
Local loss @ local epoch 1: 0.050033941864967346
Local loss @ local epoch 2: 0.049216464161872864
Local loss @ local epoch 3: 0.04722899943590164
Local loss @ local epoch 4: 0.04501214623451233
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.97 seconds!
[tester] 
SST2Metric: acc=0.8428899082568807, hinge=0.7479828663101984, ce=0.40043005481823735
Local test acc @ epoch 44: 0.8429
Client 6 execute local training on 8 samples...
Local loss @ local epoch 0: 0.17929068207740784
Local loss @ local epoch 1: 0.15620610117912292
Local loss @ local epoch 2: 0.1481235921382904
Local loss @ local epoch 3: 0.14494207501411438
Local loss @ local epoch 4: 0.1396787315607071
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.01 seconds!
[tester] 
SST2Metric: acc=0.8405963302752294, hinge=0.781660807515503, ce=0.4212759783482948
Local test acc @ epoch 44: 0.8406
Client 4 execute local training on 8 samples...
Local loss @ local epoch 0: 0.22767043113708496
Local loss @ local epoch 1: 0.1718708872795105
Local loss @ local epoch 2: 0.13251043856143951
Local loss @ local epoch 3: 0.10832089185714722
Local loss @ local epoch 4: 0.09593424201011658
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.98 seconds!
[tester] 
SST2Metric: acc=0.8509174311926605, hinge=0.7198830573657237, ce=0.3863454044352226
Local test acc @ epoch 44: 0.8509
Client 1 execute local training on 8 samples...
Local loss @ local epoch 0: 0.01110167894512415
Local loss @ local epoch 1: 0.009748308919370174
Local loss @ local epoch 2: 0.00906207226216793
Local loss @ local epoch 3: 0.00838509015738964
Local loss @ local epoch 4: 0.007666806224733591
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.96 seconds!
[tester] 
SST2Metric: acc=0.8440366972477065, hinge=0.7995514227162808, ce=0.43872180499495306
Local test acc @ epoch 44: 0.844
Client 5 execute local training on 8 samples...
Local loss @ local epoch 0: 0.10218752920627594
Local loss @ local epoch 1: 0.06637964397668839
Local loss @ local epoch 2: 0.04372616484761238
Local loss @ local epoch 3: 0.030087905004620552
Local loss @ local epoch 4: 0.022346047684550285
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.02 seconds!
[tester] 
SST2Metric: acc=0.8555045871559633, hinge=0.7192942824112166, ce=0.378298492055523
Local test acc @ epoch 44: 0.8555
Client 0 execute local training on 8 samples...
Local loss @ local epoch 0: 0.046274181455373764
Local loss @ local epoch 1: 0.03324819356203079
Local loss @ local epoch 2: 0.025665929540991783
Local loss @ local epoch 3: 0.02221664786338806
Local loss @ local epoch 4: 0.02143719606101513
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.93 seconds!
[tester] 
SST2Metric: acc=0.8474770642201835, hinge=0.7435233515063557, ce=0.40392497336115996
Local test acc @ epoch 44: 0.8475
Global evaluate on test data...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.85 seconds!
[tester] 
SST2Metric: acc=0.8038990825688074, hinge=0.9034163755014402, ce=0.4463984367412028
Global test acc @ epoch 44: 0.8039
Global epoch 45...
Client 5 execute local training on 8 samples...
Local loss @ local epoch 0: 0.040764663368463516
Local loss @ local epoch 1: 0.027405425906181335
Local loss @ local epoch 2: 0.020094186067581177
Local loss @ local epoch 3: 0.016862673684954643
Local loss @ local epoch 4: 0.01626027375459671
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.91 seconds!
[tester] 
SST2Metric: acc=0.8394495412844036, hinge=0.8063536275417433, ce=0.4361209409106352
Local test acc @ epoch 45: 0.8394
Client 6 execute local training on 8 samples...
Local loss @ local epoch 0: 0.14274194836616516
Local loss @ local epoch 1: 0.12212777882814407
Local loss @ local epoch 2: 0.11601941287517548
Local loss @ local epoch 3: 0.11429709941148758
Local loss @ local epoch 4: 0.11070477962493896
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.9 seconds!
[tester] 
SST2Metric: acc=0.856651376146789, hinge=0.7005696583778487, ce=0.3689998935549221
Local test acc @ epoch 45: 0.8567
Client 2 execute local training on 8 samples...
Local loss @ local epoch 0: 0.13502490520477295
Local loss @ local epoch 1: 0.0922478511929512
Local loss @ local epoch 2: 0.06445083022117615
Local loss @ local epoch 3: 0.04748377948999405
Local loss @ local epoch 4: 0.03800731897354126
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.03 seconds!
[tester] 
SST2Metric: acc=0.8325688073394495, hinge=0.8728643327678015, ce=0.48992128546110464
Local test acc @ epoch 45: 0.8326
Client 9 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0583171583712101
Local loss @ local epoch 1: 0.04004029929637909
Local loss @ local epoch 2: 0.0300412867218256
Local loss @ local epoch 3: 0.025746440514922142
Local loss @ local epoch 4: 0.024870919063687325
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.02 seconds!
[tester] 
SST2Metric: acc=0.8520642201834863, hinge=0.7076677066197089, ce=0.37521558762912494
Local test acc @ epoch 45: 0.8521
Client 8 execute local training on 8 samples...
Local loss @ local epoch 0: 0.05675303936004639
Local loss @ local epoch 1: 0.03932686895132065
Local loss @ local epoch 2: 0.02928251028060913
Local loss @ local epoch 3: 0.024373650550842285
Local loss @ local epoch 4: 0.022762006148695946
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.01 seconds!
[tester] 
SST2Metric: acc=0.8405963302752294, hinge=0.8482608385042313, ce=0.47743808599231646
Local test acc @ epoch 45: 0.8406
Client 7 execute local training on 8 samples...
Local loss @ local epoch 0: 0.1453297883272171
Local loss @ local epoch 1: 0.10122569650411606
Local loss @ local epoch 2: 0.07302860915660858
Local loss @ local epoch 3: 0.05679989233613014
Local loss @ local epoch 4: 0.049156323075294495
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.01 seconds!
[tester] 
SST2Metric: acc=0.8474770642201835, hinge=0.7200542067715882, ce=0.38224979964674477
Local test acc @ epoch 45: 0.8475
Client 3 execute local training on 8 samples...
Local loss @ local epoch 0: 0.15247297286987305
Local loss @ local epoch 1: 0.13392090797424316
Local loss @ local epoch 2: 0.1273934543132782
Local loss @ local epoch 3: 0.1280043125152588
Local loss @ local epoch 4: 0.12858787178993225
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.94 seconds!
[tester] 
SST2Metric: acc=0.8394495412844036, hinge=0.8038612200852928, ce=0.4361568525040505
Local test acc @ epoch 45: 0.8394
Client 1 execute local training on 8 samples...
Local loss @ local epoch 0: 0.050773702561855316
Local loss @ local epoch 1: 0.03354059159755707
Local loss @ local epoch 2: 0.022630099207162857
Local loss @ local epoch 3: 0.016106372699141502
Local loss @ local epoch 4: 0.012473823502659798
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.0 seconds!
[tester] 
SST2Metric: acc=0.8532110091743119, hinge=0.7240401007440112, ce=0.38878958867948266
Local test acc @ epoch 45: 0.8532
Client 4 execute local training on 8 samples...
Local loss @ local epoch 0: 0.1077064797282219
Local loss @ local epoch 1: 0.0941484197974205
Local loss @ local epoch 2: 0.08322267234325409
Local loss @ local epoch 3: 0.07394948601722717
Local loss @ local epoch 4: 0.06611146032810211
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.93 seconds!
[tester] 
SST2Metric: acc=0.8520642201834863, hinge=0.7200395433454339, ce=0.38745241912098927
Local test acc @ epoch 45: 0.8521
Client 0 execute local training on 8 samples...
Local loss @ local epoch 0: 0.026595409959554672
Local loss @ local epoch 1: 0.023623619228601456
Local loss @ local epoch 2: 0.021384000778198242
Local loss @ local epoch 3: 0.019584419205784798
Local loss @ local epoch 4: 0.018184907734394073
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.13 seconds!
[tester] 
SST2Metric: acc=0.8520642201834863, hinge=0.7136600342912411, ce=0.3815755444138798
Local test acc @ epoch 45: 0.8521
Global evaluate on test data...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.46 seconds!
[tester] 
SST2Metric: acc=0.8096330275229358, hinge=0.8910537055872996, ce=0.4366503452728374
Global test acc @ epoch 45: 0.8096
Global epoch 46...
Client 5 execute local training on 8 samples...
Local loss @ local epoch 0: 0.030861351639032364
Local loss @ local epoch 1: 0.021377630531787872
Local loss @ local epoch 2: 0.016664300113916397
Local loss @ local epoch 3: 0.015153074637055397
Local loss @ local epoch 4: 0.015498688444495201
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.98 seconds!
[tester] 
SST2Metric: acc=0.8360091743119266, hinge=0.8176230267373794, ce=0.44525695059858605
Local test acc @ epoch 46: 0.836
Client 8 execute local training on 8 samples...
Local loss @ local epoch 0: 0.2021806687116623
Local loss @ local epoch 1: 0.1401073932647705
Local loss @ local epoch 2: 0.09593206644058228
Local loss @ local epoch 3: 0.06607107073068619
Local loss @ local epoch 4: 0.04674360901117325
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.98 seconds!
[tester] 
SST2Metric: acc=0.8555045871559633, hinge=0.716831197022298, ce=0.38341659173762443
Local test acc @ epoch 46: 0.8555
Client 2 execute local training on 8 samples...
Local loss @ local epoch 0: 0.06759404391050339
Local loss @ local epoch 1: 0.0510198138654232
Local loss @ local epoch 2: 0.042847052216529846
Local loss @ local epoch 3: 0.0397862084209919
Local loss @ local epoch 4: 0.03884510695934296
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.99 seconds!
[tester] 
SST2Metric: acc=0.823394495412844, hinge=0.9217837474761753, ce=0.5272608470287037
Local test acc @ epoch 46: 0.8234
Client 9 execute local training on 8 samples...
Local loss @ local epoch 0: 0.06369464844465256
Local loss @ local epoch 1: 0.04244468733668327
Local loss @ local epoch 2: 0.029922908172011375
Local loss @ local epoch 3: 0.023522166535258293
Local loss @ local epoch 4: 0.02113228291273117
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.06 seconds!
[tester] 
SST2Metric: acc=0.8555045871559633, hinge=0.7180222823805765, ce=0.3834507082324099
Local test acc @ epoch 46: 0.8555
Client 1 execute local training on 8 samples...
Local loss @ local epoch 0: 0.014333092607557774
Local loss @ local epoch 1: 0.012094447389245033
Local loss @ local epoch 2: 0.011130917817354202
Local loss @ local epoch 3: 0.010402804240584373
Local loss @ local epoch 4: 0.00958254374563694
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.93 seconds!
[tester] 
SST2Metric: acc=0.8440366972477065, hinge=0.7734347341257498, ce=0.42583209351388684
Local test acc @ epoch 46: 0.844
Client 7 execute local training on 8 samples...
Local loss @ local epoch 0: 0.07944681495428085
Local loss @ local epoch 1: 0.061309847980737686
Local loss @ local epoch 2: 0.05367439240217209
Local loss @ local epoch 3: 0.052374839782714844
Local loss @ local epoch 4: 0.053094130009412766
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.87 seconds!
[tester] 
SST2Metric: acc=0.8463302752293578, hinge=0.7169035033348503, ce=0.3797858441803912
Local test acc @ epoch 46: 0.8463
Client 0 execute local training on 8 samples...
Local loss @ local epoch 0: 0.023904496803879738
Local loss @ local epoch 1: 0.01889987662434578
Local loss @ local epoch 2: 0.01729363016784191
Local loss @ local epoch 3: 0.017544090747833252
Local loss @ local epoch 4: 0.01793152652680874
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.92 seconds!
[tester] 
SST2Metric: acc=0.8532110091743119, hinge=0.7314791738166722, ce=0.3952997360493851
Local test acc @ epoch 46: 0.8532
Client 6 execute local training on 8 samples...
Local loss @ local epoch 0: 0.23571810126304626
Local loss @ local epoch 1: 0.18108434975147247
Local loss @ local epoch 2: 0.14305292069911957
Local loss @ local epoch 3: 0.12149962037801743
Local loss @ local epoch 4: 0.11383975297212601
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.98 seconds!
[tester] 
SST2Metric: acc=0.8463302752293578, hinge=0.7626049655293106, ce=0.41026126178528327
Local test acc @ epoch 46: 0.8463
Client 4 execute local training on 8 samples...
Local loss @ local epoch 0: 0.19648805260658264
Local loss @ local epoch 1: 0.1460319459438324
Local loss @ local epoch 2: 0.11184454709291458
Local loss @ local epoch 3: 0.09199757128953934
Local loss @ local epoch 4: 0.08274818956851959
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.03 seconds!
[tester] 
SST2Metric: acc=0.8532110091743119, hinge=0.7302502706783627, ce=0.39678142282154855
Local test acc @ epoch 46: 0.8532
Client 3 execute local training on 8 samples...
Local loss @ local epoch 0: 0.17368702590465546
Local loss @ local epoch 1: 0.14656560122966766
Local loss @ local epoch 2: 0.13327805697917938
Local loss @ local epoch 3: 0.13044802844524384
Local loss @ local epoch 4: 0.13193313777446747
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.18 seconds!
[tester] 
SST2Metric: acc=0.8509174311926605, hinge=0.7376165472039389, ce=0.3914772345675924
Local test acc @ epoch 46: 0.8509
Global evaluate on test data...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.79 seconds!
[tester] 
SST2Metric: acc=0.8061926605504587, hinge=0.893327103978997, ce=0.4390617537423285
Global test acc @ epoch 46: 0.8062
Global epoch 47...
Client 6 execute local training on 8 samples...
Local loss @ local epoch 0: 0.14740456640720367
Local loss @ local epoch 1: 0.11990741640329361
Local loss @ local epoch 2: 0.10787421464920044
Local loss @ local epoch 3: 0.10587158799171448
Local loss @ local epoch 4: 0.10596533119678497
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.85 seconds!
[tester] 
SST2Metric: acc=0.8520642201834863, hinge=0.740599008328324, ce=0.3936257018203582
Local test acc @ epoch 47: 0.8521
Client 7 execute local training on 8 samples...
Local loss @ local epoch 0: 0.09783192723989487
Local loss @ local epoch 1: 0.0730234757065773
Local loss @ local epoch 2: 0.05869848281145096
Local loss @ local epoch 3: 0.05270717293024063
Local loss @ local epoch 4: 0.052126444876194
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.85 seconds!
[tester] 
SST2Metric: acc=0.8520642201834863, hinge=0.7185403563834112, ce=0.3870695398483904
Local test acc @ epoch 47: 0.8521
Client 8 execute local training on 8 samples...
Local loss @ local epoch 0: 0.04175703227519989
Local loss @ local epoch 1: 0.030446793884038925
Local loss @ local epoch 2: 0.0247952863574028
Local loss @ local epoch 3: 0.022813070565462112
Local loss @ local epoch 4: 0.022637156769633293
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.84 seconds!
[tester] 
SST2Metric: acc=0.8314220183486238, hinge=0.8931162915098558, ce=0.5114221346303991
Local test acc @ epoch 47: 0.8314
Client 4 execute local training on 8 samples...
Local loss @ local epoch 0: 0.25454196333885193
Local loss @ local epoch 1: 0.18860377371311188
Local loss @ local epoch 2: 0.14386174082756042
Local loss @ local epoch 3: 0.11719880253076553
Local loss @ local epoch 4: 0.10371603071689606
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.83 seconds!
[tester] 
SST2Metric: acc=0.8532110091743119, hinge=0.7069662321324742, ce=0.378475399924989
Local test acc @ epoch 47: 0.8532
Client 3 execute local training on 8 samples...
Local loss @ local epoch 0: 0.15832257270812988
Local loss @ local epoch 1: 0.1366911381483078
Local loss @ local epoch 2: 0.12763071060180664
Local loss @ local epoch 3: 0.12729282677173615
Local loss @ local epoch 4: 0.1289045363664627
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.86 seconds!
[tester] 
SST2Metric: acc=0.8463302752293578, hinge=0.7672908072351315, ce=0.41278262817463196
Local test acc @ epoch 47: 0.8463
Client 5 execute local training on 8 samples...
Local loss @ local epoch 0: 0.014914875850081444
Local loss @ local epoch 1: 0.014096477068960667
Local loss @ local epoch 2: 0.013202684000134468
Local loss @ local epoch 3: 0.012453566305339336
Local loss @ local epoch 4: 0.011775394901633263
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.74 seconds!
[tester] 
SST2Metric: acc=0.841743119266055, hinge=0.759028602904136, ce=0.40927144465968013
Local test acc @ epoch 47: 0.8417
Client 9 execute local training on 8 samples...
Local loss @ local epoch 0: 0.050876837223768234
Local loss @ local epoch 1: 0.03511789068579674
Local loss @ local epoch 2: 0.026446901261806488
Local loss @ local epoch 3: 0.02265557087957859
Local loss @ local epoch 4: 0.02185358665883541
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.7 seconds!
[tester] 
SST2Metric: acc=0.8497706422018348, hinge=0.7359297959082717, ce=0.4013118984972323
Local test acc @ epoch 47: 0.8498
Client 1 execute local training on 8 samples...
Local loss @ local epoch 0: 0.011113502085208893
Local loss @ local epoch 1: 0.009952114894986153
Local loss @ local epoch 2: 0.00900108739733696
Local loss @ local epoch 3: 0.008118056692183018
Local loss @ local epoch 4: 0.007337520364671946
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.75 seconds!
[tester] 
SST2Metric: acc=0.8497706422018348, hinge=0.7641416697873982, ce=0.42323594019158717
Local test acc @ epoch 47: 0.8498
Client 2 execute local training on 8 samples...
Local loss @ local epoch 0: 0.05356668308377266
Local loss @ local epoch 1: 0.03898105397820473
Local loss @ local epoch 2: 0.03135323524475098
Local loss @ local epoch 3: 0.028510715812444687
Local loss @ local epoch 4: 0.02842971496284008
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.91 seconds!
[tester] 
SST2Metric: acc=0.7958715596330275, hinge=1.1271142194030481, ce=0.6645130570294386
Local test acc @ epoch 47: 0.7959
Client 0 execute local training on 8 samples...
Local loss @ local epoch 0: 0.07614791393280029
Local loss @ local epoch 1: 0.050485000014305115
Local loss @ local epoch 2: 0.0345466285943985
Local loss @ local epoch 3: 0.02529350481927395
Local loss @ local epoch 4: 0.020468950271606445
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.02 seconds!
[tester] 
SST2Metric: acc=0.8497706422018348, hinge=0.7435282898058585, ce=0.408607640466012
Local test acc @ epoch 47: 0.8498
Global evaluate on test data...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.41 seconds!
[tester] 
SST2Metric: acc=0.801605504587156, hinge=0.9002726000109944, ce=0.4492185295499656
Global test acc @ epoch 47: 0.8016
Global epoch 48...
Client 7 execute local training on 8 samples...
Local loss @ local epoch 0: 0.051706042140722275
Local loss @ local epoch 1: 0.0476541593670845
Local loss @ local epoch 2: 0.04603207856416702
Local loss @ local epoch 3: 0.04377255588769913
Local loss @ local epoch 4: 0.04136371240019798
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.88 seconds!
[tester] 
SST2Metric: acc=0.8543577981651376, hinge=0.7130892663647276, ce=0.3814887240563237
Local test acc @ epoch 48: 0.8544
Client 3 execute local training on 8 samples...
Local loss @ local epoch 0: 0.14742295444011688
Local loss @ local epoch 1: 0.12762564420700073
Local loss @ local epoch 2: 0.11984884738922119
Local loss @ local epoch 3: 0.12015623599290848
Local loss @ local epoch 4: 0.12160547822713852
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.99 seconds!
[tester] 
SST2Metric: acc=0.8394495412844036, hinge=0.8009275269070897, ce=0.43795991096850656
Local test acc @ epoch 48: 0.8394
Client 6 execute local training on 8 samples...
Local loss @ local epoch 0: 0.13349105417728424
Local loss @ local epoch 1: 0.11728397011756897
Local loss @ local epoch 2: 0.11281295865774155
Local loss @ local epoch 3: 0.10979905724525452
Local loss @ local epoch 4: 0.10458610951900482
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.83 seconds!
[tester] 
SST2Metric: acc=0.8532110091743119, hinge=0.7042151459860145, ce=0.37454590967796536
Local test acc @ epoch 48: 0.8532
Client 2 execute local training on 8 samples...
Local loss @ local epoch 0: 0.14446596801280975
Local loss @ local epoch 1: 0.09843947738409042
Local loss @ local epoch 2: 0.0681799128651619
Local loss @ local epoch 3: 0.0493597574532032
Local loss @ local epoch 4: 0.03844853863120079
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.95 seconds!
[tester] 
SST2Metric: acc=0.838302752293578, hinge=0.8430873240352771, ce=0.47540022161524365
Local test acc @ epoch 48: 0.8383
Client 4 execute local training on 8 samples...
Local loss @ local epoch 0: 0.1375350058078766
Local loss @ local epoch 1: 0.10465848445892334
Local loss @ local epoch 2: 0.08646875619888306
Local loss @ local epoch 3: 0.07782081514596939
Local loss @ local epoch 4: 0.0735040232539177
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.98 seconds!
[tester] 
SST2Metric: acc=0.8509174311926605, hinge=0.7046477504826467, ce=0.37840961023242375
Local test acc @ epoch 48: 0.8509
Client 5 execute local training on 8 samples...
Local loss @ local epoch 0: 0.02660449780523777
Local loss @ local epoch 1: 0.018198439851403236
Local loss @ local epoch 2: 0.014002878218889236
Local loss @ local epoch 3: 0.01259033102542162
Local loss @ local epoch 4: 0.012794402427971363
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.91 seconds!
[tester] 
SST2Metric: acc=0.8405963302752294, hinge=0.7914544457415922, ce=0.42952428187816105
Local test acc @ epoch 48: 0.8406
Client 1 execute local training on 8 samples...
Local loss @ local epoch 0: 0.04783690720796585
Local loss @ local epoch 1: 0.03146318346261978
Local loss @ local epoch 2: 0.02083984762430191
Local loss @ local epoch 3: 0.01424901932477951
Local loss @ local epoch 4: 0.010324656963348389
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.99 seconds!
[tester] 
SST2Metric: acc=0.8577981651376146, hinge=0.7144465751330787, ce=0.3879751917466077
Local test acc @ epoch 48: 0.8578
Client 8 execute local training on 8 samples...
Local loss @ local epoch 0: 0.06753040850162506
Local loss @ local epoch 1: 0.047313228249549866
Local loss @ local epoch 2: 0.03556149825453758
Local loss @ local epoch 3: 0.029712477698922157
Local loss @ local epoch 4: 0.027551261708140373
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.92 seconds!
[tester] 
SST2Metric: acc=0.8256880733944955, hinge=0.9326515886761727, ce=0.537200605458315
Local test acc @ epoch 48: 0.8257
Client 0 execute local training on 8 samples...
Local loss @ local epoch 0: 0.04079931229352951
Local loss @ local epoch 1: 0.028156716376543045
Local loss @ local epoch 2: 0.021206965669989586
Local loss @ local epoch 3: 0.018146879971027374
Local loss @ local epoch 4: 0.017555199563503265
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.97 seconds!
[tester] 
SST2Metric: acc=0.8520642201834863, hinge=0.7075396545162989, ce=0.3809058882975353
Local test acc @ epoch 48: 0.8521
Client 9 execute local training on 8 samples...
Local loss @ local epoch 0: 0.025023382157087326
Local loss @ local epoch 1: 0.02255421131849289
Local loss @ local epoch 2: 0.02043072320520878
Local loss @ local epoch 3: 0.018569884821772575
Local loss @ local epoch 4: 0.017000071704387665
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.87 seconds!
[tester] 
SST2Metric: acc=0.8577981651376146, hinge=0.72643699886602, ce=0.39550775930140203
Local test acc @ epoch 48: 0.8578
Global evaluate on test data...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.47 seconds!
[tester] 
SST2Metric: acc=0.8130733944954128, hinge=0.8744318438779324, ce=0.42915471424476814
Global test acc @ epoch 48: 0.8131
Global epoch 49...
Client 4 execute local training on 8 samples...
Local loss @ local epoch 0: 0.09682583808898926
Local loss @ local epoch 1: 0.0826665386557579
Local loss @ local epoch 2: 0.07384207844734192
Local loss @ local epoch 3: 0.06656549870967865
Local loss @ local epoch 4: 0.05985017120838165
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.08 seconds!
[tester] 
SST2Metric: acc=0.856651376146789, hinge=0.7343714297364611, ce=0.40200807583313264
Local test acc @ epoch 49: 0.8567
Client 0 execute local training on 8 samples...
Local loss @ local epoch 0: 0.025280963629484177
Local loss @ local epoch 1: 0.02248373255133629
Local loss @ local epoch 2: 0.020379802212119102
Local loss @ local epoch 3: 0.018680710345506668
Local loss @ local epoch 4: 0.01735125668346882
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.96 seconds!
[tester] 
SST2Metric: acc=0.8520642201834863, hinge=0.7208724240644262, ce=0.39152553789274885
Local test acc @ epoch 49: 0.8521
Client 5 execute local training on 8 samples...
Local loss @ local epoch 0: 0.03251064196228981
Local loss @ local epoch 1: 0.021424084901809692
Local loss @ local epoch 2: 0.015186277218163013
Local loss @ local epoch 3: 0.012150193564593792
Local loss @ local epoch 4: 0.011196278035640717
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.06 seconds!
[tester] 
SST2Metric: acc=0.8440366972477065, hinge=0.7777027500058533, ce=0.4230077320350966
Local test acc @ epoch 49: 0.844
Client 9 execute local training on 8 samples...
Local loss @ local epoch 0: 0.06170104071497917
Local loss @ local epoch 1: 0.0418989397585392
Local loss @ local epoch 2: 0.030294789001345634
Local loss @ local epoch 3: 0.024427616968750954
Local loss @ local epoch 4: 0.022309331223368645
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.94 seconds!
[tester] 
SST2Metric: acc=0.8509174311926605, hinge=0.7231998863296771, ce=0.3930878457741453
Local test acc @ epoch 49: 0.8509
Client 6 execute local training on 8 samples...
Local loss @ local epoch 0: 0.19233238697052002
Local loss @ local epoch 1: 0.14726689457893372
Local loss @ local epoch 2: 0.11813585460186005
Local loss @ local epoch 3: 0.10376336425542831
Local loss @ local epoch 4: 0.10051161795854568
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.95 seconds!
[tester] 
SST2Metric: acc=0.8486238532110092, hinge=0.7457222744412378, ce=0.40139398742210836
Local test acc @ epoch 49: 0.8486
Client 7 execute local training on 8 samples...
Local loss @ local epoch 0: 0.07778209447860718
Local loss @ local epoch 1: 0.0574536956846714
Local loss @ local epoch 2: 0.04603687673807144
Local loss @ local epoch 3: 0.041504569351673126
Local loss @ local epoch 4: 0.04139630123972893
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.97 seconds!
[tester] 
SST2Metric: acc=0.8497706422018348, hinge=0.7164307806743394, ce=0.3886309062822833
Local test acc @ epoch 49: 0.8498
Client 1 execute local training on 8 samples...
Local loss @ local epoch 0: 0.010451816953718662
Local loss @ local epoch 1: 0.009099649265408516
Local loss @ local epoch 2: 0.008454366587102413
Local loss @ local epoch 3: 0.007841396145522594
Local loss @ local epoch 4: 0.007170549593865871
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.77 seconds!
[tester] 
SST2Metric: acc=0.8486238532110092, hinge=0.7743971715553091, ce=0.43187419453276954
Local test acc @ epoch 49: 0.8486
Client 8 execute local training on 8 samples...
Local loss @ local epoch 0: 0.03674381598830223
Local loss @ local epoch 1: 0.029050230979919434
Local loss @ local epoch 2: 0.025835752487182617
Local loss @ local epoch 3: 0.024646630510687828
Local loss @ local epoch 4: 0.023722685873508453
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.74 seconds!
[tester] 
SST2Metric: acc=0.8279816513761468, hinge=0.9206549510223057, ce=0.534117057105663
Local test acc @ epoch 49: 0.828
Client 3 execute local training on 8 samples...
Local loss @ local epoch 0: 0.36079153418540955
Local loss @ local epoch 1: 0.2927505373954773
Local loss @ local epoch 2: 0.23588716983795166
Local loss @ local epoch 3: 0.19009332358837128
Local loss @ local epoch 4: 0.1556822806596756
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.79 seconds!
[tester] 
SST2Metric: acc=0.8577981651376146, hinge=0.7037799425354792, ce=0.37895223437466685
Local test acc @ epoch 49: 0.8578
Client 2 execute local training on 8 samples...
Local loss @ local epoch 0: 0.08669181168079376
Local loss @ local epoch 1: 0.05981743335723877
Local loss @ local epoch 2: 0.0435023307800293
Local loss @ local epoch 3: 0.034616611897945404
Local loss @ local epoch 4: 0.0306938998401165
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.78 seconds!
[tester] 
SST2Metric: acc=0.8314220183486238, hinge=0.8939376883550522, ce=0.514342161850286
Local test acc @ epoch 49: 0.8314
Global evaluate on test data...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.35 seconds!
[tester] 
SST2Metric: acc=0.7993119266055045, hinge=0.8938882277099365, ce=0.44792966689880287
Global test acc @ epoch 49: 0.7993
Global epoch 50...
Client 8 execute local training on 8 samples...
Local loss @ local epoch 0: 0.04302084445953369
Local loss @ local epoch 1: 0.03102889657020569
Local loss @ local epoch 2: 0.024832677096128464
Local loss @ local epoch 3: 0.02245115116238594
Local loss @ local epoch 4: 0.022080812603235245
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.93 seconds!
[tester] 
SST2Metric: acc=0.8268348623853211, hinge=0.8943031185537303, ce=0.5160979388597841
Local test acc @ epoch 50: 0.8268
Client 2 execute local training on 8 samples...
Local loss @ local epoch 0: 0.03737335279583931
Local loss @ local epoch 1: 0.03304034471511841
Local loss @ local epoch 2: 0.031140387058258057
Local loss @ local epoch 3: 0.02947056107223034
Local loss @ local epoch 4: 0.027631938457489014
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.86 seconds!
[tester] 
SST2Metric: acc=0.8268348623853211, hinge=0.931885443969604, ce=0.5366305116000115
Local test acc @ epoch 50: 0.8268
Client 0 execute local training on 8 samples...
Local loss @ local epoch 0: 0.03717297315597534
Local loss @ local epoch 1: 0.025684241205453873
Local loss @ local epoch 2: 0.019508887082338333
Local loss @ local epoch 3: 0.016889607533812523
Local loss @ local epoch 4: 0.016419917345046997
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.85 seconds!
[tester] 
SST2Metric: acc=0.8509174311926605, hinge=0.7101411768876085, ce=0.3837577820212982
Local test acc @ epoch 50: 0.8509
Client 7 execute local training on 8 samples...
Local loss @ local epoch 0: 0.04505705088376999
Local loss @ local epoch 1: 0.04173653572797775
Local loss @ local epoch 2: 0.03896576166152954
Local loss @ local epoch 3: 0.03664696216583252
Local loss @ local epoch 4: 0.03467889130115509
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.9 seconds!
[tester] 
SST2Metric: acc=0.8474770642201835, hinge=0.7239125681306244, ce=0.3913295779765985
Local test acc @ epoch 50: 0.8475
Client 3 execute local training on 8 samples...
Local loss @ local epoch 0: 0.142437145113945
Local loss @ local epoch 1: 0.12204267084598541
Local loss @ local epoch 2: 0.11341337114572525
Local loss @ local epoch 3: 0.1132872998714447
Local loss @ local epoch 4: 0.11506202816963196
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.96 seconds!
[tester] 
SST2Metric: acc=0.8428899082568807, hinge=0.8203421203641716, ce=0.4554892120532555
Local test acc @ epoch 50: 0.8429
Client 1 execute local training on 8 samples...
Local loss @ local epoch 0: 0.04528803005814552
Local loss @ local epoch 1: 0.02970978058874607
Local loss @ local epoch 2: 0.019862236455082893
Local loss @ local epoch 3: 0.013939700089395046
Local loss @ local epoch 4: 0.010577980428934097
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.92 seconds!
[tester] 
SST2Metric: acc=0.8543577981651376, hinge=0.7167639186896315, ce=0.38957674783406726
Local test acc @ epoch 50: 0.8544
Client 5 execute local training on 8 samples...
Local loss @ local epoch 0: 0.026776760816574097
Local loss @ local epoch 1: 0.017858270555734634
Local loss @ local epoch 2: 0.012946429662406445
Local loss @ local epoch 3: 0.010702002793550491
Local loss @ local epoch 4: 0.01018508244305849
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.93 seconds!
[tester] 
SST2Metric: acc=0.8405963302752294, hinge=0.7973042577505112, ce=0.44008487955573095
Local test acc @ epoch 50: 0.8406
Client 9 execute local training on 8 samples...
Local loss @ local epoch 0: 0.061752356588840485
Local loss @ local epoch 1: 0.04142928496003151
Local loss @ local epoch 2: 0.029284298419952393
Local loss @ local epoch 3: 0.022812463343143463
Local loss @ local epoch 4: 0.02009556069970131
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.89 seconds!
[tester] 
SST2Metric: acc=0.8543577981651376, hinge=0.7230449677060503, ce=0.3943962739255074
Local test acc @ epoch 50: 0.8544
Client 4 execute local training on 8 samples...
Local loss @ local epoch 0: 0.11362379044294357
Local loss @ local epoch 1: 0.09787600487470627
Local loss @ local epoch 2: 0.08464936912059784
Local loss @ local epoch 3: 0.07362920045852661
Local loss @ local epoch 4: 0.06451385468244553
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.9 seconds!
[tester] 
SST2Metric: acc=0.8555045871559633, hinge=0.7107222899111039, ce=0.38882850530053226
Local test acc @ epoch 50: 0.8555
Client 6 execute local training on 8 samples...
Local loss @ local epoch 0: 0.11211609095335007
Local loss @ local epoch 1: 0.09073798358440399
Local loss @ local epoch 2: 0.08216002583503723
Local loss @ local epoch 3: 0.08113111555576324
Local loss @ local epoch 4: 0.08104805648326874
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.96 seconds!
[tester] 
SST2Metric: acc=0.8543577981651376, hinge=0.7074525208648191, ce=0.37988286288326967
Local test acc @ epoch 50: 0.8544
Global evaluate on test data...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.26 seconds!
[tester] 
SST2Metric: acc=0.8130733944954128, hinge=0.8754277478117461, ce=0.4334078208210135
Global test acc @ epoch 50: 0.8131
Global epoch 51...
Client 5 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0208465363830328
Local loss @ local epoch 1: 0.014398298226296902
Local loss @ local epoch 2: 0.011213128454983234
Local loss @ local epoch 3: 0.010201659984886646
Local loss @ local epoch 4: 0.01044770609587431
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.91 seconds!
[tester] 
SST2Metric: acc=0.841743119266055, hinge=0.8046076962160408, ce=0.444573683570216
Local test acc @ epoch 51: 0.8417
Client 2 execute local training on 8 samples...
Local loss @ local epoch 0: 0.3033961355686188
Local loss @ local epoch 1: 0.2135259509086609
Local loss @ local epoch 2: 0.14812234044075012
Local loss @ local epoch 3: 0.10286197811365128
Local loss @ local epoch 4: 0.07286745309829712
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.93 seconds!
[tester] 
SST2Metric: acc=0.8520642201834863, hinge=0.7210982969594658, ce=0.3931566609898497
Local test acc @ epoch 51: 0.8521
Client 3 execute local training on 8 samples...
Local loss @ local epoch 0: 0.16758832335472107
Local loss @ local epoch 1: 0.13619112968444824
Local loss @ local epoch 2: 0.11676546186208725
Local loss @ local epoch 3: 0.10880239307880402
Local loss @ local epoch 4: 0.10912233591079712
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.9 seconds!
[tester] 
SST2Metric: acc=0.8486238532110092, hinge=0.7694027248872529, ce=0.4191366835634028
Local test acc @ epoch 51: 0.8486
Client 1 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0316094346344471
Local loss @ local epoch 1: 0.020700139924883842
Local loss @ local epoch 2: 0.013995167799293995
Local loss @ local epoch 3: 0.010110466741025448
Local loss @ local epoch 4: 0.008040189743041992
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.98 seconds!
[tester] 
SST2Metric: acc=0.8509174311926605, hinge=0.7318741650482931, ce=0.40170818054194163
Local test acc @ epoch 51: 0.8509
Client 7 execute local training on 8 samples...
Local loss @ local epoch 0: 0.045587774366140366
Local loss @ local epoch 1: 0.03958117589354515
Local loss @ local epoch 2: 0.03832368552684784
Local loss @ local epoch 3: 0.037563156336545944
Local loss @ local epoch 4: 0.03594178706407547
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.15 seconds!
[tester] 
SST2Metric: acc=0.8577981651376146, hinge=0.7122692483554193, ce=0.38323865251598555
Local test acc @ epoch 51: 0.8578
Client 9 execute local training on 8 samples...
Local loss @ local epoch 0: 0.02636166475713253
Local loss @ local epoch 1: 0.021524706855416298
Local loss @ local epoch 2: 0.020026303827762604
Local loss @ local epoch 3: 0.019398504868149757
Local loss @ local epoch 4: 0.01839078962802887
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.95 seconds!
[tester] 
SST2Metric: acc=0.8520642201834863, hinge=0.7350807880316306, ce=0.4046376918254574
Local test acc @ epoch 51: 0.8521
Client 6 execute local training on 8 samples...
Local loss @ local epoch 0: 0.25808587670326233
Local loss @ local epoch 1: 0.19884683191776276
Local loss @ local epoch 2: 0.15530623495578766
Local loss @ local epoch 3: 0.1279079020023346
Local loss @ local epoch 4: 0.11511948704719543
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.88 seconds!
[tester] 
SST2Metric: acc=0.8428899082568807, hinge=0.763410443696407, ce=0.4181985613727652
Local test acc @ epoch 51: 0.8429
Client 8 execute local training on 8 samples...
Local loss @ local epoch 0: 0.11296554654836655
Local loss @ local epoch 1: 0.07595832645893097
Local loss @ local epoch 2: 0.051383621990680695
Local loss @ local epoch 3: 0.03582920879125595
Local loss @ local epoch 4: 0.02645193040370941
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.78 seconds!
[tester] 
SST2Metric: acc=0.8497706422018348, hinge=0.7382535860625976, ce=0.40745064836217587
Local test acc @ epoch 51: 0.8498
Client 4 execute local training on 8 samples...
Local loss @ local epoch 0: 0.13533218204975128
Local loss @ local epoch 1: 0.11540670692920685
Local loss @ local epoch 2: 0.10119915008544922
Local loss @ local epoch 3: 0.08920557796955109
Local loss @ local epoch 4: 0.078599713742733
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.77 seconds!
[tester] 
SST2Metric: acc=0.8532110091743119, hinge=0.7033140811078046, ce=0.3831265449220605
Local test acc @ epoch 51: 0.8532
Client 0 execute local training on 8 samples...
Local loss @ local epoch 0: 0.01800113543868065
Local loss @ local epoch 1: 0.015943406149744987
Local loss @ local epoch 2: 0.01564081199467182
Local loss @ local epoch 3: 0.015262098982930183
Local loss @ local epoch 4: 0.014516931027173996
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.77 seconds!
[tester] 
SST2Metric: acc=0.8509174311926605, hinge=0.727811351555203, ce=0.3995945293845079
Local test acc @ epoch 51: 0.8509
Global evaluate on test data...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.1 seconds!
[tester] 
SST2Metric: acc=0.8084862385321101, hinge=0.8749121856798819, ce=0.43248852520087444
Global test acc @ epoch 51: 0.8085
Global epoch 52...
Client 1 execute local training on 8 samples...
Local loss @ local epoch 0: 0.011378840543329716
Local loss @ local epoch 1: 0.008753066882491112
Local loss @ local epoch 2: 0.007689585909247398
Local loss @ local epoch 3: 0.007351938635110855
Local loss @ local epoch 4: 0.007141161244362593
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.8 seconds!
[tester] 
SST2Metric: acc=0.8497706422018348, hinge=0.7875513557447206, ce=0.4436956532812173
Local test acc @ epoch 52: 0.8498
Client 4 execute local training on 8 samples...
Local loss @ local epoch 0: 0.09926791489124298
Local loss @ local epoch 1: 0.0779419019818306
Local loss @ local epoch 2: 0.06741877645254135
Local loss @ local epoch 3: 0.0624711737036705
Local loss @ local epoch 4: 0.059001192450523376
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.77 seconds!
[tester] 
SST2Metric: acc=0.8509174311926605, hinge=0.7011157174449448, ce=0.38054097951347127
Local test acc @ epoch 52: 0.8509
Client 5 execute local training on 8 samples...
Local loss @ local epoch 0: 0.02083558589220047
Local loss @ local epoch 1: 0.01435845997184515
Local loss @ local epoch 2: 0.011233311146497726
Local loss @ local epoch 3: 0.010293303057551384
Local loss @ local epoch 4: 0.010552305728197098
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.77 seconds!
[tester] 
SST2Metric: acc=0.8440366972477065, hinge=0.7886236408434877, ce=0.43248725579111674
Local test acc @ epoch 52: 0.844
Client 3 execute local training on 8 samples...
Local loss @ local epoch 0: 0.16074000298976898
Local loss @ local epoch 1: 0.1333356350660324
Local loss @ local epoch 2: 0.11668163537979126
Local loss @ local epoch 3: 0.11055091768503189
Local loss @ local epoch 4: 0.11154092848300934
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.69 seconds!
[tester] 
SST2Metric: acc=0.8555045871559633, hinge=0.7007091134239775, ce=0.3784373785740773
Local test acc @ epoch 52: 0.8555
Client 9 execute local training on 8 samples...
Local loss @ local epoch 0: 0.026736734434962273
Local loss @ local epoch 1: 0.023560570552945137
Local loss @ local epoch 2: 0.021791309118270874
Local loss @ local epoch 3: 0.019930237904191017
Local loss @ local epoch 4: 0.018075259402394295
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.65 seconds!
[tester] 
SST2Metric: acc=0.8589449541284404, hinge=0.719124116481991, ce=0.39375171210796256
Local test acc @ epoch 52: 0.8589
Client 7 execute local training on 8 samples...
Local loss @ local epoch 0: 0.03626703843474388
Local loss @ local epoch 1: 0.031477123498916626
Local loss @ local epoch 2: 0.031033838167786598
Local loss @ local epoch 3: 0.0311494842171669
Local loss @ local epoch 4: 0.030219824984669685
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.73 seconds!
[tester] 
SST2Metric: acc=0.8451834862385321, hinge=0.7363871851372062, ce=0.4028051706253115
Local test acc @ epoch 52: 0.8452
Client 2 execute local training on 8 samples...
Local loss @ local epoch 0: 0.13794061541557312
Local loss @ local epoch 1: 0.09568245708942413
Local loss @ local epoch 2: 0.06789467483758926
Local loss @ local epoch 3: 0.051080167293548584
Local loss @ local epoch 4: 0.041931089013814926
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.75 seconds!
[tester] 
SST2Metric: acc=0.8451834862385321, hinge=0.7906878654836514, ce=0.44843785210358783
Local test acc @ epoch 52: 0.8452
Client 8 execute local training on 8 samples...
Local loss @ local epoch 0: 0.018937449902296066
Local loss @ local epoch 1: 0.01726452447474003
Local loss @ local epoch 2: 0.015798818320035934
Local loss @ local epoch 3: 0.01448508445173502
Local loss @ local epoch 4: 0.013315938413143158
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.66 seconds!
[tester] 
SST2Metric: acc=0.8451834862385321, hinge=0.778276672877303, ce=0.43959642354176814
Local test acc @ epoch 52: 0.8452
Client 6 execute local training on 8 samples...
Local loss @ local epoch 0: 0.4101595878601074
Local loss @ local epoch 1: 0.3275873064994812
Local loss @ local epoch 2: 0.2573074400424957
Local loss @ local epoch 3: 0.2016657590866089
Local loss @ local epoch 4: 0.16204026341438293
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.7 seconds!
[tester] 
SST2Metric: acc=0.8405963302752294, hinge=0.7677115687263121, ce=0.4269819281089607
Local test acc @ epoch 52: 0.8406
Client 0 execute local training on 8 samples...
Local loss @ local epoch 0: 0.019034864380955696
Local loss @ local epoch 1: 0.014837534166872501
Local loss @ local epoch 2: 0.013441993854939938
Local loss @ local epoch 3: 0.013655023649334908
Local loss @ local epoch 4: 0.014090975746512413
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.75 seconds!
[tester] 
SST2Metric: acc=0.8589449541284404, hinge=0.7413498900078852, ce=0.4100919028556128
Local test acc @ epoch 52: 0.8589
Global evaluate on test data...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.29 seconds!
[tester] 
SST2Metric: acc=0.8073394495412844, hinge=0.885749362204053, ce=0.4450945528736765
Global test acc @ epoch 52: 0.8073
Global epoch 53...
Client 2 execute local training on 8 samples...
Local loss @ local epoch 0: 0.06681499630212784
Local loss @ local epoch 1: 0.04757697880268097
Local loss @ local epoch 2: 0.036879535764455795
Local loss @ local epoch 3: 0.031963981688022614
Local loss @ local epoch 4: 0.03045675903558731
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.67 seconds!
[tester] 
SST2Metric: acc=0.8325688073394495, hinge=0.8926098257850069, ce=0.5185113806307419
Local test acc @ epoch 53: 0.8326
Client 5 execute local training on 8 samples...
Local loss @ local epoch 0: 0.10817324370145798
Local loss @ local epoch 1: 0.06994485855102539
Local loss @ local epoch 2: 0.04548691585659981
Local loss @ local epoch 3: 0.030230680480599403
Local loss @ local epoch 4: 0.020837055519223213
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.68 seconds!
[tester] 
SST2Metric: acc=0.8589449541284404, hinge=0.7098548633789797, ce=0.3862875299036947
Local test acc @ epoch 53: 0.8589
Client 3 execute local training on 8 samples...
Local loss @ local epoch 0: 0.1379013955593109
Local loss @ local epoch 1: 0.11643728613853455
Local loss @ local epoch 2: 0.10669031739234924
Local loss @ local epoch 3: 0.10593155771493912
Local loss @ local epoch 4: 0.10816091299057007
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.61 seconds!
[tester] 
SST2Metric: acc=0.8428899082568807, hinge=0.7914721721082653, ce=0.43819034151698744
Local test acc @ epoch 53: 0.8429
Client 7 execute local training on 8 samples...
Local loss @ local epoch 0: 0.059702638536691666
Local loss @ local epoch 1: 0.04339182749390602
Local loss @ local epoch 2: 0.034292060881853104
Local loss @ local epoch 3: 0.03064429946243763
Local loss @ local epoch 4: 0.03058713488280773
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.72 seconds!
[tester] 
SST2Metric: acc=0.8589449541284404, hinge=0.7142300060309401, ce=0.3896101409795805
Local test acc @ epoch 53: 0.8589
Client 6 execute local training on 8 samples...
Local loss @ local epoch 0: 0.18107551336288452
Local loss @ local epoch 1: 0.14213089644908905
Local loss @ local epoch 2: 0.11949267238378525
Local loss @ local epoch 3: 0.11092759668827057
Local loss @ local epoch 4: 0.11079968512058258
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.61 seconds!
[tester] 
SST2Metric: acc=0.8497706422018348, hinge=0.769839980596796, ce=0.4229806042618093
Local test acc @ epoch 53: 0.8498
Client 1 execute local training on 8 samples...
Local loss @ local epoch 0: 0.026551906019449234
Local loss @ local epoch 1: 0.017311718314886093
Local loss @ local epoch 2: 0.011707865633070469
Local loss @ local epoch 3: 0.008501483127474785
Local loss @ local epoch 4: 0.00681933993473649
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.64 seconds!
[tester] 
SST2Metric: acc=0.8520642201834863, hinge=0.7276689224286911, ce=0.4018178603338471
Local test acc @ epoch 53: 0.8521
Client 8 execute local training on 8 samples...
Local loss @ local epoch 0: 0.035314347594976425
Local loss @ local epoch 1: 0.025858860462903976
Local loss @ local epoch 2: 0.02120843157172203
Local loss @ local epoch 3: 0.019582604989409447
Local loss @ local epoch 4: 0.01935475505888462
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.65 seconds!
[tester] 
SST2Metric: acc=0.8302752293577982, hinge=0.9011953289902538, ce=0.5256075770337852
Local test acc @ epoch 53: 0.8303
Client 0 execute local training on 8 samples...
Local loss @ local epoch 0: 0.03596482798457146
Local loss @ local epoch 1: 0.024509070441126823
Local loss @ local epoch 2: 0.017925741150975227
Local loss @ local epoch 3: 0.014704592525959015
Local loss @ local epoch 4: 0.01373223029077053
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.66 seconds!
[tester] 
SST2Metric: acc=0.8589449541284404, hinge=0.7116427696352705, ce=0.389153936046919
Local test acc @ epoch 53: 0.8589
Client 9 execute local training on 8 samples...
Local loss @ local epoch 0: 0.02004520408809185
Local loss @ local epoch 1: 0.0181074570864439
Local loss @ local epoch 2: 0.01646552048623562
Local loss @ local epoch 3: 0.014965914189815521
Local loss @ local epoch 4: 0.013711158186197281
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.62 seconds!
[tester] 
SST2Metric: acc=0.8623853211009175, hinge=0.7348156780551333, ce=0.4067854005074419
Local test acc @ epoch 53: 0.8624
Client 4 execute local training on 8 samples...
Local loss @ local epoch 0: 0.1532076895236969
Local loss @ local epoch 1: 0.1318514198064804
Local loss @ local epoch 2: 0.11361332982778549
Local loss @ local epoch 3: 0.09821944683790207
Local loss @ local epoch 4: 0.08535772562026978
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.6 seconds!
[tester] 
SST2Metric: acc=0.8555045871559633, hinge=0.7107953129831804, ce=0.3913043341663487
Local test acc @ epoch 53: 0.8555
Global evaluate on test data...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.0 seconds!
[tester] 
SST2Metric: acc=0.8107798165137615, hinge=0.8917252701630286, ce=0.45089899268377265
Global test acc @ epoch 53: 0.8108
Global epoch 54...
Client 9 execute local training on 8 samples...
Local loss @ local epoch 0: 0.018988661468029022
Local loss @ local epoch 1: 0.017034202814102173
Local loss @ local epoch 2: 0.015358127653598785
Local loss @ local epoch 3: 0.013923394493758678
Local loss @ local epoch 4: 0.012693989090621471
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.58 seconds!
[tester] 
SST2Metric: acc=0.8589449541284404, hinge=0.7384221419828747, ce=0.4098276645628684
Local test acc @ epoch 54: 0.8589
Client 1 execute local training on 8 samples...
Local loss @ local epoch 0: 0.01090051420032978
Local loss @ local epoch 1: 0.00962704885751009
Local loss @ local epoch 2: 0.008520107716321945
Local loss @ local epoch 3: 0.007560208905488253
Local loss @ local epoch 4: 0.00673053041100502
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.71 seconds!
[tester] 
SST2Metric: acc=0.8532110091743119, hinge=0.7360948089065902, ce=0.41008814998449533
Local test acc @ epoch 54: 0.8532
Client 6 execute local training on 8 samples...
Local loss @ local epoch 0: 0.18772165477275848
Local loss @ local epoch 1: 0.14209148287773132
Local loss @ local epoch 2: 0.11169885098934174
Local loss @ local epoch 3: 0.09549155086278915
Local loss @ local epoch 4: 0.09050077944993973
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.6 seconds!
[tester] 
SST2Metric: acc=0.8520642201834863, hinge=0.7351326329992451, ce=0.4015331731002709
Local test acc @ epoch 54: 0.8521
Client 4 execute local training on 8 samples...
Local loss @ local epoch 0: 0.13844016194343567
Local loss @ local epoch 1: 0.10042107850313187
Local loss @ local epoch 2: 0.07610267400741577
Local loss @ local epoch 3: 0.06290461868047714
Local loss @ local epoch 4: 0.057173337787389755
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.57 seconds!
[tester] 
SST2Metric: acc=0.856651376146789, hinge=0.7401440704087598, ce=0.41477163925707133
Local test acc @ epoch 54: 0.8567
Client 7 execute local training on 8 samples...
Local loss @ local epoch 0: 0.041899655014276505
Local loss @ local epoch 1: 0.03774840012192726
Local loss @ local epoch 2: 0.035690147429704666
Local loss @ local epoch 3: 0.033503420650959015
Local loss @ local epoch 4: 0.03126876428723335
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.55 seconds!
[tester] 
SST2Metric: acc=0.8623853211009175, hinge=0.6998992789229121, ce=0.3842161119244822
Local test acc @ epoch 54: 0.8624
Client 3 execute local training on 8 samples...
Local loss @ local epoch 0: 0.13109742105007172
Local loss @ local epoch 1: 0.11605218052864075
Local loss @ local epoch 2: 0.11198464781045914
Local loss @ local epoch 3: 0.11262080073356628
Local loss @ local epoch 4: 0.11193758994340897
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.65 seconds!
[tester] 
SST2Metric: acc=0.8497706422018348, hinge=0.7484259558927029, ce=0.41101393594466357
Local test acc @ epoch 54: 0.8498
Client 0 execute local training on 8 samples...
Local loss @ local epoch 0: 0.03959779441356659
Local loss @ local epoch 1: 0.027259157970547676
Local loss @ local epoch 2: 0.01961430162191391
Local loss @ local epoch 3: 0.015492765232920647
Local loss @ local epoch 4: 0.013862508349120617
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.69 seconds!
[tester] 
SST2Metric: acc=0.8532110091743119, hinge=0.7297193100419613, ce=0.40578331224569075
Local test acc @ epoch 54: 0.8532
Client 2 execute local training on 8 samples...
Local loss @ local epoch 0: 0.04150504991412163
Local loss @ local epoch 1: 0.03200404345989227
Local loss @ local epoch 2: 0.02788510173559189
Local loss @ local epoch 3: 0.026758721098303795
Local loss @ local epoch 4: 0.02649860456585884
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.63 seconds!
[tester] 
SST2Metric: acc=0.8268348623853211, hinge=0.9579520486636993, ce=0.5655578555722952
Local test acc @ epoch 54: 0.8268
Client 8 execute local training on 8 samples...
Local loss @ local epoch 0: 0.021390199661254883
Local loss @ local epoch 1: 0.0181676484644413
Local loss @ local epoch 2: 0.016956549137830734
Local loss @ local epoch 3: 0.016057081520557404
Local loss @ local epoch 4: 0.014951691031455994
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.64 seconds!
[tester] 
SST2Metric: acc=0.8428899082568807, hinge=0.7977141649077791, ce=0.457293447856834
Local test acc @ epoch 54: 0.8429
Client 5 execute local training on 8 samples...
Local loss @ local epoch 0: 0.06658052653074265
Local loss @ local epoch 1: 0.04286474734544754
Local loss @ local epoch 2: 0.028059452772140503
Local loss @ local epoch 3: 0.019053565338253975
Local loss @ local epoch 4: 0.013718693517148495
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.66 seconds!
[tester] 
SST2Metric: acc=0.8440366972477065, hinge=0.7366170835330945, ce=0.4055060122606404
Local test acc @ epoch 54: 0.844
Global evaluate on test data...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.25 seconds!
[tester] 
SST2Metric: acc=0.8084862385321101, hinge=0.8952001305895114, ce=0.45607343039244685
Global test acc @ epoch 54: 0.8085
Global epoch 55...
Client 2 execute local training on 8 samples...
Local loss @ local epoch 0: 0.05219429358839989
Local loss @ local epoch 1: 0.03763189539313316
Local loss @ local epoch 2: 0.029911838471889496
Local loss @ local epoch 3: 0.026757681742310524
Local loss @ local epoch 4: 0.026161525398492813
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.77 seconds!
[tester] 
SST2Metric: acc=0.8302752293577982, hinge=0.9307166016703352, ce=0.5477763378999085
Local test acc @ epoch 55: 0.8303
Client 1 execute local training on 8 samples...
Local loss @ local epoch 0: 0.01205013319849968
Local loss @ local epoch 1: 0.00834282673895359
Local loss @ local epoch 2: 0.006232132203876972
Local loss @ local epoch 3: 0.005205289926379919
Local loss @ local epoch 4: 0.004855143837630749
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.86 seconds!
[tester] 
SST2Metric: acc=0.8577981651376146, hinge=0.7212773621629137, ce=0.40099620384830603
Local test acc @ epoch 55: 0.8578
Client 6 execute local training on 8 samples...
Local loss @ local epoch 0: 0.12996742129325867
Local loss @ local epoch 1: 0.100566066801548
Local loss @ local epoch 2: 0.0851113423705101
Local loss @ local epoch 3: 0.0805521011352539
Local loss @ local epoch 4: 0.08145637065172195
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.79 seconds!
[tester] 
SST2Metric: acc=0.8532110091743119, hinge=0.7241422886695337, ce=0.3940939191011114
Local test acc @ epoch 55: 0.8532
Client 8 execute local training on 8 samples...
Local loss @ local epoch 0: 0.17951112985610962
Local loss @ local epoch 1: 0.12419720739126205
Local loss @ local epoch 2: 0.08496539294719696
Local loss @ local epoch 3: 0.05846818909049034
Local loss @ local epoch 4: 0.04127902537584305
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.78 seconds!
[tester] 
SST2Metric: acc=0.8509174311926605, hinge=0.7664262335234826, ce=0.43394788493265596
Local test acc @ epoch 55: 0.8509
Client 0 execute local training on 8 samples...
Local loss @ local epoch 0: 0.017444608733057976
Local loss @ local epoch 1: 0.015380386263132095
Local loss @ local epoch 2: 0.014647407457232475
Local loss @ local epoch 3: 0.013918725773692131
Local loss @ local epoch 4: 0.013095106929540634
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.81 seconds!
[tester] 
SST2Metric: acc=0.8555045871559633, hinge=0.7210076093126875, ce=0.3992146472126211
Local test acc @ epoch 55: 0.8555
Client 9 execute local training on 8 samples...
Local loss @ local epoch 0: 0.01607039012014866
Local loss @ local epoch 1: 0.01443467102944851
Local loss @ local epoch 2: 0.01303362101316452
Local loss @ local epoch 3: 0.01183442585170269
Local loss @ local epoch 4: 0.010805674828588963
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.74 seconds!
[tester] 
SST2Metric: acc=0.856651376146789, hinge=0.7451950101130599, ce=0.4179857892300421
Local test acc @ epoch 55: 0.8567
Client 7 execute local training on 8 samples...
Local loss @ local epoch 0: 0.04224972799420357
Local loss @ local epoch 1: 0.03150765597820282
Local loss @ local epoch 2: 0.026488061994314194
Local loss @ local epoch 3: 0.02547573111951351
Local loss @ local epoch 4: 0.026514779776334763
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.76 seconds!
[tester] 
SST2Metric: acc=0.8394495412844036, hinge=0.7855610727170191, ce=0.4437527213343889
Local test acc @ epoch 55: 0.8394
Client 3 execute local training on 8 samples...
Local loss @ local epoch 0: 0.1215105652809143
Local loss @ local epoch 1: 0.11303167045116425
Local loss @ local epoch 2: 0.1116328313946724
Local loss @ local epoch 3: 0.10955949872732162
Local loss @ local epoch 4: 0.1057070717215538
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.84 seconds!
[tester] 
SST2Metric: acc=0.8428899082568807, hinge=0.806787988051362, ce=0.4561064204782111
Local test acc @ epoch 55: 0.8429
Client 4 execute local training on 8 samples...
Local loss @ local epoch 0: 0.26045942306518555
Local loss @ local epoch 1: 0.199250265955925
Local loss @ local epoch 2: 0.15170922875404358
Local loss @ local epoch 3: 0.11808594316244125
Local loss @ local epoch 4: 0.0971883162856102
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.92 seconds!
[tester] 
SST2Metric: acc=0.8555045871559633, hinge=0.7212048644593002, ce=0.40097864443402087
Local test acc @ epoch 55: 0.8555
Client 5 execute local training on 8 samples...
Local loss @ local epoch 0: 0.021589986979961395
Local loss @ local epoch 1: 0.014400487765669823
Local loss @ local epoch 2: 0.01043221727013588
Local loss @ local epoch 3: 0.008603818714618683
Local loss @ local epoch 4: 0.00816167239099741
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.87 seconds!
[tester] 
SST2Metric: acc=0.8451834862385321, hinge=0.7892836563357519, ce=0.4426065073775794
Local test acc @ epoch 55: 0.8452
Global evaluate on test data...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.22 seconds!
[tester] 
SST2Metric: acc=0.8073394495412844, hinge=0.8844470313382805, ce=0.4468379175768943
Global test acc @ epoch 55: 0.8073
Global epoch 56...
Client 8 execute local training on 8 samples...
Local loss @ local epoch 0: 0.04221547394990921
Local loss @ local epoch 1: 0.028706630691885948
Local loss @ local epoch 2: 0.02079188823699951
Local loss @ local epoch 3: 0.016690321266651154
Local loss @ local epoch 4: 0.015056479722261429
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.81 seconds!
[tester] 
SST2Metric: acc=0.841743119266055, hinge=0.8446044767395072, ce=0.4929160989571038
Local test acc @ epoch 56: 0.8417
Client 6 execute local training on 8 samples...
Local loss @ local epoch 0: 0.39261654019355774
Local loss @ local epoch 1: 0.3097565770149231
Local loss @ local epoch 2: 0.2389332801103592
Local loss @ local epoch 3: 0.18214675784111023
Local loss @ local epoch 4: 0.14035098254680634
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.9 seconds!
[tester] 
SST2Metric: acc=0.8543577981651376, hinge=0.7147826917401148, ce=0.3929577173084157
Local test acc @ epoch 56: 0.8544
Client 3 execute local training on 8 samples...
Local loss @ local epoch 0: 0.12339569628238678
Local loss @ local epoch 1: 0.1103244423866272
Local loss @ local epoch 2: 0.10751233994960785
Local loss @ local epoch 3: 0.1080864816904068
Local loss @ local epoch 4: 0.10670555382966995
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.92 seconds!
[tester] 
SST2Metric: acc=0.8486238532110092, hinge=0.7691698216517037, ce=0.4271247258426946
Local test acc @ epoch 56: 0.8486
Client 7 execute local training on 8 samples...
Local loss @ local epoch 0: 0.044701237231492996
Local loss @ local epoch 1: 0.03334776684641838
Local loss @ local epoch 2: 0.027786917984485626
Local loss @ local epoch 3: 0.02637121081352234
Local loss @ local epoch 4: 0.027169276028871536
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.87 seconds!
[tester] 
SST2Metric: acc=0.8589449541284404, hinge=0.7199046580343071, ce=0.3983969873740175
Local test acc @ epoch 56: 0.8589
Client 5 execute local training on 8 samples...
Local loss @ local epoch 0: 0.01872200518846512
Local loss @ local epoch 1: 0.01276762131601572
Local loss @ local epoch 2: 0.009625096805393696
Local loss @ local epoch 3: 0.008372854441404343
Local loss @ local epoch 4: 0.008304430171847343
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.85 seconds!
[tester] 
SST2Metric: acc=0.841743119266055, hinge=0.8145650046954461, ce=0.4612007618480183
Local test acc @ epoch 56: 0.8417
Client 4 execute local training on 8 samples...
Local loss @ local epoch 0: 0.24193139374256134
Local loss @ local epoch 1: 0.18186844885349274
Local loss @ local epoch 2: 0.13648545742034912
Local loss @ local epoch 3: 0.10547837615013123
Local loss @ local epoch 4: 0.08701814711093903
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.89 seconds!
[tester] 
SST2Metric: acc=0.8543577981651376, hinge=0.7134798356425871, ce=0.39549213390874277
Local test acc @ epoch 56: 0.8544
Client 2 execute local training on 8 samples...
Local loss @ local epoch 0: 0.05305837094783783
Local loss @ local epoch 1: 0.037874091416597366
Local loss @ local epoch 2: 0.0296502485871315
Local loss @ local epoch 3: 0.026107273995876312
Local loss @ local epoch 4: 0.025292154401540756
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.01 seconds!
[tester] 
SST2Metric: acc=0.8291284403669725, hinge=0.9246489642136687, ce=0.5458507413405567
Local test acc @ epoch 56: 0.8291
Client 9 execute local training on 8 samples...
Local loss @ local epoch 0: 0.05174742266535759
Local loss @ local epoch 1: 0.033887673169374466
Local loss @ local epoch 2: 0.023262381553649902
Local loss @ local epoch 3: 0.01753017120063305
Local loss @ local epoch 4: 0.014935831539332867
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.84 seconds!
[tester] 
SST2Metric: acc=0.8612385321100917, hinge=0.7196917913922476, ce=0.39905651015819077
Local test acc @ epoch 56: 0.8612
Client 0 execute local training on 8 samples...
Local loss @ local epoch 0: 0.01229775045067072
Local loss @ local epoch 1: 0.011778543703258038
Local loss @ local epoch 2: 0.011170531623065472
Local loss @ local epoch 3: 0.010684886015951633
Local loss @ local epoch 4: 0.010228274390101433
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.85 seconds!
[tester] 
SST2Metric: acc=0.8600917431192661, hinge=0.7315233605443885, ce=0.40581697255689136
Local test acc @ epoch 56: 0.8601
Client 1 execute local training on 8 samples...
Local loss @ local epoch 0: 0.008207355625927448
Local loss @ local epoch 1: 0.006946898065507412
Local loss @ local epoch 2: 0.0063809179700911045
Local loss @ local epoch 3: 0.0059256707318127155
Local loss @ local epoch 4: 0.005423134192824364
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.93 seconds!
[tester] 
SST2Metric: acc=0.8497706422018348, hinge=0.7611545631918338, ce=0.4305883301422
Local test acc @ epoch 56: 0.8498
Global evaluate on test data...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.57 seconds!
[tester] 
SST2Metric: acc=0.8107798165137615, hinge=0.900737891913554, ce=0.46182085803513406
Global test acc @ epoch 56: 0.8108
Global epoch 57...
Client 3 execute local training on 8 samples...
Local loss @ local epoch 0: 0.1467619389295578
Local loss @ local epoch 1: 0.11951620876789093
Local loss @ local epoch 2: 0.10365916043519974
Local loss @ local epoch 3: 0.0983145460486412
Local loss @ local epoch 4: 0.09976888447999954
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.88 seconds!
[tester] 
SST2Metric: acc=0.8474770642201835, hinge=0.7857761862901372, ce=0.43859259307880055
Local test acc @ epoch 57: 0.8475
Client 9 execute local training on 8 samples...
Local loss @ local epoch 0: 0.05015494301915169
Local loss @ local epoch 1: 0.03345073387026787
Local loss @ local epoch 2: 0.023471610620617867
Local loss @ local epoch 3: 0.01809442974627018
Local loss @ local epoch 4: 0.015729650855064392
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.81 seconds!
[tester] 
SST2Metric: acc=0.8577981651376146, hinge=0.7294470049919338, ce=0.4061838460001035
Local test acc @ epoch 57: 0.8578
Client 6 execute local training on 8 samples...
Local loss @ local epoch 0: 0.181338369846344
Local loss @ local epoch 1: 0.13832315802574158
Local loss @ local epoch 2: 0.11074990779161453
Local loss @ local epoch 3: 0.09725409746170044
Local loss @ local epoch 4: 0.09421723335981369
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.88 seconds!
[tester] 
SST2Metric: acc=0.8497706422018348, hinge=0.7522618636625622, ce=0.4148191661774736
Local test acc @ epoch 57: 0.8498
Client 0 execute local training on 8 samples...
Local loss @ local epoch 0: 0.03502984344959259
Local loss @ local epoch 1: 0.023983515799045563
Local loss @ local epoch 2: 0.017208276316523552
Local loss @ local epoch 3: 0.013577085919678211
Local loss @ local epoch 4: 0.012131568975746632
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.88 seconds!
[tester] 
SST2Metric: acc=0.8520642201834863, hinge=0.7388589022629851, ce=0.4134297875277356
Local test acc @ epoch 57: 0.8521
Client 7 execute local training on 8 samples...
Local loss @ local epoch 0: 0.034477412700653076
Local loss @ local epoch 1: 0.028498290106654167
Local loss @ local epoch 2: 0.02706148475408554
Local loss @ local epoch 3: 0.027248868718743324
Local loss @ local epoch 4: 0.026941806077957153
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.95 seconds!
[tester] 
SST2Metric: acc=0.8486238532110092, hinge=0.7298561792581453, ce=0.40216952761199426
Local test acc @ epoch 57: 0.8486
Client 5 execute local training on 8 samples...
Local loss @ local epoch 0: 0.009734437800943851
Local loss @ local epoch 1: 0.008201094344258308
Local loss @ local epoch 2: 0.008042000234127045
Local loss @ local epoch 3: 0.008061273023486137
Local loss @ local epoch 4: 0.007744539063423872
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.91 seconds!
[tester] 
SST2Metric: acc=0.8440366972477065, hinge=0.7851598188964599, ce=0.44071518586640923
Local test acc @ epoch 57: 0.844
Client 2 execute local training on 8 samples...
Local loss @ local epoch 0: 0.20435525476932526
Local loss @ local epoch 1: 0.1407102793455124
Local loss @ local epoch 2: 0.0963740423321724
Local loss @ local epoch 3: 0.06698647886514664
Local loss @ local epoch 4: 0.04835358262062073
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.88 seconds!
[tester] 
SST2Metric: acc=0.8520642201834863, hinge=0.7418672069223649, ce=0.4168233794390882
Local test acc @ epoch 57: 0.8521
Client 1 execute local training on 8 samples...
Local loss @ local epoch 0: 0.006376347038894892
Local loss @ local epoch 1: 0.005676925182342529
Local loss @ local epoch 2: 0.005064715631306171
Local loss @ local epoch 3: 0.004531329497694969
Local loss @ local epoch 4: 0.004067982081323862
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.88 seconds!
[tester] 
SST2Metric: acc=0.8520642201834863, hinge=0.7589454307742075, ce=0.43122894486087726
Local test acc @ epoch 57: 0.8521
Client 8 execute local training on 8 samples...
Local loss @ local epoch 0: 0.03288160264492035
Local loss @ local epoch 1: 0.023895539343357086
Local loss @ local epoch 2: 0.01932855322957039
Local loss @ local epoch 3: 0.017531434074044228
Local loss @ local epoch 4: 0.017063258215785027
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.92 seconds!
[tester] 
SST2Metric: acc=0.8291284403669725, hinge=0.9603614741509113, ce=0.5733358015716418
Local test acc @ epoch 57: 0.8291
Client 4 execute local training on 8 samples...
Local loss @ local epoch 0: 0.20802085101604462
Local loss @ local epoch 1: 0.1490195393562317
Local loss @ local epoch 2: 0.10865247249603271
Local loss @ local epoch 3: 0.08377736061811447
Local loss @ local epoch 4: 0.07039935141801834
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.98 seconds!
[tester] 
SST2Metric: acc=0.8589449541284404, hinge=0.7244635593453679, ce=0.40675127032477787
Local test acc @ epoch 57: 0.8589
Global evaluate on test data...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.43 seconds!
[tester] 
SST2Metric: acc=0.8119266055045872, hinge=0.8815566410712146, ce=0.446668763354489
Global test acc @ epoch 57: 0.8119
Global epoch 58...
Client 3 execute local training on 8 samples...
Local loss @ local epoch 0: 0.12005570530891418
Local loss @ local epoch 1: 0.10255047678947449
Local loss @ local epoch 2: 0.0962003692984581
Local loss @ local epoch 3: 0.09673888236284256
Local loss @ local epoch 4: 0.09794123470783234
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.91 seconds!
[tester] 
SST2Metric: acc=0.8474770642201835, hinge=0.7665810511199707, ce=0.4252063420626822
Local test acc @ epoch 58: 0.8475
Client 1 execute local training on 8 samples...
Local loss @ local epoch 0: 0.018975412473082542
Local loss @ local epoch 1: 0.012325194664299488
Local loss @ local epoch 2: 0.008362490683794022
Local loss @ local epoch 3: 0.006138133816421032
Local loss @ local epoch 4: 0.0050044069066643715
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.91 seconds!
[tester] 
SST2Metric: acc=0.8520642201834863, hinge=0.7400220184151186, ce=0.41564712178286745
Local test acc @ epoch 58: 0.8521
Client 6 execute local training on 8 samples...
Local loss @ local epoch 0: 0.1568344682455063
Local loss @ local epoch 1: 0.1181720569729805
Local loss @ local epoch 2: 0.09413024038076401
Local loss @ local epoch 3: 0.08292461931705475
Local loss @ local epoch 4: 0.08091897517442703
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.9 seconds!
[tester] 
SST2Metric: acc=0.8555045871559633, hinge=0.7324110355672486, ce=0.40147369355604123
Local test acc @ epoch 58: 0.8555
Client 5 execute local training on 8 samples...
Local loss @ local epoch 0: 0.008653669618070126
Local loss @ local epoch 1: 0.008081596344709396
Local loss @ local epoch 2: 0.007810813840478659
Local loss @ local epoch 3: 0.007336658891290426
Local loss @ local epoch 4: 0.006889583542943001
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.94 seconds!
[tester] 
SST2Metric: acc=0.8497706422018348, hinge=0.7440285137213698, ce=0.4105178085348031
Local test acc @ epoch 58: 0.8498
Client 9 execute local training on 8 samples...
Local loss @ local epoch 0: 0.039970893412828445
Local loss @ local epoch 1: 0.027749642729759216
Local loss @ local epoch 2: 0.02113405615091324
Local loss @ local epoch 3: 0.018286390230059624
Local loss @ local epoch 4: 0.017602846026420593
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.87 seconds!
[tester] 
SST2Metric: acc=0.8520642201834863, hinge=0.7411955372182601, ce=0.41875414520185955
Local test acc @ epoch 58: 0.8521
Client 4 execute local training on 8 samples...
Local loss @ local epoch 0: 0.08274877071380615
Local loss @ local epoch 1: 0.0711132064461708
Local loss @ local epoch 2: 0.06145042926073074
Local loss @ local epoch 3: 0.053447552025318146
Local loss @ local epoch 4: 0.046864017844200134
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.86 seconds!
[tester] 
SST2Metric: acc=0.8635321100917431, hinge=0.7351684972233729, ce=0.41591455962363666
Local test acc @ epoch 58: 0.8635
Client 7 execute local training on 8 samples...
Local loss @ local epoch 0: 0.03666564077138901
Local loss @ local epoch 1: 0.03310471400618553
Local loss @ local epoch 2: 0.03014637716114521
Local loss @ local epoch 3: 0.027690468356013298
Local loss @ local epoch 4: 0.025667699053883553
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.89 seconds!
[tester] 
SST2Metric: acc=0.8612385321100917, hinge=0.7078842128362131, ce=0.39470625878952514
Local test acc @ epoch 58: 0.8612
Client 2 execute local training on 8 samples...
Local loss @ local epoch 0: 0.07027103006839752
Local loss @ local epoch 1: 0.048065852373838425
Local loss @ local epoch 2: 0.034652866423130035
Local loss @ local epoch 3: 0.027318349108099937
Local loss @ local epoch 4: 0.024002760648727417
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.89 seconds!
[tester] 
SST2Metric: acc=0.838302752293578, hinge=0.9058416976567802, ce=0.5372463510534086
Local test acc @ epoch 58: 0.8383
Client 8 execute local training on 8 samples...
Local loss @ local epoch 0: 0.01832231879234314
Local loss @ local epoch 1: 0.016330644488334656
Local loss @ local epoch 2: 0.01507791131734848
Local loss @ local epoch 3: 0.013833669945597649
Local loss @ local epoch 4: 0.012655605562031269
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.96 seconds!
[tester] 
SST2Metric: acc=0.8486238532110092, hinge=0.8133006691932678, ce=0.4753339061942891
Local test acc @ epoch 58: 0.8486
Client 0 execute local training on 8 samples...
Local loss @ local epoch 0: 0.021075239405035973
Local loss @ local epoch 1: 0.015046990476548672
Local loss @ local epoch 2: 0.012051819823682308
Local loss @ local epoch 3: 0.011149327270686626
Local loss @ local epoch 4: 0.011466482654213905
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.83 seconds!
[tester] 
SST2Metric: acc=0.8589449541284404, hinge=0.7252819881253286, ce=0.40470685122786687
Local test acc @ epoch 58: 0.8589
Global evaluate on test data...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.23 seconds!
[tester] 
SST2Metric: acc=0.8153669724770642, hinge=0.8752436257830454, ce=0.44421645226555134
Global test acc @ epoch 58: 0.8154
Global epoch 59...
Client 9 execute local training on 8 samples...
Local loss @ local epoch 0: 0.01768070086836815
Local loss @ local epoch 1: 0.01574263721704483
Local loss @ local epoch 2: 0.014104340225458145
Local loss @ local epoch 3: 0.01267999317497015
Local loss @ local epoch 4: 0.0114749101921916
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.9 seconds!
[tester] 
SST2Metric: acc=0.8577981651376146, hinge=0.7341291185365905, ce=0.4128960952773656
Local test acc @ epoch 59: 0.8578
Client 1 execute local training on 8 samples...
Local loss @ local epoch 0: 0.006778302136808634
Local loss @ local epoch 1: 0.0060185883194208145
Local loss @ local epoch 2: 0.005428763572126627
Local loss @ local epoch 3: 0.004875163082033396
Local loss @ local epoch 4: 0.004378284327685833
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.88 seconds!
[tester] 
SST2Metric: acc=0.8532110091743119, hinge=0.7458548750899253, ce=0.42433542275263014
Local test acc @ epoch 59: 0.8532
Client 6 execute local training on 8 samples...
Local loss @ local epoch 0: 0.15864765644073486
Local loss @ local epoch 1: 0.11874394863843918
Local loss @ local epoch 2: 0.09306342154741287
Local loss @ local epoch 3: 0.08004164695739746
Local loss @ local epoch 4: 0.07656580954790115
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.98 seconds!
[tester] 
SST2Metric: acc=0.8520642201834863, hinge=0.7312691463242977, ce=0.40472127813546827
Local test acc @ epoch 59: 0.8521
Client 0 execute local training on 8 samples...
Local loss @ local epoch 0: 0.03548645228147507
Local loss @ local epoch 1: 0.024405140429735184
Local loss @ local epoch 2: 0.01764598675072193
Local loss @ local epoch 3: 0.014067492447793484
Local loss @ local epoch 4: 0.01267167553305626
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.86 seconds!
[tester] 
SST2Metric: acc=0.8520642201834863, hinge=0.7554873753031459, ce=0.43143942277374414
Local test acc @ epoch 59: 0.8521
Client 5 execute local training on 8 samples...
Local loss @ local epoch 0: 0.029530998319387436
Local loss @ local epoch 1: 0.018934359773993492
Local loss @ local epoch 2: 0.012637726031243801
Local loss @ local epoch 3: 0.009082658216357231
Local loss @ local epoch 4: 0.007267853245139122
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.88 seconds!
[tester] 
SST2Metric: acc=0.8509174311926605, hinge=0.7504171048282483, ce=0.41761214156430043
Local test acc @ epoch 59: 0.8509
Client 2 execute local training on 8 samples...
Local loss @ local epoch 0: 0.1757114678621292
Local loss @ local epoch 1: 0.11957496404647827
Local loss @ local epoch 2: 0.0813220888376236
Local loss @ local epoch 3: 0.05628484487533569
Local loss @ local epoch 4: 0.04042702913284302
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.86 seconds!
[tester] 
SST2Metric: acc=0.8532110091743119, hinge=0.7596906325139037, ce=0.4345937322722663
Local test acc @ epoch 59: 0.8532
Client 3 execute local training on 8 samples...
Local loss @ local epoch 0: 0.17583829164505005
Local loss @ local epoch 1: 0.13959406316280365
Local loss @ local epoch 2: 0.11491725593805313
Local loss @ local epoch 3: 0.10176990926265717
Local loss @ local epoch 4: 0.09826882928609848
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.96 seconds!
[tester] 
SST2Metric: acc=0.8497706422018348, hinge=0.7418041944230368, ce=0.4113244862667362
Local test acc @ epoch 59: 0.8498
Client 4 execute local training on 8 samples...
Local loss @ local epoch 0: 0.1371878683567047
Local loss @ local epoch 1: 0.09967104345560074
Local loss @ local epoch 2: 0.0753951221704483
Local loss @ local epoch 3: 0.06201624870300293
Local loss @ local epoch 4: 0.05600583553314209
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.95 seconds!
[tester] 
SST2Metric: acc=0.856651376146789, hinge=0.7476253283953448, ce=0.42588796788279754
Local test acc @ epoch 59: 0.8567
Client 7 execute local training on 8 samples...
Local loss @ local epoch 0: 0.032687000930309296
Local loss @ local epoch 1: 0.029218368232250214
Local loss @ local epoch 2: 0.027913622558116913
Local loss @ local epoch 3: 0.026488065719604492
Local loss @ local epoch 4: 0.024839984253048897
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.9 seconds!
[tester] 
SST2Metric: acc=0.8600917431192661, hinge=0.7101261666335097, ce=0.39618126896355266
Local test acc @ epoch 59: 0.8601
Client 8 execute local training on 8 samples...
Local loss @ local epoch 0: 0.04651930555701256
Local loss @ local epoch 1: 0.031199725344777107
Local loss @ local epoch 2: 0.021905504167079926
Local loss @ local epoch 3: 0.016713498160243034
Local loss @ local epoch 4: 0.01420988142490387
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.88 seconds!
[tester] 
SST2Metric: acc=0.8486238532110092, hinge=0.8438450990740313, ce=0.4976430070958177
Local test acc @ epoch 59: 0.8486
Global evaluate on test data...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.21 seconds!
[tester] 
SST2Metric: acc=0.8130733944954128, hinge=0.8857771819884624, ce=0.4539986612924605
Global test acc @ epoch 59: 0.8131
Global epoch 60...
Client 6 execute local training on 8 samples...
Local loss @ local epoch 0: 0.11911067366600037
Local loss @ local epoch 1: 0.09379029273986816
Local loss @ local epoch 2: 0.08163952082395554
Local loss @ local epoch 3: 0.07887810468673706
Local loss @ local epoch 4: 0.07958754152059555
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.81 seconds!
[tester] 
SST2Metric: acc=0.8532110091743119, hinge=0.7365552875973763, ce=0.4082287241354448
Local test acc @ epoch 60: 0.8532
Client 0 execute local training on 8 samples...
Local loss @ local epoch 0: 0.03811410069465637
Local loss @ local epoch 1: 0.026110099628567696
Local loss @ local epoch 2: 0.018632739782333374
Local loss @ local epoch 3: 0.014506080187857151
Local loss @ local epoch 4: 0.012702301144599915
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.86 seconds!
[tester] 
SST2Metric: acc=0.8543577981651376, hinge=0.7490895956481268, ce=0.42708300654092857
Local test acc @ epoch 60: 0.8544
Client 3 execute local training on 8 samples...
Local loss @ local epoch 0: 0.14616377651691437
Local loss @ local epoch 1: 0.11917444318532944
Local loss @ local epoch 2: 0.1046164259314537
Local loss @ local epoch 3: 0.10043957084417343
Local loss @ local epoch 4: 0.10195860266685486
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.91 seconds!
[tester] 
SST2Metric: acc=0.8509174311926605, hinge=0.7367122293612279, ce=0.40787032883563035
Local test acc @ epoch 60: 0.8509
Client 7 execute local training on 8 samples...
Local loss @ local epoch 0: 0.043107930570840836
Local loss @ local epoch 1: 0.03212640434503555
Local loss @ local epoch 2: 0.026691390201449394
Local loss @ local epoch 3: 0.02514670416712761
Local loss @ local epoch 4: 0.025618214160203934
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.89 seconds!
[tester] 
SST2Metric: acc=0.8532110091743119, hinge=0.7291730952645661, ce=0.41133936045054825
Local test acc @ epoch 60: 0.8532
Client 9 execute local training on 8 samples...
Local loss @ local epoch 0: 0.014160806313157082
Local loss @ local epoch 1: 0.01272668782621622
Local loss @ local epoch 2: 0.01153513789176941
Local loss @ local epoch 3: 0.010446794331073761
Local loss @ local epoch 4: 0.00952472910284996
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.91 seconds!
[tester] 
SST2Metric: acc=0.8612385321100917, hinge=0.7463497073825346, ce=0.4239955472636011
Local test acc @ epoch 60: 0.8612
Client 1 execute local training on 8 samples...
Local loss @ local epoch 0: 0.006055040750652552
Local loss @ local epoch 1: 0.0053606522269546986
Local loss @ local epoch 2: 0.004762081895023584
Local loss @ local epoch 3: 0.004239256028085947
Local loss @ local epoch 4: 0.0037882907781749964
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.92 seconds!
[tester] 
SST2Metric: acc=0.8532110091743119, hinge=0.7494128418898364, ce=0.4290493244642751
Local test acc @ epoch 60: 0.8532
Client 4 execute local training on 8 samples...
Local loss @ local epoch 0: 0.08329629898071289
Local loss @ local epoch 1: 0.07170876115560532
Local loss @ local epoch 2: 0.06230613961815834
Local loss @ local epoch 3: 0.05442734435200691
Local loss @ local epoch 4: 0.04788178205490112
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.86 seconds!
[tester] 
SST2Metric: acc=0.8577981651376146, hinge=0.7382517908144435, ce=0.42180475452914834
Local test acc @ epoch 60: 0.8578
Client 5 execute local training on 8 samples...
Local loss @ local epoch 0: 0.025300659239292145
Local loss @ local epoch 1: 0.016260698437690735
Local loss @ local epoch 2: 0.011057891882956028
Local loss @ local epoch 3: 0.008308173157274723
Local loss @ local epoch 4: 0.0071160257793962955
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.87 seconds!
[tester] 
SST2Metric: acc=0.8589449541284404, hinge=0.752408730600952, ce=0.4211109846578313
Local test acc @ epoch 60: 0.8589
Client 2 execute local training on 8 samples...
Local loss @ local epoch 0: 0.2690596878528595
Local loss @ local epoch 1: 0.18793876469135284
Local loss @ local epoch 2: 0.12963545322418213
Local loss @ local epoch 3: 0.08941735327243805
Local loss @ local epoch 4: 0.062475234270095825
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.9 seconds!
[tester] 
SST2Metric: acc=0.8555045871559633, hinge=0.7397225981732027, ce=0.42326559009748177
Local test acc @ epoch 60: 0.8555
Client 8 execute local training on 8 samples...
Local loss @ local epoch 0: 0.03904607892036438
Local loss @ local epoch 1: 0.02703503891825676
Local loss @ local epoch 2: 0.020185384899377823
Local loss @ local epoch 3: 0.016804881393909454
Local loss @ local epoch 4: 0.015541206113994122
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.89 seconds!
[tester] 
SST2Metric: acc=0.8325688073394495, hinge=0.9569619451094111, ce=0.5773260355462457
Local test acc @ epoch 60: 0.8326
Global evaluate on test data...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.56 seconds!
[tester] 
SST2Metric: acc=0.8142201834862385, hinge=0.8826325467420281, ce=0.4529600976667273
Global test acc @ epoch 60: 0.8142
Global epoch 61...
Client 6 execute local training on 8 samples...
Local loss @ local epoch 0: 0.1048407107591629
Local loss @ local epoch 1: 0.08304448425769806
Local loss @ local epoch 2: 0.07340715080499649
Local loss @ local epoch 3: 0.07176379859447479
Local loss @ local epoch 4: 0.072298564016819
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.89 seconds!
[tester] 
SST2Metric: acc=0.8543577981651376, hinge=0.726617856583464, ce=0.4032008418655279
Local test acc @ epoch 61: 0.8544
Client 7 execute local training on 8 samples...
Local loss @ local epoch 0: 0.05363410338759422
Local loss @ local epoch 1: 0.038894835859537125
Local loss @ local epoch 2: 0.03065575286746025
Local loss @ local epoch 3: 0.02716738171875477
Local loss @ local epoch 4: 0.026637624949216843
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.9 seconds!
[tester] 
SST2Metric: acc=0.8543577981651376, hinge=0.7371422986098386, ce=0.4206368926241823
Local test acc @ epoch 61: 0.8544
Client 0 execute local training on 8 samples...
Local loss @ local epoch 0: 0.012942295521497726
Local loss @ local epoch 1: 0.011862354353070259
Local loss @ local epoch 2: 0.011028685607016087
Local loss @ local epoch 3: 0.010369552299380302
Local loss @ local epoch 4: 0.009829316288232803
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.93 seconds!
[tester] 
SST2Metric: acc=0.856651376146789, hinge=0.7316341791131081, ce=0.4132094857156909
Local test acc @ epoch 61: 0.8567
Client 5 execute local training on 8 samples...
Local loss @ local epoch 0: 0.016207324340939522
Local loss @ local epoch 1: 0.010809425264596939
Local loss @ local epoch 2: 0.007815602235496044
Local loss @ local epoch 3: 0.0064169866964221
Local loss @ local epoch 4: 0.006054770667105913
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.86 seconds!
[tester] 
SST2Metric: acc=0.8440366972477065, hinge=0.8074453762911875, ce=0.4617036167049456
Local test acc @ epoch 61: 0.844
Client 1 execute local training on 8 samples...
Local loss @ local epoch 0: 0.019314156845211983
Local loss @ local epoch 1: 0.01243808027356863
Local loss @ local epoch 2: 0.008309734985232353
Local loss @ local epoch 3: 0.00594148226082325
Local loss @ local epoch 4: 0.00466998852789402
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.86 seconds!
[tester] 
SST2Metric: acc=0.856651376146789, hinge=0.7322078151713818, ce=0.41515708522905315
Local test acc @ epoch 61: 0.8567
Client 4 execute local training on 8 samples...
Local loss @ local epoch 0: 0.09842586517333984
Local loss @ local epoch 1: 0.0843886137008667
Local loss @ local epoch 2: 0.07325014472007751
Local loss @ local epoch 3: 0.06382756680250168
Local loss @ local epoch 4: 0.055865328758955
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.94 seconds!
[tester] 
SST2Metric: acc=0.856651376146789, hinge=0.7313545745447141, ce=0.4171893063851487
Local test acc @ epoch 61: 0.8567
Client 8 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0458383709192276
Local loss @ local epoch 1: 0.03127311170101166
Local loss @ local epoch 2: 0.022628437727689743
Local loss @ local epoch 3: 0.018022818490862846
Local loss @ local epoch 4: 0.0159988384693861
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.95 seconds!
[tester] 
SST2Metric: acc=0.8337155963302753, hinge=0.9476100455183502, ce=0.5700407272675758
Local test acc @ epoch 61: 0.8337
Client 2 execute local training on 8 samples...
Local loss @ local epoch 0: 0.018515851348638535
Local loss @ local epoch 1: 0.017412666231393814
Local loss @ local epoch 2: 0.016410542652010918
Local loss @ local epoch 3: 0.015528049319982529
Local loss @ local epoch 4: 0.014799731783568859
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.95 seconds!
[tester] 
SST2Metric: acc=0.7981651376146789, hinge=1.1346307848024806, ce=0.6943234072254817
Local test acc @ epoch 61: 0.7982
Client 3 execute local training on 8 samples...
Local loss @ local epoch 0: 0.39429017901420593
Local loss @ local epoch 1: 0.3060433864593506
Local loss @ local epoch 2: 0.23598118126392365
Local loss @ local epoch 3: 0.18299490213394165
Local loss @ local epoch 4: 0.145520880818367
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.87 seconds!
[tester] 
SST2Metric: acc=0.8532110091743119, hinge=0.7570747354435264, ce=0.4346505842820972
Local test acc @ epoch 61: 0.8532
Client 9 execute local training on 8 samples...
Local loss @ local epoch 0: 0.02236795425415039
Local loss @ local epoch 1: 0.018893370404839516
Local loss @ local epoch 2: 0.01729154773056507
Local loss @ local epoch 3: 0.01594499871134758
Local loss @ local epoch 4: 0.014484666287899017
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.9 seconds!
[tester] 
SST2Metric: acc=0.8543577981651376, hinge=0.7251157429786997, ce=0.41039469687336494
Local test acc @ epoch 61: 0.8544
Global evaluate on test data...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.38 seconds!
[tester] 
SST2Metric: acc=0.805045871559633, hinge=0.9064355450212409, ce=0.47298527787943634
Global test acc @ epoch 61: 0.805
Global epoch 62...
Client 0 execute local training on 8 samples...
Local loss @ local epoch 0: 0.011028523556888103
Local loss @ local epoch 1: 0.0102041345089674
Local loss @ local epoch 2: 0.00955174770206213
Local loss @ local epoch 3: 0.009014302864670753
Local loss @ local epoch 4: 0.008576958440244198
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.95 seconds!
[tester] 
SST2Metric: acc=0.8612385321100917, hinge=0.7343594154360098, ce=0.41506880354307113
Local test acc @ epoch 62: 0.8612
Client 5 execute local training on 8 samples...
Local loss @ local epoch 0: 0.014757351949810982
Local loss @ local epoch 1: 0.00989673100411892
Local loss @ local epoch 2: 0.007248216774314642
Local loss @ local epoch 3: 0.006071276031434536
Local loss @ local epoch 4: 0.005841447971761227
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.86 seconds!
[tester] 
SST2Metric: acc=0.8451834862385321, hinge=0.8197556138038635, ce=0.4712176848664333
Local test acc @ epoch 62: 0.8452
Client 1 execute local training on 8 samples...
Local loss @ local epoch 0: 0.019387196749448776
Local loss @ local epoch 1: 0.012460058555006981
Local loss @ local epoch 2: 0.008293219842016697
Local loss @ local epoch 3: 0.0058907936327159405
Local loss @ local epoch 4: 0.004586152732372284
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.88 seconds!
[tester] 
SST2Metric: acc=0.856651376146789, hinge=0.7351809421537119, ce=0.41695524953784197
Local test acc @ epoch 62: 0.8567
Client 2 execute local training on 8 samples...
Local loss @ local epoch 0: 0.03957502916455269
Local loss @ local epoch 1: 0.02830401249229908
Local loss @ local epoch 2: 0.02229735255241394
Local loss @ local epoch 3: 0.019778143614530563
Local loss @ local epoch 4: 0.01923336647450924
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.89 seconds!
[tester] 
SST2Metric: acc=0.8302752293577982, hinge=0.9350814488502818, ce=0.5631811129294524
Local test acc @ epoch 62: 0.8303
Client 7 execute local training on 8 samples...
Local loss @ local epoch 0: 0.09776677191257477
Local loss @ local epoch 1: 0.06580368429422379
Local loss @ local epoch 2: 0.04536287859082222
Local loss @ local epoch 3: 0.033023491501808167
Local loss @ local epoch 4: 0.02619587816298008
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.95 seconds!
[tester] 
SST2Metric: acc=0.8589449541284404, hinge=0.7287511965003582, ce=0.4119529607791214
Local test acc @ epoch 62: 0.8589
Client 8 execute local training on 8 samples...
Local loss @ local epoch 0: 0.03207425773143768
Local loss @ local epoch 1: 0.021845374256372452
Local loss @ local epoch 2: 0.015950895845890045
Local loss @ local epoch 3: 0.012979166582226753
Local loss @ local epoch 4: 0.011870716698467731
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.91 seconds!
[tester] 
SST2Metric: acc=0.8394495412844036, hinge=0.8781391203130057, ce=0.5245077043820873
Local test acc @ epoch 62: 0.8394
Client 3 execute local training on 8 samples...
Local loss @ local epoch 0: 0.3150903880596161
Local loss @ local epoch 1: 0.256319135427475
Local loss @ local epoch 2: 0.20539170503616333
Local loss @ local epoch 3: 0.16301949322223663
Local loss @ local epoch 4: 0.1301390677690506
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.9 seconds!
[tester] 
SST2Metric: acc=0.8589449541284404, hinge=0.7288590147407776, ce=0.41046994930099046
Local test acc @ epoch 62: 0.8589
Client 9 execute local training on 8 samples...
Local loss @ local epoch 0: 0.013936132192611694
Local loss @ local epoch 1: 0.011698673479259014
Local loss @ local epoch 2: 0.011031263507902622
Local loss @ local epoch 3: 0.010554102249443531
Local loss @ local epoch 4: 0.009840782731771469
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.9 seconds!
[tester] 
SST2Metric: acc=0.8543577981651376, hinge=0.7607478678499887, ce=0.4363983158714249
Local test acc @ epoch 62: 0.8544
Client 6 execute local training on 8 samples...
Local loss @ local epoch 0: 0.2449350506067276
Local loss @ local epoch 1: 0.18520447611808777
Local loss @ local epoch 2: 0.14042231440544128
Local loss @ local epoch 3: 0.11068404465913773
Local loss @ local epoch 4: 0.09456217288970947
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.94 seconds!
[tester] 
SST2Metric: acc=0.8451834862385321, hinge=0.7809055719627153, ce=0.4444317256138925
Local test acc @ epoch 62: 0.8452
Client 4 execute local training on 8 samples...
Local loss @ local epoch 0: 0.2025296837091446
Local loss @ local epoch 1: 0.1515168845653534
Local loss @ local epoch 2: 0.11421170085668564
Local loss @ local epoch 3: 0.09006975591182709
Local loss @ local epoch 4: 0.07671472430229187
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.9 seconds!
[tester] 
SST2Metric: acc=0.8532110091743119, hinge=0.7434895064579238, ce=0.4251327517959332
Local test acc @ epoch 62: 0.8532
Global evaluate on test data...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.32 seconds!
[tester] 
SST2Metric: acc=0.8061926605504587, hinge=0.91884289777607, ce=0.483214191789098
Global test acc @ epoch 62: 0.8062
Global epoch 63...
Client 0 execute local training on 8 samples...
Local loss @ local epoch 0: 0.00817918311804533
Local loss @ local epoch 1: 0.007743154652416706
Local loss @ local epoch 2: 0.007338955532759428
Local loss @ local epoch 3: 0.006962568964809179
Local loss @ local epoch 4: 0.006610308773815632
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.94 seconds!
[tester] 
SST2Metric: acc=0.8555045871559633, hinge=0.7477162797516639, ce=0.42426287852347305
Local test acc @ epoch 63: 0.8555
Client 3 execute local training on 8 samples...
Local loss @ local epoch 0: 0.14236822724342346
Local loss @ local epoch 1: 0.11392658948898315
Local loss @ local epoch 2: 0.09603104740381241
Local loss @ local epoch 3: 0.08844474703073502
Local loss @ local epoch 4: 0.0885825902223587
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.89 seconds!
[tester] 
SST2Metric: acc=0.8428899082568807, hinge=0.8338157221538212, ce=0.4809598462750127
Local test acc @ epoch 63: 0.8429
Client 5 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0063766613602638245
Local loss @ local epoch 1: 0.0058333296328783035
Local loss @ local epoch 2: 0.005745552014559507
Local loss @ local epoch 3: 0.00547491991892457
Local loss @ local epoch 4: 0.005134338513016701
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.91 seconds!
[tester] 
SST2Metric: acc=0.841743119266055, hinge=0.7914116893886426, ce=0.45099729094056223
Local test acc @ epoch 63: 0.8417
Client 7 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0322548970580101
Local loss @ local epoch 1: 0.02431659959256649
Local loss @ local epoch 2: 0.020753681659698486
Local loss @ local epoch 3: 0.020198198035359383
Local loss @ local epoch 4: 0.021021904423832893
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.85 seconds!
[tester] 
SST2Metric: acc=0.856651376146789, hinge=0.7490633670069756, ce=0.42652931395990756
Local test acc @ epoch 63: 0.8567
Client 9 execute local training on 8 samples...
Local loss @ local epoch 0: 0.01004000287503004
Local loss @ local epoch 1: 0.009224500507116318
Local loss @ local epoch 2: 0.008604004047811031
Local loss @ local epoch 3: 0.007924401201307774
Local loss @ local epoch 4: 0.007340671494603157
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.92 seconds!
[tester] 
SST2Metric: acc=0.856651376146789, hinge=0.7733968285792464, ce=0.4463087424781139
Local test acc @ epoch 63: 0.8567
Client 2 execute local training on 8 samples...
Local loss @ local epoch 0: 0.04289687052369118
Local loss @ local epoch 1: 0.0335206538438797
Local loss @ local epoch 2: 0.028719373047351837
Local loss @ local epoch 3: 0.02600306086242199
Local loss @ local epoch 4: 0.02392234280705452
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.98 seconds!
[tester] 
SST2Metric: acc=0.8405963302752294, hinge=0.8738961405710343, ce=0.5229354717281707
Local test acc @ epoch 63: 0.8406
Client 6 execute local training on 8 samples...
Local loss @ local epoch 0: 0.4498012065887451
Local loss @ local epoch 1: 0.3600340485572815
Local loss @ local epoch 2: 0.28063517808914185
Local loss @ local epoch 3: 0.21398621797561646
Local loss @ local epoch 4: 0.1616968959569931
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.97 seconds!
[tester] 
SST2Metric: acc=0.8612385321100917, hinge=0.7425237717705035, ce=0.42213927168578524
Local test acc @ epoch 63: 0.8612
Client 1 execute local training on 8 samples...
Local loss @ local epoch 0: 0.005298769101500511
Local loss @ local epoch 1: 0.004069076385349035
Local loss @ local epoch 2: 0.0035742614418268204
Local loss @ local epoch 3: 0.003412015503272414
Local loss @ local epoch 4: 0.0033046780154109
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.88 seconds!
[tester] 
SST2Metric: acc=0.8497706422018348, hinge=0.797058863787476, ce=0.4670846865623506
Local test acc @ epoch 63: 0.8498
Client 8 execute local training on 8 samples...
Local loss @ local epoch 0: 0.01709897071123123
Local loss @ local epoch 1: 0.01350289024412632
Local loss @ local epoch 2: 0.012133287265896797
Local loss @ local epoch 3: 0.011752220802009106
Local loss @ local epoch 4: 0.011443724855780602
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.94 seconds!
[tester] 
SST2Metric: acc=0.8337155963302753, hinge=0.926320633893713, ce=0.5611033803893364
Local test acc @ epoch 63: 0.8337
Client 4 execute local training on 8 samples...
Local loss @ local epoch 0: 0.21794143319129944
Local loss @ local epoch 1: 0.1620871126651764
Local loss @ local epoch 2: 0.12527725100517273
Local loss @ local epoch 3: 0.10386006534099579
Local loss @ local epoch 4: 0.09277216345071793
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.92 seconds!
[tester] 
SST2Metric: acc=0.8543577981651376, hinge=0.7499621172837161, ce=0.42928872474708485
Local test acc @ epoch 63: 0.8544
Global evaluate on test data...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.55 seconds!
[tester] 
SST2Metric: acc=0.801605504587156, hinge=0.9343867498800296, ce=0.4961366208244559
Global test acc @ epoch 63: 0.8016
Global epoch 64...
Client 3 execute local training on 8 samples...
Local loss @ local epoch 0: 0.14671096205711365
Local loss @ local epoch 1: 0.11637964099645615
Local loss @ local epoch 2: 0.09653405100107193
Local loss @ local epoch 3: 0.08709916472434998
Local loss @ local epoch 4: 0.08596845716238022
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.93 seconds!
[tester] 
SST2Metric: acc=0.8463302752293578, hinge=0.808605020199347, ce=0.464118877712022
Local test acc @ epoch 64: 0.8463
Client 2 execute local training on 8 samples...
Local loss @ local epoch 0: 0.17124029994010925
Local loss @ local epoch 1: 0.11614625155925751
Local loss @ local epoch 2: 0.0785229429602623
Local loss @ local epoch 3: 0.05381288751959801
Local loss @ local epoch 4: 0.038042373955249786
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.93 seconds!
[tester] 
SST2Metric: acc=0.8497706422018348, hinge=0.7589691993566828, ce=0.4343708447894133
Local test acc @ epoch 64: 0.8498
Client 0 execute local training on 8 samples...
Local loss @ local epoch 0: 0.006647131405770779
Local loss @ local epoch 1: 0.006273793522268534
Local loss @ local epoch 2: 0.0061651901341974735
Local loss @ local epoch 3: 0.0058775655925273895
Local loss @ local epoch 4: 0.005594485905021429
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.95 seconds!
[tester] 
SST2Metric: acc=0.8555045871559633, hinge=0.7548090313826132, ce=0.43027830748271984
Local test acc @ epoch 64: 0.8555
Client 4 execute local training on 8 samples...
Local loss @ local epoch 0: 0.13796107470989227
Local loss @ local epoch 1: 0.1148611307144165
Local loss @ local epoch 2: 0.09991305321455002
Local loss @ local epoch 3: 0.08895742893218994
Local loss @ local epoch 4: 0.0794045627117157
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.91 seconds!
[tester] 
SST2Metric: acc=0.8486238532110092, hinge=0.7729226693921133, ce=0.44632821617788987
Local test acc @ epoch 64: 0.8486
Client 7 execute local training on 8 samples...
Local loss @ local epoch 0: 0.02916502021253109
Local loss @ local epoch 1: 0.02435379847884178
Local loss @ local epoch 2: 0.023120056837797165
Local loss @ local epoch 3: 0.022868094965815544
Local loss @ local epoch 4: 0.022182878106832504
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.03 seconds!
[tester] 
SST2Metric: acc=0.856651376146789, hinge=0.7422295348086488, ce=0.41981654505057414
Local test acc @ epoch 64: 0.8567
Client 6 execute local training on 8 samples...
Local loss @ local epoch 0: 0.10573853552341461
Local loss @ local epoch 1: 0.08780835568904877
Local loss @ local epoch 2: 0.08172538876533508
Local loss @ local epoch 3: 0.08099348098039627
Local loss @ local epoch 4: 0.07959242165088654
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.92 seconds!
[tester] 
SST2Metric: acc=0.8509174311926605, hinge=0.7769749768830221, ce=0.4398802670334047
Local test acc @ epoch 64: 0.8509
Client 5 execute local training on 8 samples...
Local loss @ local epoch 0: 0.005489141680300236
Local loss @ local epoch 1: 0.005182201974093914
Local loss @ local epoch 2: 0.004849384538829327
Local loss @ local epoch 3: 0.004571428056806326
Local loss @ local epoch 4: 0.00432143360376358
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.01 seconds!
[tester] 
SST2Metric: acc=0.8451834862385321, hinge=0.7761026447245835, ce=0.4407800019840462
Local test acc @ epoch 64: 0.8452
Client 9 execute local training on 8 samples...
Local loss @ local epoch 0: 0.029470860958099365
Local loss @ local epoch 1: 0.0199253112077713
Local loss @ local epoch 2: 0.014600610360503197
Local loss @ local epoch 3: 0.01210077852010727
Local loss @ local epoch 4: 0.01134482678025961
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.0 seconds!
[tester] 
SST2Metric: acc=0.8474770642201835, hinge=0.7634471962484745, ce=0.44185774867025573
Local test acc @ epoch 64: 0.8475
Client 8 execute local training on 8 samples...
Local loss @ local epoch 0: 0.021004384383559227
Local loss @ local epoch 1: 0.015290506184101105
Local loss @ local epoch 2: 0.01252351887524128
Local loss @ local epoch 3: 0.011605489999055862
Local loss @ local epoch 4: 0.01155057828873396
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.91 seconds!
[tester] 
SST2Metric: acc=0.8405963302752294, hinge=0.9171197793079079, ce=0.556457420559536
Local test acc @ epoch 64: 0.8406
Client 1 execute local training on 8 samples...
Local loss @ local epoch 0: 0.008399461396038532
Local loss @ local epoch 1: 0.00579442922025919
Local loss @ local epoch 2: 0.004324290901422501
Local loss @ local epoch 3: 0.0036136836279183626
Local loss @ local epoch 4: 0.0033709141425788403
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.89 seconds!
[tester] 
SST2Metric: acc=0.8509174311926605, hinge=0.7536443147910844, ce=0.43455657757788796
Local test acc @ epoch 64: 0.8509
Global evaluate on test data...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.19 seconds!
[tester] 
SST2Metric: acc=0.8107798165137615, hinge=0.9024863744821023, ce=0.47047517761383034
Global test acc @ epoch 64: 0.8108
Global epoch 65...
Client 9 execute local training on 8 samples...
Local loss @ local epoch 0: 0.011902902275323868
Local loss @ local epoch 1: 0.010587730444967747
Local loss @ local epoch 2: 0.009911864995956421
Local loss @ local epoch 3: 0.009134823456406593
Local loss @ local epoch 4: 0.008347763679921627
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.9 seconds!
[tester] 
SST2Metric: acc=0.856651376146789, hinge=0.76550978025712, ce=0.4415470046170671
Local test acc @ epoch 65: 0.8567
Client 1 execute local training on 8 samples...
Local loss @ local epoch 0: 0.004469967447221279
Local loss @ local epoch 1: 0.003963681869208813
Local loss @ local epoch 2: 0.003524980042129755
Local loss @ local epoch 3: 0.0031424835324287415
Local loss @ local epoch 4: 0.002811524784192443
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.97 seconds!
[tester] 
SST2Metric: acc=0.8486238532110092, hinge=0.769618629999117, ce=0.44728622974084536
Local test acc @ epoch 65: 0.8486
Client 8 execute local training on 8 samples...
Local loss @ local epoch 0: 0.022651512175798416
Local loss @ local epoch 1: 0.016074955463409424
Local loss @ local epoch 2: 0.012643822468817234
Local loss @ local epoch 3: 0.011259209364652634
Local loss @ local epoch 4: 0.010992349125444889
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.84 seconds!
[tester] 
SST2Metric: acc=0.8291284403669725, hinge=0.9537907859327597, ce=0.5825662171887874
Local test acc @ epoch 65: 0.8291
Client 3 execute local training on 8 samples...
Local loss @ local epoch 0: 0.34520265460014343
Local loss @ local epoch 1: 0.28124549984931946
Local loss @ local epoch 2: 0.22581565380096436
Local loss @ local epoch 3: 0.1791353076696396
Local loss @ local epoch 4: 0.14183543622493744
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.97 seconds!
[tester] 
SST2Metric: acc=0.856651376146789, hinge=0.7466211436538521, ce=0.42787441526381553
Local test acc @ epoch 65: 0.8567
Client 0 execute local training on 8 samples...
Local loss @ local epoch 0: 0.006215037778019905
Local loss @ local epoch 1: 0.005894116126000881
Local loss @ local epoch 2: 0.005666403565555811
Local loss @ local epoch 3: 0.005368662998080254
Local loss @ local epoch 4: 0.005126556847244501
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.95 seconds!
[tester] 
SST2Metric: acc=0.8589449541284404, hinge=0.7587152489828407, ce=0.43582649336307555
Local test acc @ epoch 65: 0.8589
Client 2 execute local training on 8 samples...
Local loss @ local epoch 0: 0.038918934762477875
Local loss @ local epoch 1: 0.0269862562417984
Local loss @ local epoch 2: 0.020210739225149155
Local loss @ local epoch 3: 0.016888953745365143
Local loss @ local epoch 4: 0.01570291444659233
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.89 seconds!
[tester] 
SST2Metric: acc=0.8371559633027523, hinge=0.903122540597522, ce=0.54600948030332
Local test acc @ epoch 65: 0.8372
Client 7 execute local training on 8 samples...
Local loss @ local epoch 0: 0.07533349096775055
Local loss @ local epoch 1: 0.05072173476219177
Local loss @ local epoch 2: 0.03522523120045662
Local loss @ local epoch 3: 0.026089591905474663
Local loss @ local epoch 4: 0.021274546161293983
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.92 seconds!
[tester] 
SST2Metric: acc=0.8577981651376146, hinge=0.7560435455600056, ce=0.433408989993975
Local test acc @ epoch 65: 0.8578
Client 5 execute local training on 8 samples...
Local loss @ local epoch 0: 0.010280459187924862
Local loss @ local epoch 1: 0.006996493320912123
Local loss @ local epoch 2: 0.0052888779900968075
Local loss @ local epoch 3: 0.004632870201021433
Local loss @ local epoch 4: 0.004627483431249857
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.88 seconds!
[tester] 
SST2Metric: acc=0.8405963302752294, hinge=0.8658115018945222, ce=0.508728751316752
Local test acc @ epoch 65: 0.8406
Client 4 execute local training on 8 samples...
Local loss @ local epoch 0: 0.31214356422424316
Local loss @ local epoch 1: 0.24204184114933014
Local loss @ local epoch 2: 0.18396207690238953
Local loss @ local epoch 3: 0.13866686820983887
Local loss @ local epoch 4: 0.10641641914844513
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.9 seconds!
[tester] 
SST2Metric: acc=0.8577981651376146, hinge=0.749514148459522, ce=0.4299967019047605
Local test acc @ epoch 65: 0.8578
Client 6 execute local training on 8 samples...
Local loss @ local epoch 0: 0.12878309190273285
Local loss @ local epoch 1: 0.09730596840381622
Local loss @ local epoch 2: 0.07935529202222824
Local loss @ local epoch 3: 0.07243821769952774
Local loss @ local epoch 4: 0.07234298437833786
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.95 seconds!
[tester] 
SST2Metric: acc=0.8486238532110092, hinge=0.7929734960881942, ce=0.45417858577982756
Local test acc @ epoch 65: 0.8486
Global evaluate on test data...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.3 seconds!
[tester] 
SST2Metric: acc=0.8038990825688074, hinge=0.9337938765593625, ce=0.49742787902098184
Global test acc @ epoch 65: 0.8039
Global epoch 66...
Client 5 execute local training on 8 samples...
Local loss @ local epoch 0: 0.012652146629989147
Local loss @ local epoch 1: 0.008340742439031601
Local loss @ local epoch 2: 0.005929586011916399
Local loss @ local epoch 3: 0.004761597141623497
Local loss @ local epoch 4: 0.004400379955768585
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.97 seconds!
[tester] 
SST2Metric: acc=0.8428899082568807, hinge=0.8326973273940043, ce=0.48501144062702056
Local test acc @ epoch 66: 0.8429
Client 3 execute local training on 8 samples...
Local loss @ local epoch 0: 0.09172767400741577
Local loss @ local epoch 1: 0.08476811647415161
Local loss @ local epoch 2: 0.0843081846833229
Local loss @ local epoch 3: 0.08310917019844055
Local loss @ local epoch 4: 0.08013366162776947
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.03 seconds!
[tester] 
SST2Metric: acc=0.8440366972477065, hinge=0.7875867092281307, ce=0.4515433030036372
Local test acc @ epoch 66: 0.844
Client 0 execute local training on 8 samples...
Local loss @ local epoch 0: 0.009972475469112396
Local loss @ local epoch 1: 0.006986522115767002
Local loss @ local epoch 2: 0.00547629501670599
Local loss @ local epoch 3: 0.004972929134964943
Local loss @ local epoch 4: 0.005074773449450731
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.97 seconds!
[tester] 
SST2Metric: acc=0.8520642201834863, hinge=0.7741225996148695, ce=0.44803379598466697
Local test acc @ epoch 66: 0.8521
Client 1 execute local training on 8 samples...
Local loss @ local epoch 0: 0.003785208333283663
Local loss @ local epoch 1: 0.003365497337654233
Local loss @ local epoch 2: 0.0030012675561010838
Local loss @ local epoch 3: 0.002682566177099943
Local loss @ local epoch 4: 0.002407270250841975
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.96 seconds!
[tester] 
SST2Metric: acc=0.8497706422018348, hinge=0.7798347359676974, ce=0.4551323983559829
Local test acc @ epoch 66: 0.8498
Client 8 execute local training on 8 samples...
Local loss @ local epoch 0: 0.023011237382888794
Local loss @ local epoch 1: 0.016257021576166153
Local loss @ local epoch 2: 0.012683099135756493
Local loss @ local epoch 3: 0.011185931041836739
Local loss @ local epoch 4: 0.01084523368626833
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.95 seconds!
[tester] 
SST2Metric: acc=0.823394495412844, hinge=0.9737262280162321, ce=0.5966895235560348
Local test acc @ epoch 66: 0.8234
Client 9 execute local training on 8 samples...
Local loss @ local epoch 0: 0.03539593145251274
Local loss @ local epoch 1: 0.02270474284887314
Local loss @ local epoch 2: 0.0152423782274127
Local loss @ local epoch 3: 0.011184659786522388
Local loss @ local epoch 4: 0.00926877185702324
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.03 seconds!
[tester] 
SST2Metric: acc=0.856651376146789, hinge=0.7687608267462581, ce=0.4446797163242799
Local test acc @ epoch 66: 0.8567
Client 6 execute local training on 8 samples...
Local loss @ local epoch 0: 0.20280805230140686
Local loss @ local epoch 1: 0.15119129419326782
Local loss @ local epoch 2: 0.1146705374121666
Local loss @ local epoch 3: 0.09247169643640518
Local loss @ local epoch 4: 0.08235064893960953
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.89 seconds!
[tester] 
SST2Metric: acc=0.8451834862385321, hinge=0.8084498170865785, ce=0.4670349265589729
Local test acc @ epoch 66: 0.8452
Client 4 execute local training on 8 samples...
Local loss @ local epoch 0: 0.22124505043029785
Local loss @ local epoch 1: 0.16512280702590942
Local loss @ local epoch 2: 0.12228686362504959
Local loss @ local epoch 3: 0.09276045858860016
Local loss @ local epoch 4: 0.07505442202091217
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.96 seconds!
[tester] 
SST2Metric: acc=0.8520642201834863, hinge=0.760672140832341, ce=0.4404844230878244
Local test acc @ epoch 66: 0.8521
Client 2 execute local training on 8 samples...
Local loss @ local epoch 0: 0.032949481159448624
Local loss @ local epoch 1: 0.02270674891769886
Local loss @ local epoch 2: 0.016728894785046577
Local loss @ local epoch 3: 0.013681016862392426
Local loss @ local epoch 4: 0.012584151700139046
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.04 seconds!
[tester] 
SST2Metric: acc=0.8222477064220184, hinge=0.9884547596677727, ce=0.6063778262426265
Local test acc @ epoch 66: 0.8222
Client 7 execute local training on 8 samples...
Local loss @ local epoch 0: 0.09410123527050018
Local loss @ local epoch 1: 0.06280750781297684
Local loss @ local epoch 2: 0.04302001744508743
Local loss @ local epoch 3: 0.031169399619102478
Local loss @ local epoch 4: 0.024613987654447556
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.01 seconds!
[tester] 
SST2Metric: acc=0.8543577981651376, hinge=0.755287988470235, ce=0.4355842052721803
Local test acc @ epoch 66: 0.8544
Global evaluate on test data...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.35 seconds!
[tester] 
SST2Metric: acc=0.8038990825688074, hinge=0.9399699385559886, ce=0.5025343597875139
Global test acc @ epoch 66: 0.8039
Global epoch 67...
Client 9 execute local training on 8 samples...
Local loss @ local epoch 0: 0.010214581154286861
Local loss @ local epoch 1: 0.00920106004923582
Local loss @ local epoch 2: 0.008333940990269184
Local loss @ local epoch 3: 0.0075825778767466545
Local loss @ local epoch 4: 0.0069478047080338
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.97 seconds!
[tester] 
SST2Metric: acc=0.8555045871559633, hinge=0.7792794693500624, ce=0.4538819350199689
Local test acc @ epoch 67: 0.8555
Client 1 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0038785526994615793
Local loss @ local epoch 1: 0.0034275497309863567
Local loss @ local epoch 2: 0.003043323289602995
Local loss @ local epoch 3: 0.002705620601773262
Local loss @ local epoch 4: 0.0024147285148501396
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.0 seconds!
[tester] 
SST2Metric: acc=0.8463302752293578, hinge=0.776553847921004, ce=0.45413510635208953
Local test acc @ epoch 67: 0.8463
Client 0 execute local training on 8 samples...
Local loss @ local epoch 0: 0.005700663197785616
Local loss @ local epoch 1: 0.005226219538599253
Local loss @ local epoch 2: 0.005092993378639221
Local loss @ local epoch 3: 0.004859015811234713
Local loss @ local epoch 4: 0.004600358195602894
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.97 seconds!
[tester] 
SST2Metric: acc=0.8555045871559633, hinge=0.7649264696541183, ce=0.44301662198481484
Local test acc @ epoch 67: 0.8555
Client 6 execute local training on 8 samples...
Local loss @ local epoch 0: 0.16543838381767273
Local loss @ local epoch 1: 0.12362919002771378
Local loss @ local epoch 2: 0.09657179564237595
Local loss @ local epoch 3: 0.08265884220600128
Local loss @ local epoch 4: 0.07869938015937805
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.05 seconds!
[tester] 
SST2Metric: acc=0.8474770642201835, hinge=0.8246232394231569, ce=0.4810760295010009
Local test acc @ epoch 67: 0.8475
Client 5 execute local training on 8 samples...
Local loss @ local epoch 0: 0.004316822625696659
Local loss @ local epoch 1: 0.004127862863242626
Local loss @ local epoch 2: 0.0038811764679849148
Local loss @ local epoch 3: 0.0036565258633345366
Local loss @ local epoch 4: 0.00348121696151793
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.98 seconds!
[tester] 
SST2Metric: acc=0.841743119266055, hinge=0.816448509283022, ce=0.47626982828486847
Local test acc @ epoch 67: 0.8417
Client 7 execute local training on 8 samples...
Local loss @ local epoch 0: 0.038812682032585144
Local loss @ local epoch 1: 0.02737806923687458
Local loss @ local epoch 2: 0.020839711651206017
Local loss @ local epoch 3: 0.017857367172837257
Local loss @ local epoch 4: 0.01727679930627346
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.97 seconds!
[tester] 
SST2Metric: acc=0.8577981651376146, hinge=0.7651660550351537, ce=0.44593208369482823
Local test acc @ epoch 67: 0.8578
Client 3 execute local training on 8 samples...
Local loss @ local epoch 0: 0.1370779126882553
Local loss @ local epoch 1: 0.10839742422103882
Local loss @ local epoch 2: 0.09006547927856445
Local loss @ local epoch 3: 0.08178643137216568
Local loss @ local epoch 4: 0.08124429732561111
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.94 seconds!
[tester] 
SST2Metric: acc=0.8405963302752294, hinge=0.8236475556815436, ce=0.48174447114066726
Local test acc @ epoch 67: 0.8406
Client 2 execute local training on 8 samples...
Local loss @ local epoch 0: 0.18043044209480286
Local loss @ local epoch 1: 0.12298530340194702
Local loss @ local epoch 2: 0.08322369307279587
Local loss @ local epoch 3: 0.056748151779174805
Local loss @ local epoch 4: 0.03957919776439667
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.93 seconds!
[tester] 
SST2Metric: acc=0.8497706422018348, hinge=0.7736084545971057, ce=0.45200928595395534
Local test acc @ epoch 67: 0.8498
Client 4 execute local training on 8 samples...
Local loss @ local epoch 0: 0.12133368849754333
Local loss @ local epoch 1: 0.10255054384469986
Local loss @ local epoch 2: 0.08944516628980637
Local loss @ local epoch 3: 0.07875841856002808
Local loss @ local epoch 4: 0.06924080848693848
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.88 seconds!
[tester] 
SST2Metric: acc=0.8520642201834863, hinge=0.7931258004466328, ce=0.4688286825399333
Local test acc @ epoch 67: 0.8521
Client 8 execute local training on 8 samples...
Local loss @ local epoch 0: 0.024512065574526787
Local loss @ local epoch 1: 0.017167389392852783
Local loss @ local epoch 2: 0.013172282837331295
Local loss @ local epoch 3: 0.011369859799742699
Local loss @ local epoch 4: 0.010823769494891167
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.87 seconds!
[tester] 
SST2Metric: acc=0.8176605504587156, hinge=1.0034195274934856, ce=0.6215662883430155
Local test acc @ epoch 67: 0.8177
Global evaluate on test data...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.35 seconds!
[tester] 
SST2Metric: acc=0.8061926605504587, hinge=0.9287782476856075, ce=0.495747048685481
Global test acc @ epoch 67: 0.8062
Global epoch 68...
Client 6 execute local training on 8 samples...
Local loss @ local epoch 0: 0.13054470717906952
Local loss @ local epoch 1: 0.09834374487400055
Local loss @ local epoch 2: 0.0796784907579422
Local loss @ local epoch 3: 0.07215514779090881
Local loss @ local epoch 4: 0.07172991335391998
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.0 seconds!
[tester] 
SST2Metric: acc=0.8486238532110092, hinge=0.8049943612529598, ce=0.46619481988114503
Local test acc @ epoch 68: 0.8486
Client 1 execute local training on 8 samples...
Local loss @ local epoch 0: 0.009348956868052483
Local loss @ local epoch 1: 0.005952037405222654
Local loss @ local epoch 2: 0.003926109056919813
Local loss @ local epoch 3: 0.0027573518455028534
Local loss @ local epoch 4: 0.002114545786753297
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.91 seconds!
[tester] 
SST2Metric: acc=0.8509174311926605, hinge=0.7638209806669742, ce=0.4465716471981018
Local test acc @ epoch 68: 0.8509
Client 7 execute local training on 8 samples...
Local loss @ local epoch 0: 0.022121189162135124
Local loss @ local epoch 1: 0.02042275294661522
Local loss @ local epoch 2: 0.019139662384986877
Local loss @ local epoch 3: 0.01791290007531643
Local loss @ local epoch 4: 0.016920924186706543
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.97 seconds!
[tester] 
SST2Metric: acc=0.8589449541284404, hinge=0.7553069654407851, ce=0.4379488424816673
Local test acc @ epoch 68: 0.8589
Client 5 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0074285417795181274
Local loss @ local epoch 1: 0.0050756861455738544
Local loss @ local epoch 2: 0.0038619711995124817
Local loss @ local epoch 3: 0.0034097512252628803
Local loss @ local epoch 4: 0.0034258700907230377
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.03 seconds!
[tester] 
SST2Metric: acc=0.8428899082568807, hinge=0.8546861840770879, ce=0.504833256872798
Local test acc @ epoch 68: 0.8429
Client 8 execute local training on 8 samples...
Local loss @ local epoch 0: 0.11609828472137451
Local loss @ local epoch 1: 0.07801419496536255
Local loss @ local epoch 2: 0.0521068200469017
Local loss @ local epoch 3: 0.03510233387351036
Local loss @ local epoch 4: 0.024186760187149048
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.94 seconds!
[tester] 
SST2Metric: acc=0.856651376146789, hinge=0.7650160968577097, ce=0.4460371398140565
Local test acc @ epoch 68: 0.8567
Client 2 execute local training on 8 samples...
Local loss @ local epoch 0: 0.040244996547698975
Local loss @ local epoch 1: 0.02796630561351776
Local loss @ local epoch 2: 0.02101888507604599
Local loss @ local epoch 3: 0.017600592225790024
Local loss @ local epoch 4: 0.01628550887107849
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.93 seconds!
[tester] 
SST2Metric: acc=0.8428899082568807, hinge=0.9085401068313406, ce=0.5559306440921927
Local test acc @ epoch 68: 0.8429
Client 3 execute local training on 8 samples...
Local loss @ local epoch 0: 0.3197788894176483
Local loss @ local epoch 1: 0.2608701288700104
Local loss @ local epoch 2: 0.2090815305709839
Local loss @ local epoch 3: 0.16511952877044678
Local loss @ local epoch 4: 0.12994089722633362
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.91 seconds!
[tester] 
SST2Metric: acc=0.8577981651376146, hinge=0.7558050888393997, ce=0.4380189731766239
Local test acc @ epoch 68: 0.8578
Client 4 execute local training on 8 samples...
Local loss @ local epoch 0: 0.1295321136713028
Local loss @ local epoch 1: 0.10233860462903976
Local loss @ local epoch 2: 0.08577533066272736
Local loss @ local epoch 3: 0.07633034139871597
Local loss @ local epoch 4: 0.06995372474193573
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.02 seconds!
[tester] 
SST2Metric: acc=0.8474770642201835, hinge=0.7970360658310969, ce=0.4729410652562532
Local test acc @ epoch 68: 0.8475
Client 9 execute local training on 8 samples...
Local loss @ local epoch 0: 0.013284293003380299
Local loss @ local epoch 1: 0.010731792077422142
Local loss @ local epoch 2: 0.00983008835464716
Local loss @ local epoch 3: 0.00939601194113493
Local loss @ local epoch 4: 0.0088580884039402
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.93 seconds!
[tester] 
SST2Metric: acc=0.8589449541284404, hinge=0.7562761867265089, ce=0.4393740432110504
Local test acc @ epoch 68: 0.8589
Client 0 execute local training on 8 samples...
Local loss @ local epoch 0: 0.005071274004876614
Local loss @ local epoch 1: 0.004756683949381113
Local loss @ local epoch 2: 0.004665129352360964
Local loss @ local epoch 3: 0.004449611529707909
Local loss @ local epoch 4: 0.004227000754326582
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.92 seconds!
[tester] 
SST2Metric: acc=0.856651376146789, hinge=0.768707483596758, ce=0.44826759131082733
Local test acc @ epoch 68: 0.8567
Global evaluate on test data...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.38 seconds!
[tester] 
SST2Metric: acc=0.8073394495412844, hinge=0.9236056857973064, ce=0.4928160368276914
Global test acc @ epoch 68: 0.8073
Global epoch 69...
Client 6 execute local training on 8 samples...
Local loss @ local epoch 0: 0.12145934253931046
Local loss @ local epoch 1: 0.09214455634355545
Local loss @ local epoch 2: 0.07590144872665405
Local loss @ local epoch 3: 0.07006656378507614
Local loss @ local epoch 4: 0.07027359306812286
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.92 seconds!
[tester] 
SST2Metric: acc=0.8486238532110092, hinge=0.8014603735656913, ce=0.464897446578941
Local test acc @ epoch 69: 0.8486
Client 7 execute local training on 8 samples...
Local loss @ local epoch 0: 0.04416331648826599
Local loss @ local epoch 1: 0.03071136400103569
Local loss @ local epoch 2: 0.022628283128142357
Local loss @ local epoch 3: 0.0184391550719738
Local loss @ local epoch 4: 0.01693723537027836
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.96 seconds!
[tester] 
SST2Metric: acc=0.8555045871559633, hinge=0.7573211015364446, ce=0.44123510112174263
Local test acc @ epoch 69: 0.8555
Client 8 execute local training on 8 samples...
Local loss @ local epoch 0: 0.02420436590909958
Local loss @ local epoch 1: 0.016357310116291046
Local loss @ local epoch 2: 0.011820519343018532
Local loss @ local epoch 3: 0.009490646421909332
Local loss @ local epoch 4: 0.008566463366150856
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.92 seconds!
[tester] 
SST2Metric: acc=0.841743119266055, hinge=0.9115536532817631, ce=0.5581993098934651
Local test acc @ epoch 69: 0.8417
Client 4 execute local training on 8 samples...
Local loss @ local epoch 0: 0.186186745762825
Local loss @ local epoch 1: 0.14060911536216736
Local loss @ local epoch 2: 0.11227633059024811
Local loss @ local epoch 3: 0.09661580622196198
Local loss @ local epoch 4: 0.08819563686847687
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.93 seconds!
[tester] 
SST2Metric: acc=0.8532110091743119, hinge=0.7606646646873667, ce=0.4439944246978234
Local test acc @ epoch 69: 0.8532
Client 0 execute local training on 8 samples...
Local loss @ local epoch 0: 0.005113124847412109
Local loss @ local epoch 1: 0.00485927052795887
Local loss @ local epoch 2: 0.004606565460562706
Local loss @ local epoch 3: 0.004369450733065605
Local loss @ local epoch 4: 0.0041890619322657585
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.9 seconds!
[tester] 
SST2Metric: acc=0.8577981651376146, hinge=0.7681113852000018, ce=0.44775549491542743
Local test acc @ epoch 69: 0.8578
Client 2 execute local training on 8 samples...
Local loss @ local epoch 0: 0.03462052345275879
Local loss @ local epoch 1: 0.023972611874341965
Local loss @ local epoch 2: 0.017939575016498566
Local loss @ local epoch 3: 0.014979534782469273
Local loss @ local epoch 4: 0.013909294269979
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.93 seconds!
[tester] 
SST2Metric: acc=0.8394495412844036, hinge=0.9187009826712652, ce=0.5629923261607006
Local test acc @ epoch 69: 0.8394
Client 3 execute local training on 8 samples...
Local loss @ local epoch 0: 0.3142869472503662
Local loss @ local epoch 1: 0.2557471692562103
Local loss @ local epoch 2: 0.20447587966918945
Local loss @ local epoch 3: 0.1611795276403427
Local loss @ local epoch 4: 0.12679192423820496
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.94 seconds!
[tester] 
SST2Metric: acc=0.856651376146789, hinge=0.7618287651363863, ce=0.4419171508415517
Local test acc @ epoch 69: 0.8567
Client 9 execute local training on 8 samples...
Local loss @ local epoch 0: 0.009279613383114338
Local loss @ local epoch 1: 0.007899115793406963
Local loss @ local epoch 2: 0.007490600924938917
Local loss @ local epoch 3: 0.007127565331757069
Local loss @ local epoch 4: 0.006609948817640543
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.94 seconds!
[tester] 
SST2Metric: acc=0.8532110091743119, hinge=0.794595031700003, ce=0.46896059667678774
Local test acc @ epoch 69: 0.8532
Client 1 execute local training on 8 samples...
Local loss @ local epoch 0: 0.002832729835063219
Local loss @ local epoch 1: 0.0025107860565185547
Local loss @ local epoch 2: 0.0022797626443207264
Local loss @ local epoch 3: 0.0020524875726550817
Local loss @ local epoch 4: 0.0018428076291456819
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.01 seconds!
[tester] 
SST2Metric: acc=0.8474770642201835, hinge=0.7841093783531714, ce=0.46305508384323
Local test acc @ epoch 69: 0.8475
Client 5 execute local training on 8 samples...
Local loss @ local epoch 0: 0.01621498167514801
Local loss @ local epoch 1: 0.010347158648073673
Local loss @ local epoch 2: 0.006861505098640919
Local loss @ local epoch 3: 0.004873578902333975
Local loss @ local epoch 4: 0.0038264605682343245
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.99 seconds!
[tester] 
SST2Metric: acc=0.8428899082568807, hinge=0.8007439556745214, ce=0.4669327741924571
Local test acc @ epoch 69: 0.8429
Global evaluate on test data...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.31 seconds!
[tester] 
SST2Metric: acc=0.805045871559633, hinge=0.9509860937475064, ce=0.5157118978816162
Global test acc @ epoch 69: 0.805
Global epoch 70...
Client 8 execute local training on 8 samples...
Local loss @ local epoch 0: 0.020236290991306305
Local loss @ local epoch 1: 0.013777772895991802
Local loss @ local epoch 2: 0.010126860812306404
Local loss @ local epoch 3: 0.008342321962118149
Local loss @ local epoch 4: 0.007729353383183479
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.89 seconds!
[tester] 
SST2Metric: acc=0.838302752293578, hinge=0.9177415000462751, ce=0.5635264653075965
Local test acc @ epoch 70: 0.8383
Client 2 execute local training on 8 samples...
Local loss @ local epoch 0: 0.015607696026563644
Local loss @ local epoch 1: 0.013769395649433136
Local loss @ local epoch 2: 0.012999685481190681
Local loss @ local epoch 3: 0.01231444999575615
Local loss @ local epoch 4: 0.011563473381102085
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.94 seconds!
[tester] 
SST2Metric: acc=0.8302752293577982, hinge=0.9442174590508873, ce=0.5798809136768136
Local test acc @ epoch 70: 0.8303
Client 1 execute local training on 8 samples...
Local loss @ local epoch 0: 0.003555330913513899
Local loss @ local epoch 1: 0.0024703554809093475
Local loss @ local epoch 2: 0.0018717708298936486
Local loss @ local epoch 3: 0.0015958633739501238
Local loss @ local epoch 4: 0.0015143700875341892
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.94 seconds!
[tester] 
SST2Metric: acc=0.8543577981651376, hinge=0.7716134178255676, ce=0.45333557412105213
Local test acc @ epoch 70: 0.8544
Client 7 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0204242505133152
Local loss @ local epoch 1: 0.0187851469963789
Local loss @ local epoch 2: 0.017474306747317314
Local loss @ local epoch 3: 0.0162956640124321
Local loss @ local epoch 4: 0.015361679717898369
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.9 seconds!
[tester] 
SST2Metric: acc=0.856651376146789, hinge=0.7636339697269124, ce=0.44509290454059053
Local test acc @ epoch 70: 0.8567
Client 6 execute local training on 8 samples...
Local loss @ local epoch 0: 0.12040586024522781
Local loss @ local epoch 1: 0.09156937152147293
Local loss @ local epoch 2: 0.07580115646123886
Local loss @ local epoch 3: 0.07033921778202057
Local loss @ local epoch 4: 0.07069141417741776
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.0 seconds!
[tester] 
SST2Metric: acc=0.8463302752293578, hinge=0.815597904384683, ce=0.47779023833893275
Local test acc @ epoch 70: 0.8463
Client 5 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0035195089876651764
Local loss @ local epoch 1: 0.003327731741592288
Local loss @ local epoch 2: 0.0031173662282526493
Local loss @ local epoch 3: 0.002939035650342703
Local loss @ local epoch 4: 0.002777400426566601
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.97 seconds!
[tester] 
SST2Metric: acc=0.841743119266055, hinge=0.8103810703262276, ce=0.47563273592642724
Local test acc @ epoch 70: 0.8417
Client 3 execute local training on 8 samples...
Local loss @ local epoch 0: 0.08993454277515411
Local loss @ local epoch 1: 0.08312715590000153
Local loss @ local epoch 2: 0.08257140964269638
Local loss @ local epoch 3: 0.08129538595676422
Local loss @ local epoch 4: 0.07831905037164688
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.0 seconds!
[tester] 
SST2Metric: acc=0.8474770642201835, hinge=0.7875117306862403, ce=0.4596550245213946
Local test acc @ epoch 70: 0.8475
Client 9 execute local training on 8 samples...
Local loss @ local epoch 0: 0.01581742987036705
Local loss @ local epoch 1: 0.0108101274818182
Local loss @ local epoch 2: 0.008167842403054237
Local loss @ local epoch 3: 0.007076699286699295
Local loss @ local epoch 4: 0.006888562813401222
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.96 seconds!
[tester] 
SST2Metric: acc=0.8497706422018348, hinge=0.7967074613232131, ce=0.4750032295267858
Local test acc @ epoch 70: 0.8498
Client 4 execute local training on 8 samples...
Local loss @ local epoch 0: 0.12424495816230774
Local loss @ local epoch 1: 0.10621169209480286
Local loss @ local epoch 2: 0.09155252575874329
Local loss @ local epoch 3: 0.0790717676281929
Local loss @ local epoch 4: 0.06837154179811478
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.88 seconds!
[tester] 
SST2Metric: acc=0.8532110091743119, hinge=0.7942438254115778, ce=0.474014798535991
Local test acc @ epoch 70: 0.8532
Client 0 execute local training on 8 samples...
Local loss @ local epoch 0: 0.005079213064163923
Local loss @ local epoch 1: 0.004655920900404453
Local loss @ local epoch 2: 0.004325288813561201
Local loss @ local epoch 3: 0.004066923633217812
Local loss @ local epoch 4: 0.003856796771287918
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.01 seconds!
[tester] 
SST2Metric: acc=0.8532110091743119, hinge=0.7805463876746116, ce=0.4614940507384444
Local test acc @ epoch 70: 0.8532
Global evaluate on test data...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.34 seconds!
[tester] 
SST2Metric: acc=0.805045871559633, hinge=0.9373674657913523, ce=0.5065058609043512
Global test acc @ epoch 70: 0.805
Global epoch 71...
Client 3 execute local training on 8 samples...
Local loss @ local epoch 0: 0.11576966196298599
Local loss @ local epoch 1: 0.09374324977397919
Local loss @ local epoch 2: 0.08233650773763657
Local loss @ local epoch 3: 0.07976670563220978
Local loss @ local epoch 4: 0.08174867182970047
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.92 seconds!
[tester] 
SST2Metric: acc=0.8440366972477065, hinge=0.802948123544728, ce=0.4677887606111979
Local test acc @ epoch 71: 0.844
Client 5 execute local training on 8 samples...
Local loss @ local epoch 0: 0.003028001869097352
Local loss @ local epoch 1: 0.0028695620130747557
Local loss @ local epoch 2: 0.002696144627407193
Local loss @ local epoch 3: 0.002550490666180849
Local loss @ local epoch 4: 0.0024163941852748394
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.94 seconds!
[tester] 
SST2Metric: acc=0.8474770642201835, hinge=0.8027812340937623, ce=0.4699518957843902
Local test acc @ epoch 71: 0.8475
Client 6 execute local training on 8 samples...
Local loss @ local epoch 0: 0.07055103778839111
Local loss @ local epoch 1: 0.06515543907880783
Local loss @ local epoch 2: 0.06022195890545845
Local loss @ local epoch 3: 0.05568185821175575
Local loss @ local epoch 4: 0.05152551084756851
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.93 seconds!
[tester] 
SST2Metric: acc=0.8532110091743119, hinge=0.7614569002335224, ce=0.4418661504847194
Local test acc @ epoch 71: 0.8532
Client 2 execute local training on 8 samples...
Local loss @ local epoch 0: 0.1503904163837433
Local loss @ local epoch 1: 0.10206105560064316
Local loss @ local epoch 2: 0.06908785551786423
Local loss @ local epoch 3: 0.047224074602127075
Local loss @ local epoch 4: 0.03296228125691414
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.91 seconds!
[tester] 
SST2Metric: acc=0.8543577981651376, hinge=0.8367702250086934, ce=0.5045886274924878
Local test acc @ epoch 71: 0.8544
Client 1 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0015278701903298497
Local loss @ local epoch 1: 0.0013918394688516855
Local loss @ local epoch 2: 0.00128878781106323
Local loss @ local epoch 3: 0.0011790591524913907
Local loss @ local epoch 4: 0.0010765212355181575
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.94 seconds!
[tester] 
SST2Metric: acc=0.8532110091743119, hinge=0.8273789881292833, ce=0.497999514591957
Local test acc @ epoch 71: 0.8532
Client 7 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0246720090508461
Local loss @ local epoch 1: 0.021666863933205605
Local loss @ local epoch 2: 0.020144253969192505
Local loss @ local epoch 3: 0.01872871443629265
Local loss @ local epoch 4: 0.017289862036705017
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.9 seconds!
[tester] 
SST2Metric: acc=0.8532110091743119, hinge=0.7629103984581221, ce=0.4513195694516063
Local test acc @ epoch 71: 0.8532
Client 8 execute local training on 8 samples...
Local loss @ local epoch 0: 0.03602280095219612
Local loss @ local epoch 1: 0.02387950010597706
Local loss @ local epoch 2: 0.01641622558236122
Local loss @ local epoch 3: 0.012090940028429031
Local loss @ local epoch 4: 0.00980268232524395
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.96 seconds!
[tester] 
SST2Metric: acc=0.8394495412844036, hinge=0.9331646844607975, ce=0.5778461670684052
Local test acc @ epoch 71: 0.8394
Client 0 execute local training on 8 samples...
Local loss @ local epoch 0: 0.009126922115683556
Local loss @ local epoch 1: 0.006332056131213903
Local loss @ local epoch 2: 0.004869960248470306
Local loss @ local epoch 3: 0.004306079354137182
Local loss @ local epoch 4: 0.004297751933336258
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.97 seconds!
[tester] 
SST2Metric: acc=0.856651376146789, hinge=0.7683322112494653, ce=0.4541478340338847
Local test acc @ epoch 71: 0.8567
Client 4 execute local training on 8 samples...
Local loss @ local epoch 0: 0.10574091970920563
Local loss @ local epoch 1: 0.08468885719776154
Local loss @ local epoch 2: 0.07248666137456894
Local loss @ local epoch 3: 0.06506458669900894
Local loss @ local epoch 4: 0.059137724339962006
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.92 seconds!
[tester] 
SST2Metric: acc=0.8532110091743119, hinge=0.8219618895731935, ce=0.4958498493806724
Local test acc @ epoch 71: 0.8532
Client 9 execute local training on 8 samples...
Local loss @ local epoch 0: 0.014986848458647728
Local loss @ local epoch 1: 0.011550231836736202
Local loss @ local epoch 2: 0.010226949118077755
Local loss @ local epoch 3: 0.009768354706466198
Local loss @ local epoch 4: 0.00938045047223568
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.89 seconds!
[tester] 
SST2Metric: acc=0.856651376146789, hinge=0.7610545381220108, ce=0.4481118950237725
Local test acc @ epoch 71: 0.8567
Global evaluate on test data...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.34 seconds!
[tester] 
SST2Metric: acc=0.8038990825688074, hinge=0.9162430153527391, ce=0.4908020541353852
Global test acc @ epoch 71: 0.8039
Global epoch 72...
Client 5 execute local training on 8 samples...
Local loss @ local epoch 0: 0.006760349962860346
Local loss @ local epoch 1: 0.004410724621266127
Local loss @ local epoch 2: 0.0031075929291546345
Local loss @ local epoch 3: 0.0024771082680672407
Local loss @ local epoch 4: 0.002277261344715953
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.93 seconds!
[tester] 
SST2Metric: acc=0.8463302752293578, hinge=0.8216843116994298, ce=0.4845716645311
Local test acc @ epoch 72: 0.8463
Client 4 execute local training on 8 samples...
Local loss @ local epoch 0: 0.2460831254720688
Local loss @ local epoch 1: 0.18584401905536652
Local loss @ local epoch 2: 0.13681815564632416
Local loss @ local epoch 3: 0.09978695958852768
Local loss @ local epoch 4: 0.07445035874843597
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.87 seconds!
[tester] 
SST2Metric: acc=0.8509174311926605, hinge=0.7629025812269351, ce=0.4517409412388972
Local test acc @ epoch 72: 0.8509
Client 7 execute local training on 8 samples...
Local loss @ local epoch 0: 0.017536388710141182
Local loss @ local epoch 1: 0.01626385748386383
Local loss @ local epoch 2: 0.015218952670693398
Local loss @ local epoch 3: 0.014230801723897457
Local loss @ local epoch 4: 0.0134264025837183
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.94 seconds!
[tester] 
SST2Metric: acc=0.8635321100917431, hinge=0.7598877009175239, ce=0.44664262760956047
Local test acc @ epoch 72: 0.8635
Client 8 execute local training on 8 samples...
Local loss @ local epoch 0: 0.027403853833675385
Local loss @ local epoch 1: 0.01815664768218994
Local loss @ local epoch 2: 0.012593185529112816
Local loss @ local epoch 3: 0.009470699355006218
Local loss @ local epoch 4: 0.007924214005470276
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.92 seconds!
[tester] 
SST2Metric: acc=0.8451834862385321, hinge=0.884348272867159, ce=0.5436435912745372
Local test acc @ epoch 72: 0.8452
Client 9 execute local training on 8 samples...
Local loss @ local epoch 0: 0.014219602569937706
Local loss @ local epoch 1: 0.009433187544345856
Local loss @ local epoch 2: 0.006939595565199852
Local loss @ local epoch 3: 0.005892109591513872
Local loss @ local epoch 4: 0.0056809429079294205
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.91 seconds!
[tester] 
SST2Metric: acc=0.8577981651376146, hinge=0.7779684584895405, ce=0.4600155989782577
Local test acc @ epoch 72: 0.8578
Client 2 execute local training on 8 samples...
Local loss @ local epoch 0: 0.040841858834028244
Local loss @ local epoch 1: 0.028101004660129547
Local loss @ local epoch 2: 0.02078002691268921
Local loss @ local epoch 3: 0.01703951135277748
Local loss @ local epoch 4: 0.015444010496139526
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.92 seconds!
[tester] 
SST2Metric: acc=0.8440366972477065, hinge=0.9017431372349415, ce=0.5568628678778657
Local test acc @ epoch 72: 0.844
Client 6 execute local training on 8 samples...
Local loss @ local epoch 0: 0.42257243394851685
Local loss @ local epoch 1: 0.33546897768974304
Local loss @ local epoch 2: 0.25905856490135193
Local loss @ local epoch 3: 0.19538448750972748
Local loss @ local epoch 4: 0.1455552875995636
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.86 seconds!
[tester] 
SST2Metric: acc=0.8612385321100917, hinge=0.765643950859341, ce=0.4501891094665769
Local test acc @ epoch 72: 0.8612
Client 1 execute local training on 8 samples...
Local loss @ local epoch 0: 0.002367334673181176
Local loss @ local epoch 1: 0.001878750859759748
Local loss @ local epoch 2: 0.0016906821401789784
Local loss @ local epoch 3: 0.001614158507436514
Local loss @ local epoch 4: 0.0015388953033834696
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.91 seconds!
[tester] 
SST2Metric: acc=0.8497706422018348, hinge=0.8137249294497552, ce=0.4919438173795768
Local test acc @ epoch 72: 0.8498
Client 3 execute local training on 8 samples...
Local loss @ local epoch 0: 0.16016051173210144
Local loss @ local epoch 1: 0.12478742003440857
Local loss @ local epoch 2: 0.10022490471601486
Local loss @ local epoch 3: 0.0863962471485138
Local loss @ local epoch 4: 0.08175172656774521
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.91 seconds!
[tester] 
SST2Metric: acc=0.8520642201834863, hinge=0.7664180797174436, ce=0.4471712477196546
Local test acc @ epoch 72: 0.8521
Client 0 execute local training on 8 samples...
Local loss @ local epoch 0: 0.011503609828650951
Local loss @ local epoch 1: 0.007766316644847393
Local loss @ local epoch 2: 0.005665907636284828
Local loss @ local epoch 3: 0.004667819477617741
Local loss @ local epoch 4: 0.004377232864499092
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.87 seconds!
[tester] 
SST2Metric: acc=0.8497706422018348, hinge=0.8000569589641116, ce=0.481270748932863
Local test acc @ epoch 72: 0.8498
Global evaluate on test data...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.19 seconds!
[tester] 
SST2Metric: acc=0.801605504587156, hinge=0.9405290764679602, ce=0.5119191627979381
Global test acc @ epoch 72: 0.8016
Global epoch 73...
Client 5 execute local training on 8 samples...
Local loss @ local epoch 0: 0.006553588900715113
Local loss @ local epoch 1: 0.00427580950781703
Local loss @ local epoch 2: 0.002991804387420416
Local loss @ local epoch 3: 0.0023485030978918076
Local loss @ local epoch 4: 0.002118760487064719
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.95 seconds!
[tester] 
SST2Metric: acc=0.8463302752293578, hinge=0.8238654054632975, ce=0.4887823461354155
Local test acc @ epoch 73: 0.8463
Client 2 execute local training on 8 samples...
Local loss @ local epoch 0: 0.16851729154586792
Local loss @ local epoch 1: 0.11442046612501144
Local loss @ local epoch 2: 0.07720404118299484
Local loss @ local epoch 3: 0.05253888666629791
Local loss @ local epoch 4: 0.036584146320819855
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.99 seconds!
[tester] 
SST2Metric: acc=0.8520642201834863, hinge=0.7760803178636306, ce=0.46173629334886396
Local test acc @ epoch 73: 0.8521
Client 8 execute local training on 8 samples...
Local loss @ local epoch 0: 0.015480197966098785
Local loss @ local epoch 1: 0.01073660422116518
Local loss @ local epoch 2: 0.00818256102502346
Local loss @ local epoch 3: 0.007068066857755184
Local loss @ local epoch 4: 0.006808116566389799
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.02 seconds!
[tester] 
SST2Metric: acc=0.8451834862385321, hinge=0.9306991459853059, ce=0.5786078130730765
Local test acc @ epoch 73: 0.8452
Client 9 execute local training on 8 samples...
Local loss @ local epoch 0: 0.01704994961619377
Local loss @ local epoch 1: 0.010957247577607632
Local loss @ local epoch 2: 0.007559289690107107
Local loss @ local epoch 3: 0.005863172933459282
Local loss @ local epoch 4: 0.005212876014411449
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.96 seconds!
[tester] 
SST2Metric: acc=0.8589449541284404, hinge=0.7865757927161838, ce=0.4679230253181181
Local test acc @ epoch 73: 0.8589
Client 4 execute local training on 8 samples...
Local loss @ local epoch 0: 0.1435011774301529
Local loss @ local epoch 1: 0.12133905291557312
Local loss @ local epoch 2: 0.10517333447933197
Local loss @ local epoch 3: 0.09201601892709732
Local loss @ local epoch 4: 0.08049105107784271
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.95 seconds!
[tester] 
SST2Metric: acc=0.8486238532110092, hinge=0.7837287996887067, ce=0.4681743395027764
Local test acc @ epoch 73: 0.8486
Client 3 execute local training on 8 samples...
Local loss @ local epoch 0: 0.14811067283153534
Local loss @ local epoch 1: 0.11583983898162842
Local loss @ local epoch 2: 0.09371960908174515
Local loss @ local epoch 3: 0.08178574591875076
Local loss @ local epoch 4: 0.0784618929028511
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.95 seconds!
[tester] 
SST2Metric: acc=0.8440366972477065, hinge=0.7897003294404493, ce=0.46247358576591135
Local test acc @ epoch 73: 0.844
Client 1 execute local training on 8 samples...
Local loss @ local epoch 0: 0.004827911499887705
Local loss @ local epoch 1: 0.0031281919218599796
Local loss @ local epoch 2: 0.002150154672563076
Local loss @ local epoch 3: 0.0016240563709288836
Local loss @ local epoch 4: 0.0013745703035965562
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.99 seconds!
[tester] 
SST2Metric: acc=0.8520642201834863, hinge=0.7853536045332568, ce=0.47003328518027765
Local test acc @ epoch 73: 0.8521
Client 0 execute local training on 8 samples...
Local loss @ local epoch 0: 0.004850444383919239
Local loss @ local epoch 1: 0.004482127260416746
Local loss @ local epoch 2: 0.004188658203929663
Local loss @ local epoch 3: 0.00395770650357008
Local loss @ local epoch 4: 0.003764381865039468
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.95 seconds!
[tester] 
SST2Metric: acc=0.8555045871559633, hinge=0.7791060678182392, ce=0.4626289993191326
Local test acc @ epoch 73: 0.8555
Client 7 execute local training on 8 samples...
Local loss @ local epoch 0: 0.014555331319570541
Local loss @ local epoch 1: 0.012715744785964489
Local loss @ local epoch 2: 0.012615478597581387
Local loss @ local epoch 3: 0.012616091407835484
Local loss @ local epoch 4: 0.012168042361736298
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.94 seconds!
[tester] 
SST2Metric: acc=0.8543577981651376, hinge=0.7836927498152496, ce=0.4618577697023818
Local test acc @ epoch 73: 0.8544
Client 6 execute local training on 8 samples...
Local loss @ local epoch 0: 0.10445737838745117
Local loss @ local epoch 1: 0.08345525711774826
Local loss @ local epoch 2: 0.07440982758998871
Local loss @ local epoch 3: 0.07283075153827667
Local loss @ local epoch 4: 0.07293509691953659
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.92 seconds!
[tester] 
SST2Metric: acc=0.8474770642201835, hinge=0.8383602997578612, ce=0.49968218184375296
Local test acc @ epoch 73: 0.8475
Global evaluate on test data...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.38 seconds!
[tester] 
SST2Metric: acc=0.8084862385321101, hinge=0.9252602449797709, ce=0.5000915964082772
Global test acc @ epoch 73: 0.8085
Global epoch 74...
Client 3 execute local training on 8 samples...
Local loss @ local epoch 0: 0.10608880966901779
Local loss @ local epoch 1: 0.08633162081241608
Local loss @ local epoch 2: 0.07671472430229187
Local loss @ local epoch 3: 0.07520761340856552
Local loss @ local epoch 4: 0.07730207592248917
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.91 seconds!
[tester] 
SST2Metric: acc=0.8463302752293578, hinge=0.8241176404263995, ce=0.4881164442626076
Local test acc @ epoch 74: 0.8463
Client 9 execute local training on 8 samples...
Local loss @ local epoch 0: 0.02782447263598442
Local loss @ local epoch 1: 0.018170658499002457
Local loss @ local epoch 2: 0.012416084297001362
Local loss @ local epoch 3: 0.009222004562616348
Local loss @ local epoch 4: 0.007669687271118164
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.91 seconds!
[tester] 
SST2Metric: acc=0.8543577981651376, hinge=0.7743450785995624, ce=0.4600305430935012
Local test acc @ epoch 74: 0.8544
Client 1 execute local training on 8 samples...
Local loss @ local epoch 0: 0.001804819330573082
Local loss @ local epoch 1: 0.0016120137879624963
Local loss @ local epoch 2: 0.0014647662173956633
Local loss @ local epoch 3: 0.0013234183425083756
Local loss @ local epoch 4: 0.0011946986196562648
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.88 seconds!
[tester] 
SST2Metric: acc=0.8532110091743119, hinge=0.7931563214424553, ce=0.4769724375010818
Local test acc @ epoch 74: 0.8532
Client 7 execute local training on 8 samples...
Local loss @ local epoch 0: 0.016788126900792122
Local loss @ local epoch 1: 0.014895928092300892
Local loss @ local epoch 2: 0.014348908327519894
Local loss @ local epoch 3: 0.013742364011704922
Local loss @ local epoch 4: 0.012939848005771637
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.86 seconds!
[tester] 
SST2Metric: acc=0.8635321100917431, hinge=0.7601525847791532, ce=0.44855125185097955
Local test acc @ epoch 74: 0.8635
Client 0 execute local training on 8 samples...
Local loss @ local epoch 0: 0.005149257835000753
Local loss @ local epoch 1: 0.004210778046399355
Local loss @ local epoch 2: 0.0040258681401610374
Local loss @ local epoch 3: 0.0041086808778345585
Local loss @ local epoch 4: 0.004089448135346174
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.89 seconds!
[tester] 
SST2Metric: acc=0.8497706422018348, hinge=0.7971841176168635, ce=0.4794174170318068
Local test acc @ epoch 74: 0.8498
Client 8 execute local training on 8 samples...
Local loss @ local epoch 0: 0.010921638458967209
Local loss @ local epoch 1: 0.00791390985250473
Local loss @ local epoch 2: 0.006471854168921709
Local loss @ local epoch 3: 0.006007721181958914
Local loss @ local epoch 4: 0.006002811249345541
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.91 seconds!
[tester] 
SST2Metric: acc=0.8371559633027523, hinge=0.9505287068426063, ce=0.5951318077087817
Local test acc @ epoch 74: 0.8372
Client 4 execute local training on 8 samples...
Local loss @ local epoch 0: 0.2322860062122345
Local loss @ local epoch 1: 0.17230992019176483
Local loss @ local epoch 2: 0.1315348595380783
Local loss @ local epoch 3: 0.10681154578924179
Local loss @ local epoch 4: 0.09346215426921844
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.89 seconds!
[tester] 
SST2Metric: acc=0.856651376146789, hinge=0.7816222348344435, ce=0.46628140173895166
Local test acc @ epoch 74: 0.8567
Client 2 execute local training on 8 samples...
Local loss @ local epoch 0: 0.022943224757909775
Local loss @ local epoch 1: 0.01634025014936924
Local loss @ local epoch 2: 0.012849808670580387
Local loss @ local epoch 3: 0.011404660530388355
Local loss @ local epoch 4: 0.01111739594489336
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.92 seconds!
[tester] 
SST2Metric: acc=0.8371559633027523, hinge=0.9630104768166848, ce=0.6024025624610992
Local test acc @ epoch 74: 0.8372
Client 5 execute local training on 8 samples...
Local loss @ local epoch 0: 0.04865894094109535
Local loss @ local epoch 1: 0.030799372121691704
Local loss @ local epoch 2: 0.019726522266864777
Local loss @ local epoch 3: 0.012928798794746399
Local loss @ local epoch 4: 0.008737482130527496
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.94 seconds!
[tester] 
SST2Metric: acc=0.8555045871559633, hinge=0.7778159783247414, ce=0.46252590690316514
Local test acc @ epoch 74: 0.8555
Client 6 execute local training on 8 samples...
Local loss @ local epoch 0: 0.15369442105293274
Local loss @ local epoch 1: 0.11315378546714783
Local loss @ local epoch 2: 0.08633493632078171
Local loss @ local epoch 3: 0.07161083817481995
Local loss @ local epoch 4: 0.06628367304801941
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.94 seconds!
[tester] 
SST2Metric: acc=0.8463302752293578, hinge=0.8214651608685835, ce=0.48754901484556395
Local test acc @ epoch 74: 0.8463
Global evaluate on test data...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.37 seconds!
[tester] 
SST2Metric: acc=0.7993119266055045, hinge=0.9617657336073184, ce=0.5300002606807772
Global test acc @ epoch 74: 0.7993
Global epoch 75...
Client 3 execute local training on 8 samples...
Local loss @ local epoch 0: 0.12094609439373016
Local loss @ local epoch 1: 0.09492086619138718
Local loss @ local epoch 2: 0.079031802713871
Local loss @ local epoch 3: 0.07250454276800156
Local loss @ local epoch 4: 0.072686105966568
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.09 seconds!
[tester] 
SST2Metric: acc=0.8451834862385321, hinge=0.8073846209760106, ce=0.4758338721366514
Local test acc @ epoch 75: 0.8452
Client 0 execute local training on 8 samples...
Local loss @ local epoch 0: 0.012483987025916576
Local loss @ local epoch 1: 0.008223529905080795
Local loss @ local epoch 2: 0.005699621979147196
Local loss @ local epoch 3: 0.004333216696977615
Local loss @ local epoch 4: 0.0037242406979203224
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.93 seconds!
[tester] 
SST2Metric: acc=0.8520642201834863, hinge=0.7820180610779228, ce=0.46573270236594827
Local test acc @ epoch 75: 0.8521
Client 1 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0015436970861628652
Local loss @ local epoch 1: 0.0013780678855255246
Local loss @ local epoch 2: 0.0012345496797934175
Local loss @ local epoch 3: 0.0011076783994212747
Local loss @ local epoch 4: 0.000997252413071692
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.9 seconds!
[tester] 
SST2Metric: acc=0.8497706422018348, hinge=0.7959204137051871, ce=0.4789835629907838
Local test acc @ epoch 75: 0.8498
Client 7 execute local training on 8 samples...
Local loss @ local epoch 0: 0.016608459874987602
Local loss @ local epoch 1: 0.01487751305103302
Local loss @ local epoch 2: 0.014204706065356731
Local loss @ local epoch 3: 0.013448893092572689
Local loss @ local epoch 4: 0.012598332948982716
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.95 seconds!
[tester] 
SST2Metric: acc=0.8635321100917431, hinge=0.7639900698847727, ce=0.451897481036663
Local test acc @ epoch 75: 0.8635
Client 5 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0051290798000991344
Local loss @ local epoch 1: 0.003419701009988785
Local loss @ local epoch 2: 0.00250700069591403
Local loss @ local epoch 3: 0.0021155469585210085
Local loss @ local epoch 4: 0.0020535504445433617
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.81 seconds!
[tester] 
SST2Metric: acc=0.8405963302752294, hinge=0.8500926653179554, ce=0.5091582640210901
Local test acc @ epoch 75: 0.8406
Client 9 execute local training on 8 samples...
Local loss @ local epoch 0: 0.035569533705711365
Local loss @ local epoch 1: 0.023042086511850357
Local loss @ local epoch 2: 0.015348020941019058
Local loss @ local epoch 3: 0.010831666179001331
Local loss @ local epoch 4: 0.008349072188138962
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.76 seconds!
[tester] 
SST2Metric: acc=0.8600917431192661, hinge=0.7702244569402222, ce=0.45799412078933294
Local test acc @ epoch 75: 0.8601
Client 4 execute local training on 8 samples...
Local loss @ local epoch 0: 0.09334435313940048
Local loss @ local epoch 1: 0.07629323750734329
Local loss @ local epoch 2: 0.0663600042462349
Local loss @ local epoch 3: 0.059459537267684937
Local loss @ local epoch 4: 0.053342703729867935
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.7 seconds!
[tester] 
SST2Metric: acc=0.8520642201834863, hinge=0.7965932651123869, ce=0.48130855887515234
Local test acc @ epoch 75: 0.8521
Client 8 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0203709714114666
Local loss @ local epoch 1: 0.013826059177517891
Local loss @ local epoch 2: 0.010068248957395554
Local loss @ local epoch 3: 0.008144119754433632
Local loss @ local epoch 4: 0.00735317450016737
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.69 seconds!
[tester] 
SST2Metric: acc=0.8291284403669725, hinge=1.0086888405707999, ce=0.6369519525398757
Local test acc @ epoch 75: 0.8291
Client 6 execute local training on 8 samples...
Local loss @ local epoch 0: 0.4013913571834564
Local loss @ local epoch 1: 0.31576985120773315
Local loss @ local epoch 2: 0.24121081829071045
Local loss @ local epoch 3: 0.17958150804042816
Local loss @ local epoch 4: 0.13164889812469482
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.7 seconds!
[tester] 
SST2Metric: acc=0.8532110091743119, hinge=0.7755778429158237, ce=0.46410565407265303
Local test acc @ epoch 75: 0.8532
Client 2 execute local training on 8 samples...
Local loss @ local epoch 0: 0.03169560432434082
Local loss @ local epoch 1: 0.021390793845057487
Local loss @ local epoch 2: 0.01510971412062645
Local loss @ local epoch 3: 0.011554138734936714
Local loss @ local epoch 4: 0.009818894788622856
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.69 seconds!
[tester] 
SST2Metric: acc=0.8291284403669725, hinge=0.9905244426442943, ce=0.6229962013261884
Local test acc @ epoch 75: 0.8291
Global evaluate on test data...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.08 seconds!
[tester] 
SST2Metric: acc=0.8027522935779816, hinge=0.9530483865683231, ce=0.5245759128891957
Global test acc @ epoch 75: 0.8028
Global epoch 76...
Client 1 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0011897793738171458
Local loss @ local epoch 1: 0.001067759352736175
Local loss @ local epoch 2: 0.0009766920702531934
Local loss @ local epoch 3: 0.0008878757944330573
Local loss @ local epoch 4: 0.0008058494422584772
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.71 seconds!
[tester] 
SST2Metric: acc=0.8543577981651376, hinge=0.8029207562634705, ce=0.4856355465642057
Local test acc @ epoch 76: 0.8544
Client 2 execute local training on 8 samples...
Local loss @ local epoch 0: 0.03151322528719902
Local loss @ local epoch 1: 0.021213393658399582
Local loss @ local epoch 2: 0.014815679751336575
Local loss @ local epoch 3: 0.011077485978603363
Local loss @ local epoch 4: 0.009127659723162651
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.73 seconds!
[tester] 
SST2Metric: acc=0.8153669724770642, hinge=1.0617511821996182, ce=0.6732471227459502
Local test acc @ epoch 76: 0.8154
Client 0 execute local training on 8 samples...
Local loss @ local epoch 0: 0.01813530921936035
Local loss @ local epoch 1: 0.011626124382019043
Local loss @ local epoch 2: 0.007817736826837063
Local loss @ local epoch 3: 0.005699289031326771
Local loss @ local epoch 4: 0.004625735338777304
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.67 seconds!
[tester] 
SST2Metric: acc=0.8509174311926605, hinge=0.7932019627422368, ce=0.47849638457036225
Local test acc @ epoch 76: 0.8509
Client 9 execute local training on 8 samples...
Local loss @ local epoch 0: 0.010498209856450558
Local loss @ local epoch 1: 0.008935626596212387
Local loss @ local epoch 2: 0.008237040601670742
Local loss @ local epoch 3: 0.0076133860275149345
Local loss @ local epoch 4: 0.006925787776708603
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.61 seconds!
[tester] 
SST2Metric: acc=0.856651376146789, hinge=0.777798799883335, ce=0.4645602201787678
Local test acc @ epoch 76: 0.8567
Client 8 execute local training on 8 samples...
Local loss @ local epoch 0: 0.01685350015759468
Local loss @ local epoch 1: 0.01121705211699009
Local loss @ local epoch 2: 0.00791272334754467
Local loss @ local epoch 3: 0.006140394136309624
Local loss @ local epoch 4: 0.0053480202332139015
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.66 seconds!
[tester] 
SST2Metric: acc=0.8405963302752294, hinge=0.9201866105335568, ce=0.5746651120823509
Local test acc @ epoch 76: 0.8406
Client 7 execute local training on 8 samples...
Local loss @ local epoch 0: 0.04992545396089554
Local loss @ local epoch 1: 0.03328152000904083
Local loss @ local epoch 2: 0.0227211881428957
Local loss @ local epoch 3: 0.01634003035724163
Local loss @ local epoch 4: 0.012755472213029861
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.64 seconds!
[tester] 
SST2Metric: acc=0.8612385321100917, hinge=0.77916447640559, ce=0.464429649919562
Local test acc @ epoch 76: 0.8612
Client 6 execute local training on 8 samples...
Local loss @ local epoch 0: 0.132868692278862
Local loss @ local epoch 1: 0.09878060966730118
Local loss @ local epoch 2: 0.07783588021993637
Local loss @ local epoch 3: 0.06801091134548187
Local loss @ local epoch 4: 0.06601081788539886
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.72 seconds!
[tester] 
SST2Metric: acc=0.8497706422018348, hinge=0.8342020764536814, ce=0.499646156504024
Local test acc @ epoch 76: 0.8498
Client 3 execute local training on 8 samples...
Local loss @ local epoch 0: 0.08515583723783493
Local loss @ local epoch 1: 0.07540426403284073
Local loss @ local epoch 2: 0.07375826686620712
Local loss @ local epoch 3: 0.07428339868783951
Local loss @ local epoch 4: 0.07296227663755417
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.83 seconds!
[tester] 
SST2Metric: acc=0.8509174311926605, hinge=0.7911075866277065, ce=0.4689241779873088
Local test acc @ epoch 76: 0.8509
Client 4 execute local training on 8 samples...
Local loss @ local epoch 0: 0.1511273831129074
Local loss @ local epoch 1: 0.11208831518888474
Local loss @ local epoch 2: 0.08543086051940918
Local loss @ local epoch 3: 0.06965455412864685
Local loss @ local epoch 4: 0.06163911893963814
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.85 seconds!
[tester] 
SST2Metric: acc=0.8520642201834863, hinge=0.7955069971193961, ce=0.48066431106871355
Local test acc @ epoch 76: 0.8521
Client 5 execute local training on 8 samples...
Local loss @ local epoch 0: 0.010525213554501534
Local loss @ local epoch 1: 0.006652233190834522
Local loss @ local epoch 2: 0.004360356368124485
Local loss @ local epoch 3: 0.0030458075925707817
Local loss @ local epoch 4: 0.0023344873916357756
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.85 seconds!
[tester] 
SST2Metric: acc=0.8440366972477065, hinge=0.7965904884382126, ce=0.471599169703491
Local test acc @ epoch 76: 0.844
Global evaluate on test data...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.22 seconds!
[tester] 
SST2Metric: acc=0.801605504587156, hinge=0.9589789671088578, ce=0.5301080802028332
Global test acc @ epoch 76: 0.8016
Global epoch 77...
Client 5 execute local training on 8 samples...
Local loss @ local epoch 0: 0.007010162808001041
Local loss @ local epoch 1: 0.00451688701286912
Local loss @ local epoch 2: 0.0030813170596957207
Local loss @ local epoch 3: 0.0023164842277765274
Local loss @ local epoch 4: 0.0019793095998466015
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.81 seconds!
[tester] 
SST2Metric: acc=0.8486238532110092, hinge=0.8258370568446063, ce=0.4931351689188186
Local test acc @ epoch 77: 0.8486
Client 3 execute local training on 8 samples...
Local loss @ local epoch 0: 0.07794227451086044
Local loss @ local epoch 1: 0.07052796334028244
Local loss @ local epoch 2: 0.0699026808142662
Local loss @ local epoch 3: 0.06959506124258041
Local loss @ local epoch 4: 0.06740757822990417
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.83 seconds!
[tester] 
SST2Metric: acc=0.8440366972477065, hinge=0.7939210162250274, ce=0.4702436223107074
Local test acc @ epoch 77: 0.844
Client 4 execute local training on 8 samples...
Local loss @ local epoch 0: 0.1545582115650177
Local loss @ local epoch 1: 0.11410953104496002
Local loss @ local epoch 2: 0.0860426276922226
Local loss @ local epoch 3: 0.06909485161304474
Local loss @ local epoch 4: 0.06034038960933685
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.87 seconds!
[tester] 
SST2Metric: acc=0.8520642201834863, hinge=0.7886038458128588, ce=0.47520197042287543
Local test acc @ epoch 77: 0.8521
Client 2 execute local training on 8 samples...
Local loss @ local epoch 0: 0.021195193752646446
Local loss @ local epoch 1: 0.014838648028671741
Local loss @ local epoch 2: 0.011328017339110374
Local loss @ local epoch 3: 0.009749909862875938
Local loss @ local epoch 4: 0.00939137488603592
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.86 seconds!
[tester] 
SST2Metric: acc=0.8245412844036697, hinge=1.0177633436994815, ce=0.643666860628275
Local test acc @ epoch 77: 0.8245
Client 7 execute local training on 8 samples...
Local loss @ local epoch 0: 0.06275283545255661
Local loss @ local epoch 1: 0.04140258952975273
Local loss @ local epoch 2: 0.027932940050959587
Local loss @ local epoch 3: 0.019753238186240196
Local loss @ local epoch 4: 0.015033643692731857
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.96 seconds!
[tester] 
SST2Metric: acc=0.8520642201834863, hinge=0.7803905857812374, ce=0.468320373400358
Local test acc @ epoch 77: 0.8521
Client 0 execute local training on 8 samples...
Local loss @ local epoch 0: 0.003737224033102393
Local loss @ local epoch 1: 0.0035197546239942312
Local loss @ local epoch 2: 0.003321355441585183
Local loss @ local epoch 3: 0.003152696415781975
Local loss @ local epoch 4: 0.0030114278197288513
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.94 seconds!
[tester] 
SST2Metric: acc=0.8577981651376146, hinge=0.7830731885148845, ce=0.4673005204472622
Local test acc @ epoch 77: 0.8578
Client 6 execute local training on 8 samples...
Local loss @ local epoch 0: 0.16304858028888702
Local loss @ local epoch 1: 0.12015067785978317
Local loss @ local epoch 2: 0.09129922837018967
Local loss @ local epoch 3: 0.07500098645687103
Local loss @ local epoch 4: 0.06864838302135468
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.92 seconds!
[tester] 
SST2Metric: acc=0.8474770642201835, hinge=0.8321810426241761, ce=0.4972980492972606
Local test acc @ epoch 77: 0.8475
Client 9 execute local training on 8 samples...
Local loss @ local epoch 0: 0.027579713612794876
Local loss @ local epoch 1: 0.01799742504954338
Local loss @ local epoch 2: 0.012279493734240532
Local loss @ local epoch 3: 0.009097251109778881
Local loss @ local epoch 4: 0.007536178920418024
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.01 seconds!
[tester] 
SST2Metric: acc=0.8543577981651376, hinge=0.783701494746252, ce=0.46956639849033993
Local test acc @ epoch 77: 0.8544
Client 1 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0012957199942320585
Local loss @ local epoch 1: 0.0011589955538511276
Local loss @ local epoch 2: 0.0010454232105985284
Local loss @ local epoch 3: 0.0009410657221451402
Local loss @ local epoch 4: 0.0008488002931699157
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.01 seconds!
[tester] 
SST2Metric: acc=0.8497706422018348, hinge=0.7978133976186087, ce=0.4830251118037935
Local test acc @ epoch 77: 0.8498
Client 8 execute local training on 8 samples...
Local loss @ local epoch 0: 0.02215832844376564
Local loss @ local epoch 1: 0.014828590676188469
Local loss @ local epoch 2: 0.010493354871869087
Local loss @ local epoch 3: 0.008134940639138222
Local loss @ local epoch 4: 0.007029206957668066
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.89 seconds!
[tester] 
SST2Metric: acc=0.8279816513761468, hinge=1.0053537494819098, ce=0.6355050904190365
Local test acc @ epoch 77: 0.828
Global evaluate on test data...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.3 seconds!
[tester] 
SST2Metric: acc=0.8027522935779816, hinge=0.9579041769745154, ce=0.5289130951747845
Global test acc @ epoch 77: 0.8028
Global epoch 78...
Client 0 execute local training on 8 samples...
Local loss @ local epoch 0: 0.003412586636841297
Local loss @ local epoch 1: 0.003254614071920514
Local loss @ local epoch 2: 0.003123343223705888
Local loss @ local epoch 3: 0.0029595629312098026
Local loss @ local epoch 4: 0.002830651355907321
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.02 seconds!
[tester] 
SST2Metric: acc=0.8555045871559633, hinge=0.7853068826942269, ce=0.4688026453662074
Local test acc @ epoch 78: 0.8555
Client 3 execute local training on 8 samples...
Local loss @ local epoch 0: 0.10710394382476807
Local loss @ local epoch 1: 0.08275381475687027
Local loss @ local epoch 2: 0.0678267851471901
Local loss @ local epoch 3: 0.06150468438863754
Local loss @ local epoch 4: 0.06144804134964943
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.92 seconds!
[tester] 
SST2Metric: acc=0.8451834862385321, hinge=0.8390116068201328, ce=0.5021727127909507
Local test acc @ epoch 78: 0.8452
Client 4 execute local training on 8 samples...
Local loss @ local epoch 0: 0.1984136998653412
Local loss @ local epoch 1: 0.14615122973918915
Local loss @ local epoch 2: 0.10635032504796982
Local loss @ local epoch 3: 0.07891272008419037
Local loss @ local epoch 4: 0.06228611618280411
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.8 seconds!
[tester] 
SST2Metric: acc=0.8543577981651376, hinge=0.7790213202664612, ce=0.4651822226555642
Local test acc @ epoch 78: 0.8544
Client 5 execute local training on 8 samples...
Local loss @ local epoch 0: 0.006654322613030672
Local loss @ local epoch 1: 0.0042767031118273735
Local loss @ local epoch 2: 0.002925292821601033
Local loss @ local epoch 3: 0.002221633680164814
Local loss @ local epoch 4: 0.0019286382012069225
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.81 seconds!
[tester] 
SST2Metric: acc=0.8474770642201835, hinge=0.8307273397478488, ce=0.4958845276765765
Local test acc @ epoch 78: 0.8475
Client 2 execute local training on 8 samples...
Local loss @ local epoch 0: 0.13375435769557953
Local loss @ local epoch 1: 0.08949116617441177
Local loss @ local epoch 2: 0.05989743024110794
Local loss @ local epoch 3: 0.040641434490680695
Local loss @ local epoch 4: 0.028288230299949646
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.78 seconds!
[tester] 
SST2Metric: acc=0.8520642201834863, hinge=0.7851573000533865, ce=0.47033813333783914
Local test acc @ epoch 78: 0.8521
Client 8 execute local training on 8 samples...
Local loss @ local epoch 0: 0.015119524672627449
Local loss @ local epoch 1: 0.010204851627349854
Local loss @ local epoch 2: 0.007403417490422726
Local loss @ local epoch 3: 0.005992557387799025
Local loss @ local epoch 4: 0.0054563540033996105
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.76 seconds!
[tester] 
SST2Metric: acc=0.838302752293578, hinge=0.9444551950474398, ce=0.5915876407779159
Local test acc @ epoch 78: 0.8383
Client 6 execute local training on 8 samples...
Local loss @ local epoch 0: 0.46905189752578735
Local loss @ local epoch 1: 0.3771609961986542
Local loss @ local epoch 2: 0.29479044675827026
Local loss @ local epoch 3: 0.22428858280181885
Local loss @ local epoch 4: 0.16731896996498108
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.77 seconds!
[tester] 
SST2Metric: acc=0.856651376146789, hinge=0.7773397588128343, ce=0.46297543825167276
Local test acc @ epoch 78: 0.8567
Client 1 execute local training on 8 samples...
Local loss @ local epoch 0: 0.001297296374104917
Local loss @ local epoch 1: 0.00112563103903085
Local loss @ local epoch 2: 0.0010382159380242229
Local loss @ local epoch 3: 0.000958182499743998
Local loss @ local epoch 4: 0.0008732639835216105
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.68 seconds!
[tester] 
SST2Metric: acc=0.8509174311926605, hinge=0.8062141327136153, ce=0.48891353607765153
Local test acc @ epoch 78: 0.8509
Client 9 execute local training on 8 samples...
Local loss @ local epoch 0: 0.012515245005488396
Local loss @ local epoch 1: 0.010320331901311874
Local loss @ local epoch 2: 0.009423813782632351
Local loss @ local epoch 3: 0.008803261443972588
Local loss @ local epoch 4: 0.008094850927591324
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.74 seconds!
[tester] 
SST2Metric: acc=0.8589449541284404, hinge=0.7801365417624833, ce=0.46612957385721093
Local test acc @ epoch 78: 0.8589
Client 7 execute local training on 8 samples...
Local loss @ local epoch 0: 0.01114326249808073
Local loss @ local epoch 1: 0.010668868198990822
Local loss @ local epoch 2: 0.010129028931260109
Local loss @ local epoch 3: 0.009653961285948753
Local loss @ local epoch 4: 0.009286599233746529
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.74 seconds!
[tester] 
SST2Metric: acc=0.8612385321100917, hinge=0.7887971431290338, ce=0.4709509609442273
Local test acc @ epoch 78: 0.8612
Global evaluate on test data...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.08 seconds!
[tester] 
SST2Metric: acc=0.8038990825688074, hinge=0.9457230711475425, ce=0.5184172253282484
Global test acc @ epoch 78: 0.8039
Global epoch 79...
Client 9 execute local training on 8 samples...
Local loss @ local epoch 0: 0.008066634647548199
Local loss @ local epoch 1: 0.0071525233797729015
Local loss @ local epoch 2: 0.006576841697096825
Local loss @ local epoch 3: 0.005974663887172937
Local loss @ local epoch 4: 0.005407828371971846
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.69 seconds!
[tester] 
SST2Metric: acc=0.8577981651376146, hinge=0.8007703387956007, ce=0.48275184844846947
Local test acc @ epoch 79: 0.8578
Client 5 execute local training on 8 samples...
Local loss @ local epoch 0: 0.006415575742721558
Local loss @ local epoch 1: 0.004157838877290487
Local loss @ local epoch 2: 0.002837895415723324
Local loss @ local epoch 3: 0.0021186943631619215
Local loss @ local epoch 4: 0.0017865299014374614
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.67 seconds!
[tester] 
SST2Metric: acc=0.8405963302752294, hinge=0.8551901213893103, ce=0.5191742614234174
Local test acc @ epoch 79: 0.8406
Client 1 execute local training on 8 samples...
Local loss @ local epoch 0: 0.005286632105708122
Local loss @ local epoch 1: 0.0034255729988217354
Local loss @ local epoch 2: 0.002361841266974807
Local loss @ local epoch 3: 0.0017937252996489406
Local loss @ local epoch 4: 0.0015255865873768926
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.65 seconds!
[tester] 
SST2Metric: acc=0.8532110091743119, hinge=0.7949471825033153, ce=0.4792960603292092
Local test acc @ epoch 79: 0.8532
Client 2 execute local training on 8 samples...
Local loss @ local epoch 0: 0.01954258792102337
Local loss @ local epoch 1: 0.014158850535750389
Local loss @ local epoch 2: 0.011448458768427372
Local loss @ local epoch 3: 0.01040629856288433
Local loss @ local epoch 4: 0.010178925469517708
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.62 seconds!
[tester] 
SST2Metric: acc=0.841743119266055, hinge=0.9545295804465582, ce=0.6008365523943746
Local test acc @ epoch 79: 0.8417
Client 8 execute local training on 8 samples...
Local loss @ local epoch 0: 0.00533282570540905
Local loss @ local epoch 1: 0.004858019761741161
Local loss @ local epoch 2: 0.004526634234935045
Local loss @ local epoch 3: 0.004170790780335665
Local loss @ local epoch 4: 0.003838998731225729
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.63 seconds!
[tester] 
SST2Metric: acc=0.8451834862385321, hinge=0.8980623344215778, ce=0.5609426587259871
Local test acc @ epoch 79: 0.8452
Client 7 execute local training on 8 samples...
Local loss @ local epoch 0: 0.047006189823150635
Local loss @ local epoch 1: 0.03142636641860008
Local loss @ local epoch 2: 0.02149014174938202
Local loss @ local epoch 3: 0.015472285449504852
Local loss @ local epoch 4: 0.01209577452391386
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.49 seconds!
[tester] 
SST2Metric: acc=0.8555045871559633, hinge=0.8020429300879119, ce=0.4819459807749308
Local test acc @ epoch 79: 0.8555
Client 3 execute local training on 8 samples...
Local loss @ local epoch 0: 0.11155780404806137
Local loss @ local epoch 1: 0.08654619008302689
Local loss @ local epoch 2: 0.07087724655866623
Local loss @ local epoch 3: 0.06397714465856552
Local loss @ local epoch 4: 0.06366202980279922
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.62 seconds!
[tester] 
SST2Metric: acc=0.8394495412844036, hinge=0.8936776107330935, ce=0.5459721866704592
Local test acc @ epoch 79: 0.8394
Client 6 execute local training on 8 samples...
Local loss @ local epoch 0: 0.07943741977214813
Local loss @ local epoch 1: 0.0730673149228096
Local loss @ local epoch 2: 0.06986863911151886
Local loss @ local epoch 3: 0.06572581082582474
Local loss @ local epoch 4: 0.061204083263874054
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.55 seconds!
[tester] 
SST2Metric: acc=0.841743119266055, hinge=0.813566242609549, ce=0.48539231020750034
Local test acc @ epoch 79: 0.8417
Client 0 execute local training on 8 samples...
Local loss @ local epoch 0: 0.00771851884201169
Local loss @ local epoch 1: 0.005136486608535051
Local loss @ local epoch 2: 0.00366949918679893
Local loss @ local epoch 3: 0.0029399581253528595
Local loss @ local epoch 4: 0.002685446059331298
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.58 seconds!
[tester] 
SST2Metric: acc=0.8497706422018348, hinge=0.8024336885695064, ce=0.485134836306599
Local test acc @ epoch 79: 0.8498
Client 4 execute local training on 8 samples...
Local loss @ local epoch 0: 0.06802967935800552
Local loss @ local epoch 1: 0.05899102985858917
Local loss @ local epoch 2: 0.051551371812820435
Local loss @ local epoch 3: 0.04538952559232712
Local loss @ local epoch 4: 0.04031462222337723
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.62 seconds!
[tester] 
SST2Metric: acc=0.8555045871559633, hinge=0.7897154517950268, ce=0.475492210677983
Local test acc @ epoch 79: 0.8555
Global evaluate on test data...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.29 seconds!
[tester] 
SST2Metric: acc=0.805045871559633, hinge=0.9718279022415843, ce=0.5406845730874653
Global test acc @ epoch 79: 0.805
Global epoch 80...
Client 6 execute local training on 8 samples...
Local loss @ local epoch 0: 0.14416863024234772
Local loss @ local epoch 1: 0.10611214488744736
Local loss @ local epoch 2: 0.0815381407737732
Local loss @ local epoch 3: 0.06863170862197876
Local loss @ local epoch 4: 0.0645245611667633
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.63 seconds!
[tester] 
SST2Metric: acc=0.8486238532110092, hinge=0.8452869675575047, ce=0.5086957044352632
Local test acc @ epoch 80: 0.8486
Client 4 execute local training on 8 samples...
Local loss @ local epoch 0: 0.17407312989234924
Local loss @ local epoch 1: 0.12512671947479248
Local loss @ local epoch 2: 0.0895313024520874
Local loss @ local epoch 3: 0.06617005169391632
Local loss @ local epoch 4: 0.052723228931427
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.61 seconds!
[tester] 
SST2Metric: acc=0.8577981651376146, hinge=0.7820527198117807, ce=0.46889100558073216
Local test acc @ epoch 80: 0.8578
Client 8 execute local training on 8 samples...
Local loss @ local epoch 0: 0.02288377657532692
Local loss @ local epoch 1: 0.01499093696475029
Local loss @ local epoch 2: 0.010165248066186905
Local loss @ local epoch 3: 0.007347879931330681
Local loss @ local epoch 4: 0.005815132521092892
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.62 seconds!
[tester] 
SST2Metric: acc=0.8371559633027523, hinge=0.9424378811766249, ce=0.5919265687204535
Local test acc @ epoch 80: 0.8372
Client 0 execute local training on 8 samples...
Local loss @ local epoch 0: 0.00770715344697237
Local loss @ local epoch 1: 0.005161487963050604
Local loss @ local epoch 2: 0.003730867523699999
Local loss @ local epoch 3: 0.003033484099432826
Local loss @ local epoch 4: 0.0028092835564166307
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.72 seconds!
[tester] 
SST2Metric: acc=0.8577981651376146, hinge=0.7850590761101574, ce=0.470648915143769
Local test acc @ epoch 80: 0.8578
Client 2 execute local training on 8 samples...
Local loss @ local epoch 0: 0.02452351711690426
Local loss @ local epoch 1: 0.01669807732105255
Local loss @ local epoch 2: 0.012110013514757156
Local loss @ local epoch 3: 0.009696759283542633
Local loss @ local epoch 4: 0.008706692606210709
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.76 seconds!
[tester] 
SST2Metric: acc=0.8371559633027523, hinge=0.9440688244793394, ce=0.593291040505133
Local test acc @ epoch 80: 0.8372
Client 1 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0027637833263725042
Local loss @ local epoch 1: 0.0018574644345790148
Local loss @ local epoch 2: 0.0013292370131239295
Local loss @ local epoch 3: 0.0010489497799426317
Local loss @ local epoch 4: 0.0009255374898202717
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.8 seconds!
[tester] 
SST2Metric: acc=0.8543577981651376, hinge=0.7903955596576043, ce=0.4759823066094856
Local test acc @ epoch 80: 0.8544
Client 3 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0941687822341919
Local loss @ local epoch 1: 0.07568498700857162
Local loss @ local epoch 2: 0.06684590131044388
Local loss @ local epoch 3: 0.06546299904584885
Local loss @ local epoch 4: 0.06734181195497513
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.76 seconds!
[tester] 
SST2Metric: acc=0.8474770642201835, hinge=0.8044428404318084, ce=0.4786580438873039
Local test acc @ epoch 80: 0.8475
Client 7 execute local training on 8 samples...
Local loss @ local epoch 0: 0.023067116737365723
Local loss @ local epoch 1: 0.016359606757760048
Local loss @ local epoch 2: 0.012711785733699799
Local loss @ local epoch 3: 0.011210004799067974
Local loss @ local epoch 4: 0.01106648426502943
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.79 seconds!
[tester] 
SST2Metric: acc=0.8520642201834863, hinge=0.7928924168195199, ce=0.4778914160154965
Local test acc @ epoch 80: 0.8521
Client 5 execute local training on 8 samples...
Local loss @ local epoch 0: 0.007054068148136139
Local loss @ local epoch 1: 0.004504250828176737
Local loss @ local epoch 2: 0.003025929443538189
Local loss @ local epoch 3: 0.002218685345724225
Local loss @ local epoch 4: 0.0018338445806875825
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.84 seconds!
[tester] 
SST2Metric: acc=0.8474770642201835, hinge=0.8350714871642786, ce=0.5023671930309795
Local test acc @ epoch 80: 0.8475
Client 9 execute local training on 8 samples...
Local loss @ local epoch 0: 0.022498540580272675
Local loss @ local epoch 1: 0.014722868800163269
Local loss @ local epoch 2: 0.010151689872145653
Local loss @ local epoch 3: 0.007671014405786991
Local loss @ local epoch 4: 0.006519181653857231
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.84 seconds!
[tester] 
SST2Metric: acc=0.8532110091743119, hinge=0.7954010184204906, ce=0.4806285813833168
Local test acc @ epoch 80: 0.8532
Global evaluate on test data...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.28 seconds!
[tester] 
SST2Metric: acc=0.8061926605504587, hinge=0.9566741320518178, ce=0.5291401943995044
Global test acc @ epoch 80: 0.8062
Global epoch 81...
Client 7 execute local training on 8 samples...
Local loss @ local epoch 0: 0.011044198647141457
Local loss @ local epoch 1: 0.010500253178179264
Local loss @ local epoch 2: 0.009939865209162235
Local loss @ local epoch 3: 0.009453765116631985
Local loss @ local epoch 4: 0.009066024795174599
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.86 seconds!
[tester] 
SST2Metric: acc=0.8623853211009175, hinge=0.7904728849273209, ce=0.473298579383398
Local test acc @ epoch 81: 0.8624
Client 6 execute local training on 8 samples...
Local loss @ local epoch 0: 0.12223545461893082
Local loss @ local epoch 1: 0.0926676094532013
Local loss @ local epoch 2: 0.07599060237407684
Local loss @ local epoch 3: 0.06965948641300201
Local loss @ local epoch 4: 0.06949713826179504
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.81 seconds!
[tester] 
SST2Metric: acc=0.8463302752293578, hinge=0.8557052692962349, ce=0.5180355652209816
Local test acc @ epoch 81: 0.8463
Client 3 execute local training on 8 samples...
Local loss @ local epoch 0: 0.07256271690130234
Local loss @ local epoch 1: 0.06321355700492859
Local loss @ local epoch 2: 0.06146562099456787
Local loss @ local epoch 3: 0.06225663423538208
Local loss @ local epoch 4: 0.06153332442045212
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.94 seconds!
[tester] 
SST2Metric: acc=0.8474770642201835, hinge=0.8027303313170004, ce=0.48031328681914065
Local test acc @ epoch 81: 0.8475
Client 4 execute local training on 8 samples...
Local loss @ local epoch 0: 0.11718843132257462
Local loss @ local epoch 1: 0.08575595170259476
Local loss @ local epoch 2: 0.06602240353822708
Local loss @ local epoch 3: 0.055432580411434174
Local loss @ local epoch 4: 0.05051767826080322
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.83 seconds!
[tester] 
SST2Metric: acc=0.8520642201834863, hinge=0.7962386029849359, ce=0.4825464093164806
Local test acc @ epoch 81: 0.8521
Client 5 execute local training on 8 samples...
Local loss @ local epoch 0: 0.007841034792363644
Local loss @ local epoch 1: 0.004967307671904564
Local loss @ local epoch 2: 0.0032900855876505375
Local loss @ local epoch 3: 0.0023540551774203777
Local loss @ local epoch 4: 0.0018785351421684027
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.93 seconds!
[tester] 
SST2Metric: acc=0.8440366972477065, hinge=0.8121226496926142, ce=0.48606766972460125
Local test acc @ epoch 81: 0.844
Client 1 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0033692519646137953
Local loss @ local epoch 1: 0.0021845365408807993
Local loss @ local epoch 2: 0.0015046891057863832
Local loss @ local epoch 3: 0.0011403181124478579
Local loss @ local epoch 4: 0.0009684570832177997
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.84 seconds!
[tester] 
SST2Metric: acc=0.8520642201834863, hinge=0.7933676369966717, ce=0.4811068927108763
Local test acc @ epoch 81: 0.8521
Client 9 execute local training on 8 samples...
Local loss @ local epoch 0: 0.008903127163648605
Local loss @ local epoch 1: 0.007730427198112011
Local loss @ local epoch 2: 0.0070705655962228775
Local loss @ local epoch 3: 0.0064224726520478725
Local loss @ local epoch 4: 0.005780003033578396
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.83 seconds!
[tester] 
SST2Metric: acc=0.8577981651376146, hinge=0.7927903811865991, ce=0.47914993293944386
Local test acc @ epoch 81: 0.8578
Client 8 execute local training on 8 samples...
Local loss @ local epoch 0: 0.010314956307411194
Local loss @ local epoch 1: 0.006992099806666374
Local loss @ local epoch 2: 0.005137030966579914
Local loss @ local epoch 3: 0.00424466747790575
Local loss @ local epoch 4: 0.003953513223677874
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.87 seconds!
[tester] 
SST2Metric: acc=0.8428899082568807, hinge=0.9218097387924107, ce=0.5805475889262423
Local test acc @ epoch 81: 0.8429
Client 0 execute local training on 8 samples...
Local loss @ local epoch 0: 0.008112290874123573
Local loss @ local epoch 1: 0.005365015938878059
Local loss @ local epoch 2: 0.003756527090445161
Local loss @ local epoch 3: 0.002897903323173523
Local loss @ local epoch 4: 0.0025320579297840595
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.89 seconds!
[tester] 
SST2Metric: acc=0.8600917431192661, hinge=0.7966354695755408, ce=0.48091001448450066
Local test acc @ epoch 81: 0.8601
Client 2 execute local training on 8 samples...
Local loss @ local epoch 0: 0.022996023297309875
Local loss @ local epoch 1: 0.01610417664051056
Local loss @ local epoch 2: 0.01237476896494627
Local loss @ local epoch 3: 0.010678631253540516
Local loss @ local epoch 4: 0.01013278029859066
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.9 seconds!
[tester] 
SST2Metric: acc=0.841743119266055, hinge=0.9218739581490876, ce=0.579681342515193
Local test acc @ epoch 81: 0.8417
Global evaluate on test data...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.46 seconds!
[tester] 
SST2Metric: acc=0.8027522935779816, hinge=0.9604702541314134, ce=0.533016562880521
Global test acc @ epoch 81: 0.8028
Global epoch 82...
Client 1 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0012382448185235262
Local loss @ local epoch 1: 0.001092517632059753
Local loss @ local epoch 2: 0.0009968498488888144
Local loss @ local epoch 3: 0.0009044737671501935
Local loss @ local epoch 4: 0.0008154608658514917
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.95 seconds!
[tester] 
SST2Metric: acc=0.8520642201834863, hinge=0.8015911169008377, ce=0.4882724535925989
Local test acc @ epoch 82: 0.8521
Client 2 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0200944971293211
Local loss @ local epoch 1: 0.013731527142226696
Local loss @ local epoch 2: 0.010016730055212975
Local loss @ local epoch 3: 0.008097050711512566
Local loss @ local epoch 4: 0.007367835845798254
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.95 seconds!
[tester] 
SST2Metric: acc=0.8245412844036697, hinge=1.026967683653219, ce=0.655358966722857
Local test acc @ epoch 82: 0.8245
Client 4 execute local training on 8 samples...
Local loss @ local epoch 0: 0.16746363043785095
Local loss @ local epoch 1: 0.11750176548957825
Local loss @ local epoch 2: 0.08268695324659348
Local loss @ local epoch 3: 0.0602966770529747
Local loss @ local epoch 4: 0.04721806198358536
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.91 seconds!
[tester] 
SST2Metric: acc=0.8520642201834863, hinge=0.797068235250788, ce=0.48431732583579545
Local test acc @ epoch 82: 0.8521
Client 7 execute local training on 8 samples...
Local loss @ local epoch 0: 0.01131146214902401
Local loss @ local epoch 1: 0.010345484130084515
Local loss @ local epoch 2: 0.009883395396173
Local loss @ local epoch 3: 0.009319011121988297
Local loss @ local epoch 4: 0.008758720941841602
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.88 seconds!
[tester] 
SST2Metric: acc=0.8635321100917431, hinge=0.7793864948213647, ce=0.4679485627246055
Local test acc @ epoch 82: 0.8635
Client 9 execute local training on 8 samples...
Local loss @ local epoch 0: 0.007238232996314764
Local loss @ local epoch 1: 0.006179552059620619
Local loss @ local epoch 2: 0.005737415049225092
Local loss @ local epoch 3: 0.005320725962519646
Local loss @ local epoch 4: 0.004843642003834248
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.87 seconds!
[tester] 
SST2Metric: acc=0.856651376146789, hinge=0.8061492587995092, ce=0.4927361015398098
Local test acc @ epoch 82: 0.8567
Client 5 execute local training on 8 samples...
Local loss @ local epoch 0: 0.006549754645675421
Local loss @ local epoch 1: 0.004218470770865679
Local loss @ local epoch 2: 0.0028341514989733696
Local loss @ local epoch 3: 0.0020507783629000187
Local loss @ local epoch 4: 0.0016498190816491842
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.87 seconds!
[tester] 
SST2Metric: acc=0.8428899082568807, hinge=0.8377428609843648, ce=0.5110173051281823
Local test acc @ epoch 82: 0.8429
Client 6 execute local training on 8 samples...
Local loss @ local epoch 0: 0.08156045526266098
Local loss @ local epoch 1: 0.07534980773925781
Local loss @ local epoch 2: 0.07117527723312378
Local loss @ local epoch 3: 0.06631433963775635
Local loss @ local epoch 4: 0.061520859599113464
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.89 seconds!
[tester] 
SST2Metric: acc=0.8497706422018348, hinge=0.8077318668365479, ce=0.48656202746937566
Local test acc @ epoch 82: 0.8498
Client 3 execute local training on 8 samples...
Local loss @ local epoch 0: 0.06459026038646698
Local loss @ local epoch 1: 0.06205521523952484
Local loss @ local epoch 2: 0.06066257506608963
Local loss @ local epoch 3: 0.058217935264110565
Local loss @ local epoch 4: 0.0559932142496109
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.9 seconds!
[tester] 
SST2Metric: acc=0.8451834862385321, hinge=0.8131935265632945, ce=0.49025015810498146
Local test acc @ epoch 82: 0.8452
Client 8 execute local training on 8 samples...
Local loss @ local epoch 0: 0.05331262946128845
Local loss @ local epoch 1: 0.03483167290687561
Local loss @ local epoch 2: 0.022895757108926773
Local loss @ local epoch 3: 0.015365568920969963
Local loss @ local epoch 4: 0.010672459378838539
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.87 seconds!
[tester] 
SST2Metric: acc=0.8543577981651376, hinge=0.8063483145258842, ce=0.49348700892099034
Local test acc @ epoch 82: 0.8544
Client 0 execute local training on 8 samples...
Local loss @ local epoch 0: 0.002560306340456009
Local loss @ local epoch 1: 0.002441530814394355
Local loss @ local epoch 2: 0.00238633481785655
Local loss @ local epoch 3: 0.002270101336762309
Local loss @ local epoch 4: 0.002167223021388054
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.93 seconds!
[tester] 
SST2Metric: acc=0.856651376146789, hinge=0.8049119494923758, ce=0.4893981044628196
Local test acc @ epoch 82: 0.8567
Global evaluate on test data...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.53 seconds!
[tester] 
SST2Metric: acc=0.8038990825688074, hinge=0.9551607408654799, ce=0.5307644322330832
Global test acc @ epoch 82: 0.8039
Global epoch 83...
Client 2 execute local training on 8 samples...
Local loss @ local epoch 0: 0.023534921929240227
Local loss @ local epoch 1: 0.01599016599357128
Local loss @ local epoch 2: 0.011597013100981712
Local loss @ local epoch 3: 0.009301823563873768
Local loss @ local epoch 4: 0.008359087631106377
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.91 seconds!
[tester] 
SST2Metric: acc=0.841743119266055, hinge=0.9420170086786288, ce=0.5946959015214281
Local test acc @ epoch 83: 0.8417
Client 4 execute local training on 8 samples...
Local loss @ local epoch 0: 0.14362889528274536
Local loss @ local epoch 1: 0.10244300216436386
Local loss @ local epoch 2: 0.0758531466126442
Local loss @ local epoch 3: 0.060597024857997894
Local loss @ local epoch 4: 0.05303790047764778
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.97 seconds!
[tester] 
SST2Metric: acc=0.856651376146789, hinge=0.7861952772107693, ce=0.475035193723353
Local test acc @ epoch 83: 0.8567
Client 6 execute local training on 8 samples...
Local loss @ local epoch 0: 0.10917764902114868
Local loss @ local epoch 1: 0.08005698025226593
Local loss @ local epoch 2: 0.06261363625526428
Local loss @ local epoch 3: 0.054663531482219696
Local loss @ local epoch 4: 0.053215596824884415
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.92 seconds!
[tester] 
SST2Metric: acc=0.8520642201834863, hinge=0.8086876711987574, ce=0.48506888860319697
Local test acc @ epoch 83: 0.8521
Client 9 execute local training on 8 samples...
Local loss @ local epoch 0: 0.02509826049208641
Local loss @ local epoch 1: 0.01654045097529888
Local loss @ local epoch 2: 0.011542070657014847
Local loss @ local epoch 3: 0.008880789391696453
Local loss @ local epoch 4: 0.007690709084272385
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.95 seconds!
[tester] 
SST2Metric: acc=0.8532110091743119, hinge=0.7945897326283499, ce=0.48371822422011457
Local test acc @ epoch 83: 0.8532
Client 0 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0030536651611328125
Local loss @ local epoch 1: 0.0028060476761311293
Local loss @ local epoch 2: 0.0026373176369816065
Local loss @ local epoch 3: 0.002473064698278904
Local loss @ local epoch 4: 0.0023369297850877047
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.01 seconds!
[tester] 
SST2Metric: acc=0.8543577981651376, hinge=0.7999820901986656, ce=0.4861683063524126
Local test acc @ epoch 83: 0.8544
Client 3 execute local training on 8 samples...
Local loss @ local epoch 0: 0.11359412968158722
Local loss @ local epoch 1: 0.08646561950445175
Local loss @ local epoch 2: 0.06853314489126205
Local loss @ local epoch 3: 0.05928567051887512
Local loss @ local epoch 4: 0.057016339153051376
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.94 seconds!
[tester] 
SST2Metric: acc=0.8451834862385321, hinge=0.8318229325867574, ce=0.5030013791236326
Local test acc @ epoch 83: 0.8452
Client 7 execute local training on 8 samples...
Local loss @ local epoch 0: 0.016646629199385643
Local loss @ local epoch 1: 0.011765792965888977
Local loss @ local epoch 2: 0.009148961864411831
Local loss @ local epoch 3: 0.008113143034279346
Local loss @ local epoch 4: 0.008085478097200394
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.95 seconds!
[tester] 
SST2Metric: acc=0.8577981651376146, hinge=0.7957605258860719, ce=0.48177908599182295
Local test acc @ epoch 83: 0.8578
Client 1 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0011101285926997662
Local loss @ local epoch 1: 0.0009875064715743065
Local loss @ local epoch 2: 0.00089583097724244
Local loss @ local epoch 3: 0.0008084318833425641
Local loss @ local epoch 4: 0.0007280443096533418
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.9 seconds!
[tester] 
SST2Metric: acc=0.8532110091743119, hinge=0.8058893028475823, ce=0.4932146068943297
Local test acc @ epoch 83: 0.8532
Client 5 execute local training on 8 samples...
Local loss @ local epoch 0: 0.007855914533138275
Local loss @ local epoch 1: 0.0049651553854346275
Local loss @ local epoch 2: 0.0032635582610964775
Local loss @ local epoch 3: 0.0022959336638450623
Local loss @ local epoch 4: 0.0017816264880821109
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.93 seconds!
[tester] 
SST2Metric: acc=0.8486238532110092, hinge=0.8121775411957994, ce=0.4890938869564273
Local test acc @ epoch 83: 0.8486
Client 8 execute local training on 8 samples...
Local loss @ local epoch 0: 0.07681101560592651
Local loss @ local epoch 1: 0.0508742518723011
Local loss @ local epoch 2: 0.03364775702357292
Local loss @ local epoch 3: 0.022527417168021202
Local loss @ local epoch 4: 0.01545136608183384
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.91 seconds!
[tester] 
SST2Metric: acc=0.8509174311926605, hinge=0.8044819350636333, ce=0.4923165127241967
Local test acc @ epoch 83: 0.8509
Global evaluate on test data...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.45 seconds!
[tester] 
SST2Metric: acc=0.8073394495412844, hinge=0.9413748361648769, ce=0.5206458652028899
Global test acc @ epoch 83: 0.8073
Global epoch 84...
Client 9 execute local training on 8 samples...
Local loss @ local epoch 0: 0.00687382509931922
Local loss @ local epoch 1: 0.006041392683982849
Local loss @ local epoch 2: 0.005548038054257631
Local loss @ local epoch 3: 0.0050375512801110744
Local loss @ local epoch 4: 0.004543437622487545
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.97 seconds!
[tester] 
SST2Metric: acc=0.8520642201834863, hinge=0.8083692578547591, ce=0.4938196258538257
Local test acc @ epoch 84: 0.8521
Client 1 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0011845678091049194
Local loss @ local epoch 1: 0.0010551447048783302
Local loss @ local epoch 2: 0.0009453404927626252
Local loss @ local epoch 3: 0.0008452042238786817
Local loss @ local epoch 4: 0.000757422880269587
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.9 seconds!
[tester] 
SST2Metric: acc=0.8509174311926605, hinge=0.8058418634287808, ce=0.4943856919369096
Local test acc @ epoch 84: 0.8509
Client 2 execute local training on 8 samples...
Local loss @ local epoch 0: 0.017557110637426376
Local loss @ local epoch 1: 0.012049945071339607
Local loss @ local epoch 2: 0.008875285275280476
Local loss @ local epoch 3: 0.0072774579748511314
Local loss @ local epoch 4: 0.0067137982696294785
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.91 seconds!
[tester] 
SST2Metric: acc=0.8268348623853211, hinge=1.019261290037304, ce=0.6533251485237029
Local test acc @ epoch 84: 0.8268
Client 7 execute local training on 8 samples...
Local loss @ local epoch 0: 0.045922569930553436
Local loss @ local epoch 1: 0.030153386294841766
Local loss @ local epoch 2: 0.020304687321186066
Local loss @ local epoch 3: 0.014370134100317955
Local loss @ local epoch 4: 0.010969930328428745
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.03 seconds!
[tester] 
SST2Metric: acc=0.8532110091743119, hinge=0.8002469212910451, ce=0.48961425294732214
Local test acc @ epoch 84: 0.8532
Client 0 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0025493111461400986
Local loss @ local epoch 1: 0.0023758120369166136
Local loss @ local epoch 2: 0.002232115250080824
Local loss @ local epoch 3: 0.0021096020936965942
Local loss @ local epoch 4: 0.0020109801553189754
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.96 seconds!
[tester] 
SST2Metric: acc=0.856651376146789, hinge=0.805934929929742, ce=0.49073539830748536
Local test acc @ epoch 84: 0.8567
Client 8 execute local training on 8 samples...
Local loss @ local epoch 0: 0.01102534681558609
Local loss @ local epoch 1: 0.007342672906816006
Local loss @ local epoch 2: 0.005210433155298233
Local loss @ local epoch 3: 0.004090517293661833
Local loss @ local epoch 4: 0.003615220542997122
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.99 seconds!
[tester] 
SST2Metric: acc=0.841743119266055, hinge=0.9312281568936251, ce=0.590185703548449
Local test acc @ epoch 84: 0.8417
Client 3 execute local training on 8 samples...
Local loss @ local epoch 0: 0.31140148639678955
Local loss @ local epoch 1: 0.25562700629234314
Local loss @ local epoch 2: 0.20506587624549866
Local loss @ local epoch 3: 0.1606929898262024
Local loss @ local epoch 4: 0.12363673001527786
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.96 seconds!
[tester] 
SST2Metric: acc=0.8589449541284404, hinge=0.8056298614641942, ce=0.48862387597189205
Local test acc @ epoch 84: 0.8589
Client 5 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0035479420330375433
Local loss @ local epoch 1: 0.0023865702096372843
Local loss @ local epoch 2: 0.001756802317686379
Local loss @ local epoch 3: 0.0014802808873355389
Local loss @ local epoch 4: 0.001430578762665391
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.9 seconds!
[tester] 
SST2Metric: acc=0.8394495412844036, hinge=0.8961480819030639, ce=0.5536860003480004
Local test acc @ epoch 84: 0.8394
Client 6 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0726771131157875
Local loss @ local epoch 1: 0.06740989536046982
Local loss @ local epoch 2: 0.06362380087375641
Local loss @ local epoch 3: 0.05928637832403183
Local loss @ local epoch 4: 0.05512573942542076
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.94 seconds!
[tester] 
SST2Metric: acc=0.8405963302752294, hinge=0.8243132345720169, ce=0.49967544902769695
Local test acc @ epoch 84: 0.8406
Client 4 execute local training on 8 samples...
Local loss @ local epoch 0: 0.16044588387012482
Local loss @ local epoch 1: 0.1156722828745842
Local loss @ local epoch 2: 0.08282477408647537
Local loss @ local epoch 3: 0.06121089309453964
Local loss @ local epoch 4: 0.04878690838813782
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.92 seconds!
[tester] 
SST2Metric: acc=0.8532110091743119, hinge=0.8034066600263665, ce=0.4924031788613562
Local test acc @ epoch 84: 0.8532
Global evaluate on test data...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.2 seconds!
[tester] 
SST2Metric: acc=0.801605504587156, hinge=0.9885816813335506, ce=0.5596204072219926
Global test acc @ epoch 84: 0.8016
Global epoch 85...
Client 4 execute local training on 8 samples...
Local loss @ local epoch 0: 0.07270856201648712
Local loss @ local epoch 1: 0.06157175451517105
Local loss @ local epoch 2: 0.05385091155767441
Local loss @ local epoch 3: 0.0473150834441185
Local loss @ local epoch 4: 0.04152383655309677
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.97 seconds!
[tester] 
SST2Metric: acc=0.8520642201834863, hinge=0.8048415722650125, ce=0.49272718928848314
Local test acc @ epoch 85: 0.8521
Client 0 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0029690240044146776
Local loss @ local epoch 1: 0.002656463999301195
Local loss @ local epoch 2: 0.002418444026261568
Local loss @ local epoch 3: 0.002231098245829344
Local loss @ local epoch 4: 0.0020873360335826874
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.99 seconds!
[tester] 
SST2Metric: acc=0.8543577981651376, hinge=0.8049054596949061, ce=0.49136269446451714
Local test acc @ epoch 85: 0.8544
Client 6 execute local training on 8 samples...
Local loss @ local epoch 0: 0.15171027183532715
Local loss @ local epoch 1: 0.10993838310241699
Local loss @ local epoch 2: 0.08141898363828659
Local loss @ local epoch 3: 0.06446727365255356
Local loss @ local epoch 4: 0.05668523535132408
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.92 seconds!
[tester] 
SST2Metric: acc=0.8428899082568807, hinge=0.8282990759118981, ce=0.5021685508801815
Local test acc @ epoch 85: 0.8429
Client 1 execute local training on 8 samples...
Local loss @ local epoch 0: 0.002582011977210641
Local loss @ local epoch 1: 0.0016574328765273094
Local loss @ local epoch 2: 0.001119084656238556
Local loss @ local epoch 3: 0.0008207161445170641
Local loss @ local epoch 4: 0.0006689508445560932
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.94 seconds!
[tester] 
SST2Metric: acc=0.8532110091743119, hinge=0.8038411901904903, ce=0.4922314801321732
Local test acc @ epoch 85: 0.8532
Client 8 execute local training on 8 samples...
Local loss @ local epoch 0: 0.01663394831120968
Local loss @ local epoch 1: 0.010963845998048782
Local loss @ local epoch 2: 0.007567127235233784
Local loss @ local epoch 3: 0.005653521046042442
Local loss @ local epoch 4: 0.004684546962380409
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.93 seconds!
[tester] 
SST2Metric: acc=0.8405963302752294, hinge=0.9813794739749453, ce=0.6263579054940662
Local test acc @ epoch 85: 0.8406
Client 9 execute local training on 8 samples...
Local loss @ local epoch 0: 0.019519787281751633
Local loss @ local epoch 1: 0.01240638829767704
Local loss @ local epoch 2: 0.008294516243040562
Local loss @ local epoch 3: 0.006066970061510801
Local loss @ local epoch 4: 0.004993607755750418
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.86 seconds!
[tester] 
SST2Metric: acc=0.8543577981651376, hinge=0.8087199711744938, ce=0.49462698023806867
Local test acc @ epoch 85: 0.8544
Client 5 execute local training on 8 samples...
Local loss @ local epoch 0: 0.005759829189628363
Local loss @ local epoch 1: 0.00370797305367887
Local loss @ local epoch 2: 0.0025025480426847935
Local loss @ local epoch 3: 0.001833426533266902
Local loss @ local epoch 4: 0.0015057496493682265
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.95 seconds!
[tester] 
SST2Metric: acc=0.8440366972477065, hinge=0.8535459189239992, ce=0.5233862067908134
Local test acc @ epoch 85: 0.844
Client 3 execute local training on 8 samples...
Local loss @ local epoch 0: 0.05944782868027687
Local loss @ local epoch 1: 0.05748820677399635
Local loss @ local epoch 2: 0.055051278322935104
Local loss @ local epoch 3: 0.053068891167640686
Local loss @ local epoch 4: 0.05118488147854805
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.93 seconds!
[tester] 
SST2Metric: acc=0.8440366972477065, hinge=0.8555446133154248, ce=0.5242111007618897
Local test acc @ epoch 85: 0.844
Client 2 execute local training on 8 samples...
Local loss @ local epoch 0: 0.08477161079645157
Local loss @ local epoch 1: 0.055805690586566925
Local loss @ local epoch 2: 0.036980822682380676
Local loss @ local epoch 3: 0.025060787796974182
Local loss @ local epoch 4: 0.017626453191041946
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.94 seconds!
[tester] 
SST2Metric: acc=0.8520642201834863, hinge=0.8145298821116806, ce=0.49871626050358875
Local test acc @ epoch 85: 0.8521
Client 7 execute local training on 8 samples...
Local loss @ local epoch 0: 0.010063632391393185
Local loss @ local epoch 1: 0.008032096549868584
Local loss @ local epoch 2: 0.007465798407793045
Local loss @ local epoch 3: 0.007642519194632769
Local loss @ local epoch 4: 0.00781166460365057
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.92 seconds!
[tester] 
SST2Metric: acc=0.8474770642201835, hinge=0.8190071338087047, ce=0.4973493283945606
Local test acc @ epoch 85: 0.8475
Global evaluate on test data...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.31 seconds!
[tester] 
SST2Metric: acc=0.8038990825688074, hinge=0.9536021092069258, ce=0.5319169886974157
Global test acc @ epoch 85: 0.8039
Global epoch 86...
Client 7 execute local training on 8 samples...
Local loss @ local epoch 0: 0.00808718241751194
Local loss @ local epoch 1: 0.007627980783581734
Local loss @ local epoch 2: 0.007224227301776409
Local loss @ local epoch 3: 0.006865172181278467
Local loss @ local epoch 4: 0.006540754809975624
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.98 seconds!
[tester] 
SST2Metric: acc=0.8555045871559633, hinge=0.8117236104033408, ce=0.49313504573965977
Local test acc @ epoch 86: 0.8555
Client 1 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0014457915676757693
Local loss @ local epoch 1: 0.001119863474741578
Local loss @ local epoch 2: 0.0009848944609984756
Local loss @ local epoch 3: 0.0009318687370978296
Local loss @ local epoch 4: 0.0008909428725019097
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.98 seconds!
[tester] 
SST2Metric: acc=0.8497706422018348, hinge=0.8211824379929709, ce=0.5082053448994208
Local test acc @ epoch 86: 0.8498
Client 2 execute local training on 8 samples...
Local loss @ local epoch 0: 0.011777348816394806
Local loss @ local epoch 1: 0.008722002618014812
Local loss @ local epoch 2: 0.007295258343219757
Local loss @ local epoch 3: 0.006882518529891968
Local loss @ local epoch 4: 0.006912482436746359
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.89 seconds!
[tester] 
SST2Metric: acc=0.8256880733944955, hinge=1.020541901845451, ce=0.6563355394340293
Local test acc @ epoch 86: 0.8257
Client 9 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0218197014182806
Local loss @ local epoch 1: 0.013742332346737385
Local loss @ local epoch 2: 0.008989769034087658
Local loss @ local epoch 3: 0.006317798979580402
Local loss @ local epoch 4: 0.004916795063763857
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.87 seconds!
[tester] 
SST2Metric: acc=0.8543577981651376, hinge=0.8169415575375251, ce=0.50375004326914
Local test acc @ epoch 86: 0.8544
Client 4 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0745249092578888
Local loss @ local epoch 1: 0.06359808146953583
Local loss @ local epoch 2: 0.05444983392953873
Local loss @ local epoch 3: 0.04684298112988472
Local loss @ local epoch 4: 0.04056541994214058
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.01 seconds!
[tester] 
SST2Metric: acc=0.856651376146789, hinge=0.7983804976721423, ce=0.48863037777242224
Local test acc @ epoch 86: 0.8567
Client 8 execute local training on 8 samples...
Local loss @ local epoch 0: 0.018783140927553177
Local loss @ local epoch 1: 0.012290190905332565
Local loss @ local epoch 2: 0.00833749957382679
Local loss @ local epoch 3: 0.006037213373929262
Local loss @ local epoch 4: 0.00478756008669734
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.94 seconds!
[tester] 
SST2Metric: acc=0.8371559633027523, hinge=0.9685103331410557, ce=0.6180787628365428
Local test acc @ epoch 86: 0.8372
Client 5 execute local training on 8 samples...
Local loss @ local epoch 0: 0.034714944660663605
Local loss @ local epoch 1: 0.021841302514076233
Local loss @ local epoch 2: 0.013932425528764725
Local loss @ local epoch 3: 0.009109161794185638
Local loss @ local epoch 4: 0.0061477371491491795
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.89 seconds!
[tester] 
SST2Metric: acc=0.8589449541284404, hinge=0.7951643852465743, ce=0.4852251026092136
Local test acc @ epoch 86: 0.8589
Client 3 execute local training on 8 samples...
Local loss @ local epoch 0: 0.08832940459251404
Local loss @ local epoch 1: 0.06921263784170151
Local loss @ local epoch 2: 0.05910990387201309
Local loss @ local epoch 3: 0.056436795741319656
Local loss @ local epoch 4: 0.057959944009780884
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.98 seconds!
[tester] 
SST2Metric: acc=0.8463302752293578, hinge=0.824539873304717, ce=0.5016369487323431
Local test acc @ epoch 86: 0.8463
Client 0 execute local training on 8 samples...
Local loss @ local epoch 0: 0.008016813546419144
Local loss @ local epoch 1: 0.005211763549596071
Local loss @ local epoch 2: 0.003543544327840209
Local loss @ local epoch 3: 0.0026123032439500093
Local loss @ local epoch 4: 0.0021520424634218216
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.95 seconds!
[tester] 
SST2Metric: acc=0.8555045871559633, hinge=0.8028692932577308, ce=0.4917090444349248
Local test acc @ epoch 86: 0.8555
Client 6 execute local training on 8 samples...
Local loss @ local epoch 0: 0.1386493593454361
Local loss @ local epoch 1: 0.10061310231685638
Local loss @ local epoch 2: 0.07543588429689407
Local loss @ local epoch 3: 0.06128145754337311
Local loss @ local epoch 4: 0.055622607469558716
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.9 seconds!
[tester] 
SST2Metric: acc=0.8509174311926605, hinge=0.8192330631641073, ce=0.4976132316557571
Local test acc @ epoch 86: 0.8509
Global evaluate on test data...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.11 seconds!
[tester] 
SST2Metric: acc=0.8061926605504587, hinge=0.9857906628092494, ce=0.5590991633538361
Global test acc @ epoch 86: 0.8062
Global epoch 87...
Client 4 execute local training on 8 samples...
Local loss @ local epoch 0: 0.05144910514354706
Local loss @ local epoch 1: 0.04417654499411583
Local loss @ local epoch 2: 0.03822944313287735
Local loss @ local epoch 3: 0.03331265226006508
Local loss @ local epoch 4: 0.029287882149219513
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.93 seconds!
[tester] 
SST2Metric: acc=0.8612385321100917, hinge=0.7931091758089328, ce=0.4837073100180207
Local test acc @ epoch 87: 0.8612
Client 7 execute local training on 8 samples...
Local loss @ local epoch 0: 0.00928414799273014
Local loss @ local epoch 1: 0.008477350696921349
Local loss @ local epoch 2: 0.007829483598470688
Local loss @ local epoch 3: 0.007255859207361937
Local loss @ local epoch 4: 0.00679013691842556
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.93 seconds!
[tester] 
SST2Metric: acc=0.8600917431192661, hinge=0.7852372789601667, ce=0.4772878207587808
Local test acc @ epoch 87: 0.8601
Client 9 execute local training on 8 samples...
Local loss @ local epoch 0: 0.006465644109994173
Local loss @ local epoch 1: 0.005469560157507658
Local loss @ local epoch 2: 0.005045717116445303
Local loss @ local epoch 3: 0.004669452551752329
Local loss @ local epoch 4: 0.004242005757987499
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.97 seconds!
[tester] 
SST2Metric: acc=0.8555045871559633, hinge=0.8128605497266175, ce=0.5030588057707064
Local test acc @ epoch 87: 0.8555
Client 2 execute local training on 8 samples...
Local loss @ local epoch 0: 0.012107128277420998
Local loss @ local epoch 1: 0.009227924980223179
Local loss @ local epoch 2: 0.007979370653629303
Local loss @ local epoch 3: 0.007610990200191736
Local loss @ local epoch 4: 0.007488206960260868
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.97 seconds!
[tester] 
SST2Metric: acc=0.838302752293578, hinge=0.967694794502827, ce=0.620144011784017
Local test acc @ epoch 87: 0.8383
Client 0 execute local training on 8 samples...
Local loss @ local epoch 0: 0.006347769871354103
Local loss @ local epoch 1: 0.004199965391308069
Local loss @ local epoch 2: 0.0029473532922565937
Local loss @ local epoch 3: 0.0022798837162554264
Local loss @ local epoch 4: 0.001994132064282894
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.98 seconds!
[tester] 
SST2Metric: acc=0.8589449541284404, hinge=0.8024939125010727, ce=0.4923014574835863
Local test acc @ epoch 87: 0.8589
Client 6 execute local training on 8 samples...
Local loss @ local epoch 0: 0.142818421125412
Local loss @ local epoch 1: 0.10447925329208374
Local loss @ local epoch 2: 0.07909983396530151
Local loss @ local epoch 3: 0.06496435403823853
Local loss @ local epoch 4: 0.059499531984329224
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.89 seconds!
[tester] 
SST2Metric: acc=0.8486238532110092, hinge=0.8285693329408628, ce=0.5067585458639376
Local test acc @ epoch 87: 0.8486
Client 8 execute local training on 8 samples...
Local loss @ local epoch 0: 0.06664347648620605
Local loss @ local epoch 1: 0.04387694597244263
Local loss @ local epoch 2: 0.028892848640680313
Local loss @ local epoch 3: 0.019276481121778488
Local loss @ local epoch 4: 0.01317034661769867
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.95 seconds!
[tester] 
SST2Metric: acc=0.856651376146789, hinge=0.8059355430646774, ce=0.49579168923536676
Local test acc @ epoch 87: 0.8567
Client 5 execute local training on 8 samples...
Local loss @ local epoch 0: 0.005196892656385899
Local loss @ local epoch 1: 0.0033533598762005568
Local loss @ local epoch 2: 0.002273834776133299
Local loss @ local epoch 3: 0.0016796784475445747
Local loss @ local epoch 4: 0.0013953335583209991
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.08 seconds!
[tester] 
SST2Metric: acc=0.8451834862385321, hinge=0.8408446273672472, ce=0.5171689596902921
Local test acc @ epoch 87: 0.8452
Client 3 execute local training on 8 samples...
Local loss @ local epoch 0: 0.05711054801940918
Local loss @ local epoch 1: 0.05518820509314537
Local loss @ local epoch 2: 0.05283215641975403
Local loss @ local epoch 3: 0.050906017422676086
Local loss @ local epoch 4: 0.04907577112317085
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.98 seconds!
[tester] 
SST2Metric: acc=0.8451834862385321, hinge=0.8484968849278371, ce=0.5221536399106658
Local test acc @ epoch 87: 0.8452
Client 1 execute local training on 8 samples...
Local loss @ local epoch 0: 0.003077284200116992
Local loss @ local epoch 1: 0.0019861883483827114
Local loss @ local epoch 2: 0.0013563280226662755
Local loss @ local epoch 3: 0.001013662084005773
Local loss @ local epoch 4: 0.0008460978278890252
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.06 seconds!
[tester] 
SST2Metric: acc=0.8543577981651376, hinge=0.8027039738970065, ce=0.4931928640655498
Local test acc @ epoch 87: 0.8544
Global evaluate on test data...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.41 seconds!
[tester] 
SST2Metric: acc=0.805045871559633, hinge=0.9430719989975658, ce=0.5276507944030978
Global test acc @ epoch 87: 0.805
Global epoch 88...
Client 6 execute local training on 8 samples...
Local loss @ local epoch 0: 0.10368406772613525
Local loss @ local epoch 1: 0.0777084156870842
Local loss @ local epoch 2: 0.06320782005786896
Local loss @ local epoch 3: 0.05769094079732895
Local loss @ local epoch 4: 0.057507630437612534
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.9 seconds!
[tester] 
SST2Metric: acc=0.8520642201834863, hinge=0.8284185111796091, ce=0.5057048183927791
Local test acc @ epoch 88: 0.8521
Client 2 execute local training on 8 samples...
Local loss @ local epoch 0: 0.10613928735256195
Local loss @ local epoch 1: 0.07052595913410187
Local loss @ local epoch 2: 0.046997249126434326
Local loss @ local epoch 3: 0.0317855179309845
Local loss @ local epoch 4: 0.022036481648683548
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.94 seconds!
[tester] 
SST2Metric: acc=0.8543577981651376, hinge=0.8014990297752783, ce=0.49260341157707377
Local test acc @ epoch 88: 0.8544
Client 1 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0006370887858793139
Local loss @ local epoch 1: 0.0005737110623158514
Local loss @ local epoch 2: 0.0005172404926270247
Local loss @ local epoch 3: 0.0004666431341320276
Local loss @ local epoch 4: 0.00042200434836559
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.98 seconds!
[tester] 
SST2Metric: acc=0.8486238532110092, hinge=0.8194797060905247, ce=0.5072194091300694
Local test acc @ epoch 88: 0.8486
Client 3 execute local training on 8 samples...
Local loss @ local epoch 0: 0.07844273746013641
Local loss @ local epoch 1: 0.06647390872240067
Local loss @ local epoch 2: 0.06312789767980576
Local loss @ local epoch 3: 0.06369545310735703
Local loss @ local epoch 4: 0.06363959610462189
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.91 seconds!
[tester] 
SST2Metric: acc=0.8577981651376146, hinge=0.7843540078456249, ce=0.4753168883510233
Local test acc @ epoch 88: 0.8578
Client 9 execute local training on 8 samples...
Local loss @ local epoch 0: 0.013382254168391228
Local loss @ local epoch 1: 0.009707821533083916
Local loss @ local epoch 2: 0.007989758625626564
Local loss @ local epoch 3: 0.00736618135124445
Local loss @ local epoch 4: 0.007130425423383713
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.9 seconds!
[tester] 
SST2Metric: acc=0.8497706422018348, hinge=0.8270322964825761, ce=0.5138041100749391
Local test acc @ epoch 88: 0.8498
Client 4 execute local training on 8 samples...
Local loss @ local epoch 0: 0.03782644122838974
Local loss @ local epoch 1: 0.031884290277957916
Local loss @ local epoch 2: 0.02872598171234131
Local loss @ local epoch 3: 0.026129532605409622
Local loss @ local epoch 4: 0.02360595017671585
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.97 seconds!
[tester] 
SST2Metric: acc=0.8623853211009175, hinge=0.7938643669316529, ce=0.4858659833550577
Local test acc @ epoch 88: 0.8624
Client 7 execute local training on 8 samples...
Local loss @ local epoch 0: 0.010034066624939442
Local loss @ local epoch 1: 0.00878360029309988
Local loss @ local epoch 2: 0.0082301776856184
Local loss @ local epoch 3: 0.007703106384724379
Local loss @ local epoch 4: 0.00714007718488574
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.03 seconds!
[tester] 
SST2Metric: acc=0.8577981651376146, hinge=0.7961720202220689, ce=0.4892996058755537
Local test acc @ epoch 88: 0.8578
Client 8 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0180375874042511
Local loss @ local epoch 1: 0.011775046586990356
Local loss @ local epoch 2: 0.007957197725772858
Local loss @ local epoch 3: 0.005723850801587105
Local loss @ local epoch 4: 0.004495173692703247
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.93 seconds!
[tester] 
SST2Metric: acc=0.8405963302752294, hinge=0.9568435849126325, ce=0.6135644262247781
Local test acc @ epoch 88: 0.8406
Client 0 execute local training on 8 samples...
Local loss @ local epoch 0: 0.005488625727593899
Local loss @ local epoch 1: 0.0036860653199255466
Local loss @ local epoch 2: 0.0026964349672198296
Local loss @ local epoch 3: 0.0022328479681164026
Local loss @ local epoch 4: 0.0020976285450160503
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.93 seconds!
[tester] 
SST2Metric: acc=0.8577981651376146, hinge=0.7988619239778694, ce=0.49098251654270936
Local test acc @ epoch 88: 0.8578
Client 5 execute local training on 8 samples...
Local loss @ local epoch 0: 0.004190735053271055
Local loss @ local epoch 1: 0.0027256303001195192
Local loss @ local epoch 2: 0.001886059995740652
Local loss @ local epoch 3: 0.0014474686468020082
Local loss @ local epoch 4: 0.0012665969552472234
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.02 seconds!
[tester] 
SST2Metric: acc=0.8497706422018348, hinge=0.8452705148983439, ce=0.5208637925247243
Local test acc @ epoch 88: 0.8498
Global evaluate on test data...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.35 seconds!
[tester] 
SST2Metric: acc=0.8084862385321101, hinge=0.9339428556348206, ce=0.5214147844411637
Global test acc @ epoch 88: 0.8085
Global epoch 89...
Client 4 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0327383428812027
Local loss @ local epoch 1: 0.02843586355447769
Local loss @ local epoch 2: 0.025376496836543083
Local loss @ local epoch 3: 0.022669518366456032
Local loss @ local epoch 4: 0.020329082384705544
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.94 seconds!
[tester] 
SST2Metric: acc=0.8589449541284404, hinge=0.7968273192917535, ce=0.4888382535993865
Local test acc @ epoch 89: 0.8589
Client 3 execute local training on 8 samples...
Local loss @ local epoch 0: 0.06515032052993774
Local loss @ local epoch 1: 0.06206865981221199
Local loss @ local epoch 2: 0.05913972854614258
Local loss @ local epoch 3: 0.05639345943927765
Local loss @ local epoch 4: 0.054044242948293686
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.98 seconds!
[tester] 
SST2Metric: acc=0.8600917431192661, hinge=0.7803474028996371, ce=0.47469687909678204
Local test acc @ epoch 89: 0.8601
Client 2 execute local training on 8 samples...
Local loss @ local epoch 0: 0.04061523824930191
Local loss @ local epoch 1: 0.02665875293314457
Local loss @ local epoch 2: 0.01780865155160427
Local loss @ local epoch 3: 0.012313601560890675
Local loss @ local epoch 4: 0.008972859010100365
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.99 seconds!
[tester] 
SST2Metric: acc=0.8451834862385321, hinge=0.8939471355545412, ce=0.5652199570059406
Local test acc @ epoch 89: 0.8452
Client 7 execute local training on 8 samples...
Local loss @ local epoch 0: 0.014961292035877705
Local loss @ local epoch 1: 0.010345852002501488
Local loss @ local epoch 2: 0.007855354808270931
Local loss @ local epoch 3: 0.006770277861505747
Local loss @ local epoch 4: 0.006545711774379015
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.0 seconds!
[tester] 
SST2Metric: acc=0.8623853211009175, hinge=0.782359293431317, ce=0.47631558957199965
Local test acc @ epoch 89: 0.8624
Client 6 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0691385567188263
Local loss @ local epoch 1: 0.056269049644470215
Local loss @ local epoch 2: 0.05183657258749008
Local loss @ local epoch 3: 0.05132098123431206
Local loss @ local epoch 4: 0.050552889704704285
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.95 seconds!
[tester] 
SST2Metric: acc=0.8543577981651376, hinge=0.7966996335655178, ce=0.48359926474769815
Local test acc @ epoch 89: 0.8544
Client 5 execute local training on 8 samples...
Local loss @ local epoch 0: 0.001669747638516128
Local loss @ local epoch 1: 0.0015136285219341516
Local loss @ local epoch 2: 0.0014712976990267634
Local loss @ local epoch 3: 0.0013917206088081002
Local loss @ local epoch 4: 0.0012955520069226623
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.88 seconds!
[tester] 
SST2Metric: acc=0.8577981651376146, hinge=0.8155899609721035, ce=0.4980034331017849
Local test acc @ epoch 89: 0.8578
Client 1 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0021181944757699966
Local loss @ local epoch 1: 0.0013543205568566918
Local loss @ local epoch 2: 0.0009053567773662508
Local loss @ local epoch 3: 0.0006519955932162702
Local loss @ local epoch 4: 0.0005183776956982911
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.91 seconds!
[tester] 
SST2Metric: acc=0.8555045871559633, hinge=0.8022292935793552, ce=0.49514233847223477
Local test acc @ epoch 89: 0.8555
Client 9 execute local training on 8 samples...
Local loss @ local epoch 0: 0.008076607249677181
Local loss @ local epoch 1: 0.007075859233736992
Local loss @ local epoch 2: 0.006281293462961912
Local loss @ local epoch 3: 0.0055562774650752544
Local loss @ local epoch 4: 0.004927291534841061
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.92 seconds!
[tester] 
SST2Metric: acc=0.856651376146789, hinge=0.8034756816581848, ce=0.49722910174510415
Local test acc @ epoch 89: 0.8567
Client 0 execute local training on 8 samples...
Local loss @ local epoch 0: 0.002028209390118718
Local loss @ local epoch 1: 0.0019142007222399116
Local loss @ local epoch 2: 0.0018532087560743093
Local loss @ local epoch 3: 0.0017584215383976698
Local loss @ local epoch 4: 0.0016723168082535267
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.95 seconds!
[tester] 
SST2Metric: acc=0.8577981651376146, hinge=0.8128939161606885, ce=0.5025124956352781
Local test acc @ epoch 89: 0.8578
Client 8 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0077370041981339455
Local loss @ local epoch 1: 0.00523038487881422
Local loss @ local epoch 2: 0.0038281534798443317
Local loss @ local epoch 3: 0.003147581359371543
Local loss @ local epoch 4: 0.002917952835559845
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.03 seconds!
[tester] 
SST2Metric: acc=0.8440366972477065, hinge=0.9388533286545255, ce=0.6025880977931676
Local test acc @ epoch 89: 0.844
Global evaluate on test data...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.2 seconds!
[tester] 
SST2Metric: acc=0.8119266055045872, hinge=0.9388316172525424, ce=0.5253683369555467
Global test acc @ epoch 89: 0.8119
Global epoch 90...
Client 6 execute local training on 8 samples...
Local loss @ local epoch 0: 0.08824193477630615
Local loss @ local epoch 1: 0.06552841514348984
Local loss @ local epoch 2: 0.05308172479271889
Local loss @ local epoch 3: 0.04849629104137421
Local loss @ local epoch 4: 0.04851360246539116
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.02 seconds!
[tester] 
SST2Metric: acc=0.8532110091743119, hinge=0.8064635256561664, ce=0.4914559453208888
Local test acc @ epoch 90: 0.8532
Client 9 execute local training on 8 samples...
Local loss @ local epoch 0: 0.02219591662287712
Local loss @ local epoch 1: 0.014659523963928223
Local loss @ local epoch 2: 0.010300835594534874
Local loss @ local epoch 3: 0.008014430291950703
Local loss @ local epoch 4: 0.007014005910605192
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.09 seconds!
[tester] 
SST2Metric: acc=0.8543577981651376, hinge=0.8071727909899633, ce=0.4991039726594805
Local test acc @ epoch 90: 0.8544
Client 0 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0025795213878154755
Local loss @ local epoch 1: 0.00230799475684762
Local loss @ local epoch 2: 0.0021476014517247677
Local loss @ local epoch 3: 0.001996434759348631
Local loss @ local epoch 4: 0.0018620748305693269
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.99 seconds!
[tester] 
SST2Metric: acc=0.8532110091743119, hinge=0.8140935223857197, ce=0.5056533607616358
Local test acc @ epoch 90: 0.8532
Client 7 execute local training on 8 samples...
Local loss @ local epoch 0: 0.007235713303089142
Local loss @ local epoch 1: 0.005931866820901632
Local loss @ local epoch 2: 0.005649817641824484
Local loss @ local epoch 3: 0.0057663992047309875
Local loss @ local epoch 4: 0.005765186157077551
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.95 seconds!
[tester] 
SST2Metric: acc=0.8555045871559633, hinge=0.8048879521975824, ce=0.49245077772802937
Local test acc @ epoch 90: 0.8555
Client 4 execute local training on 8 samples...
Local loss @ local epoch 0: 0.06922157853841782
Local loss @ local epoch 1: 0.05125965178012848
Local loss @ local epoch 2: 0.04144750162959099
Local loss @ local epoch 3: 0.03669310361146927
Local loss @ local epoch 4: 0.034160394221544266
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.07 seconds!
[tester] 
SST2Metric: acc=0.8532110091743119, hinge=0.810059917629312, ce=0.5027050257448444
Local test acc @ epoch 90: 0.8532
Client 3 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0872507318854332
Local loss @ local epoch 1: 0.06675516813993454
Local loss @ local epoch 2: 0.055107615888118744
Local loss @ local epoch 3: 0.05091525986790657
Local loss @ local epoch 4: 0.05158252269029617
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.98 seconds!
[tester] 
SST2Metric: acc=0.8532110091743119, hinge=0.8037999486977901, ce=0.4900698755737479
Local test acc @ epoch 90: 0.8532
Client 5 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0013589751906692982
Local loss @ local epoch 1: 0.0012420392595231533
Local loss @ local epoch 2: 0.0012143868952989578
Local loss @ local epoch 3: 0.0011528281029313803
Local loss @ local epoch 4: 0.0010782842291519046
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.93 seconds!
[tester] 
SST2Metric: acc=0.8532110091743119, hinge=0.8231037620010726, ce=0.5049732088943337
Local test acc @ epoch 90: 0.8532
Client 1 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0020706800278276205
Local loss @ local epoch 1: 0.0013349365908652544
Local loss @ local epoch 2: 0.0009075356647372246
Local loss @ local epoch 3: 0.0006725259590893984
Local loss @ local epoch 4: 0.0005555313546210527
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.9 seconds!
[tester] 
SST2Metric: acc=0.8543577981651376, hinge=0.807092985975633, ce=0.5009662502753941
Local test acc @ epoch 90: 0.8544
Client 8 execute local training on 8 samples...
Local loss @ local epoch 0: 0.013677369803190231
Local loss @ local epoch 1: 0.009048020467162132
Local loss @ local epoch 2: 0.006312740501016378
Local loss @ local epoch 3: 0.0048071639612317085
Local loss @ local epoch 4: 0.004079651087522507
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.89 seconds!
[tester] 
SST2Metric: acc=0.8440366972477065, hinge=0.9768031705683524, ce=0.6302352234349509
Local test acc @ epoch 90: 0.844
Client 2 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0060273827984929085
Local loss @ local epoch 1: 0.005609563551843166
Local loss @ local epoch 2: 0.005255049094557762
Local loss @ local epoch 3: 0.00492629362270236
Local loss @ local epoch 4: 0.004655106458812952
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.99 seconds!
[tester] 
SST2Metric: acc=0.8211009174311926, hinge=1.1080064532953664, ce=0.7277931558496801
Local test acc @ epoch 90: 0.8211
Global evaluate on test data...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.44 seconds!
[tester] 
SST2Metric: acc=0.8073394495412844, hinge=0.9507058257356696, ce=0.5354607452151425
Global test acc @ epoch 90: 0.8073
Global epoch 91...
Client 9 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0060051605105400085
Local loss @ local epoch 1: 0.0052681718952953815
Local loss @ local epoch 2: 0.004637729376554489
Local loss @ local epoch 3: 0.004099689889699221
Local loss @ local epoch 4: 0.0036422493867576122
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.96 seconds!
[tester] 
SST2Metric: acc=0.8589449541284404, hinge=0.815655930736743, ce=0.5059273689178913
Local test acc @ epoch 91: 0.8589
Client 1 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0008210479863919318
Local loss @ local epoch 1: 0.0007346936617977917
Local loss @ local epoch 2: 0.0006596103776246309
Local loss @ local epoch 3: 0.0005910718464292586
Local loss @ local epoch 4: 0.0005307457759045064
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.02 seconds!
[tester] 
SST2Metric: acc=0.8555045871559633, hinge=0.8129518130503663, ce=0.5064028652962682
Local test acc @ epoch 91: 0.8555
Client 4 execute local training on 8 samples...
Local loss @ local epoch 0: 0.03608919680118561
Local loss @ local epoch 1: 0.031030353158712387
Local loss @ local epoch 2: 0.027014944702386856
Local loss @ local epoch 3: 0.023654667660593987
Local loss @ local epoch 4: 0.02088090032339096
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.0 seconds!
[tester] 
SST2Metric: acc=0.8612385321100917, hinge=0.8022170066833496, ce=0.49594658209062226
Local test acc @ epoch 91: 0.8612
Client 3 execute local training on 8 samples...
Local loss @ local epoch 0: 0.06165042519569397
Local loss @ local epoch 1: 0.05856052413582802
Local loss @ local epoch 2: 0.057094261050224304
Local loss @ local epoch 3: 0.05463284254074097
Local loss @ local epoch 4: 0.052197523415088654
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.96 seconds!
[tester] 
SST2Metric: acc=0.8577981651376146, hinge=0.7858136346854201, ce=0.48246526055213135
Local test acc @ epoch 91: 0.8578
Client 7 execute local training on 8 samples...
Local loss @ local epoch 0: 0.008080351166427135
Local loss @ local epoch 1: 0.006577034946531057
Local loss @ local epoch 2: 0.006127099972218275
Local loss @ local epoch 3: 0.006084046792238951
Local loss @ local epoch 4: 0.00598009442910552
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.08 seconds!
[tester] 
SST2Metric: acc=0.856651376146789, hinge=0.8060214600158394, ce=0.5007296678625632
Local test acc @ epoch 91: 0.8567
Client 0 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0020589339546859264
Local loss @ local epoch 1: 0.0018965534400194883
Local loss @ local epoch 2: 0.0018142765620723367
Local loss @ local epoch 3: 0.0017137662507593632
Local loss @ local epoch 4: 0.0016176484059542418
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.98 seconds!
[tester] 
SST2Metric: acc=0.856651376146789, hinge=0.8149760875406615, ce=0.5064352302843547
Local test acc @ epoch 91: 0.8567
Client 8 execute local training on 8 samples...
Local loss @ local epoch 0: 0.007201496511697769
Local loss @ local epoch 1: 0.004887644201517105
Local loss @ local epoch 2: 0.0036032977513968945
Local loss @ local epoch 3: 0.0029911943711340427
Local loss @ local epoch 4: 0.002795351669192314
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.04 seconds!
[tester] 
SST2Metric: acc=0.8440366972477065, hinge=0.9522885475136819, ce=0.6141371764206603
Local test acc @ epoch 91: 0.844
Client 6 execute local training on 8 samples...
Local loss @ local epoch 0: 0.48904290795326233
Local loss @ local epoch 1: 0.39642906188964844
Local loss @ local epoch 2: 0.31280630826950073
Local loss @ local epoch 3: 0.24038353562355042
Local loss @ local epoch 4: 0.18080121278762817
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.02 seconds!
[tester] 
SST2Metric: acc=0.8612385321100917, hinge=0.8092431162202031, ce=0.4996010726643513
Local test acc @ epoch 91: 0.8612
Client 5 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0027676732279360294
Local loss @ local epoch 1: 0.0018396824598312378
Local loss @ local epoch 2: 0.0013223572168499231
Local loss @ local epoch 3: 0.0010754006216302514
Local loss @ local epoch 4: 0.0010039954213425517
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.0 seconds!
[tester] 
SST2Metric: acc=0.8440366972477065, hinge=0.8723486671480564, ce=0.5449214727327296
Local test acc @ epoch 91: 0.844
Client 2 execute local training on 8 samples...
Local loss @ local epoch 0: 0.09124219417572021
Local loss @ local epoch 1: 0.06007456034421921
Local loss @ local epoch 2: 0.03969036042690277
Local loss @ local epoch 3: 0.026706386357545853
Local loss @ local epoch 4: 0.01854325644671917
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.06 seconds!
[tester] 
SST2Metric: acc=0.856651376146789, hinge=0.8145115776072949, ce=0.504649077153363
Local test acc @ epoch 91: 0.8567
Global evaluate on test data...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.49 seconds!
[tester] 
SST2Metric: acc=0.8073394495412844, hinge=0.9693877995014191, ce=0.551035475128954
Global test acc @ epoch 91: 0.8073
Global epoch 92...
Client 7 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0054768226109445095
Local loss @ local epoch 1: 0.005096275359392166
Local loss @ local epoch 2: 0.005029576364904642
Local loss @ local epoch 3: 0.004828738048672676
Local loss @ local epoch 4: 0.004590457305312157
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.97 seconds!
[tester] 
SST2Metric: acc=0.8577981651376146, hinge=0.8058093205231045, ce=0.4956570032240669
Local test acc @ epoch 92: 0.8578
Client 6 execute local training on 8 samples...
Local loss @ local epoch 0: 0.10155048221349716
Local loss @ local epoch 1: 0.07792302966117859
Local loss @ local epoch 2: 0.0655723437666893
Local loss @ local epoch 3: 0.06162082031369209
Local loss @ local epoch 4: 0.0617971196770668
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.97 seconds!
[tester] 
SST2Metric: acc=0.8520642201834863, hinge=0.8518670986004926, ce=0.5285459548830901
Local test acc @ epoch 92: 0.8521
Client 9 execute local training on 8 samples...
Local loss @ local epoch 0: 0.01976730301976204
Local loss @ local epoch 1: 0.012720204889774323
Local loss @ local epoch 2: 0.008481236174702644
Local loss @ local epoch 3: 0.006042602937668562
Local loss @ local epoch 4: 0.004733946640044451
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.98 seconds!
[tester] 
SST2Metric: acc=0.8577981651376146, hinge=0.8043680808959751, ce=0.49827866444424
Local test acc @ epoch 92: 0.8578
Client 4 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0399690717458725
Local loss @ local epoch 1: 0.03414091095328331
Local loss @ local epoch 2: 0.029600977897644043
Local loss @ local epoch 3: 0.025754909962415695
Local loss @ local epoch 4: 0.022548789158463478
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.07 seconds!
[tester] 
SST2Metric: acc=0.8623853211009175, hinge=0.7968324094737341, ce=0.49321306123902353
Local test acc @ epoch 92: 0.8624
Client 0 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0031777473632246256
Local loss @ local epoch 1: 0.002534403232857585
Local loss @ local epoch 2: 0.002292164135724306
Local loss @ local epoch 3: 0.0021960402373224497
Local loss @ local epoch 4: 0.0021006634924560785
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.08 seconds!
[tester] 
SST2Metric: acc=0.8520642201834863, hinge=0.8481491972273643, ce=0.5349621826322501
Local test acc @ epoch 92: 0.8521
Client 8 execute local training on 8 samples...
Local loss @ local epoch 0: 0.006599518470466137
Local loss @ local epoch 1: 0.00466291606426239
Local loss @ local epoch 2: 0.0036680526100099087
Local loss @ local epoch 3: 0.0032691997475922108
Local loss @ local epoch 4: 0.0031876894645392895
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.0 seconds!
[tester] 
SST2Metric: acc=0.838302752293578, hinge=1.02219026857013, ce=0.668006742711007
Local test acc @ epoch 92: 0.8383
Client 5 execute local training on 8 samples...
Local loss @ local epoch 0: 0.031745459884405136
Local loss @ local epoch 1: 0.020025329664349556
Local loss @ local epoch 2: 0.012809894047677517
Local loss @ local epoch 3: 0.008390928618609905
Local loss @ local epoch 4: 0.005661473609507084
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.97 seconds!
[tester] 
SST2Metric: acc=0.856651376146789, hinge=0.8183591431434002, ce=0.5122073347474141
Local test acc @ epoch 92: 0.8567
Client 1 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0006838007248006761
Local loss @ local epoch 1: 0.0006047606584616005
Local loss @ local epoch 2: 0.000557424034923315
Local loss @ local epoch 3: 0.0005095646483823657
Local loss @ local epoch 4: 0.00046111669507808983
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.2 seconds!
[tester] 
SST2Metric: acc=0.8577981651376146, hinge=0.808311874303249, ce=0.5032958659984774
Local test acc @ epoch 92: 0.8578
Client 2 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0171213336288929
Local loss @ local epoch 1: 0.011462286114692688
Local loss @ local epoch 2: 0.00806746818125248
Local loss @ local epoch 3: 0.006173016969114542
Local loss @ local epoch 4: 0.00526327732950449
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.99 seconds!
[tester] 
SST2Metric: acc=0.838302752293578, hinge=0.9893896138175912, ce=0.6428255834138109
Local test acc @ epoch 92: 0.8383
Client 3 execute local training on 8 samples...
Local loss @ local epoch 0: 0.24134863913059235
Local loss @ local epoch 1: 0.18835830688476562
Local loss @ local epoch 2: 0.1438613384962082
Local loss @ local epoch 3: 0.10833917558193207
Local loss @ local epoch 4: 0.08194839954376221
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.97 seconds!
[tester] 
SST2Metric: acc=0.8612385321100917, hinge=0.8028345468941085, ce=0.49847322065247557
Local test acc @ epoch 92: 0.8612
Global evaluate on test data...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.46 seconds!
[tester] 
SST2Metric: acc=0.805045871559633, hinge=0.9914300696018639, ce=0.5696961567843966
Global test acc @ epoch 92: 0.805
Global epoch 93...
Client 0 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0018268388230353594
Local loss @ local epoch 1: 0.001709558186121285
Local loss @ local epoch 2: 0.00161396199837327
Local loss @ local epoch 3: 0.0015220189234241843
Local loss @ local epoch 4: 0.0014484847197309136
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.01 seconds!
[tester] 
SST2Metric: acc=0.856651376146789, hinge=0.8179021514610413, ce=0.5084320902445251
Local test acc @ epoch 93: 0.8567
Client 3 execute local training on 8 samples...
Local loss @ local epoch 0: 0.110463447868824
Local loss @ local epoch 1: 0.08224494755268097
Local loss @ local epoch 2: 0.0624760277569294
Local loss @ local epoch 3: 0.05072431638836861
Local loss @ local epoch 4: 0.04573866352438927
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.97 seconds!
[tester] 
SST2Metric: acc=0.8474770642201835, hinge=0.8500953505345441, ce=0.5281830876713208
Local test acc @ epoch 93: 0.8475
Client 8 execute local training on 8 samples...
Local loss @ local epoch 0: 0.04751089960336685
Local loss @ local epoch 1: 0.030928701162338257
Local loss @ local epoch 2: 0.020236153155565262
Local loss @ local epoch 3: 0.013475731015205383
Local loss @ local epoch 4: 0.00922761857509613
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.0 seconds!
[tester] 
SST2Metric: acc=0.856651376146789, hinge=0.8192047204719771, ce=0.5093484084466857
Local test acc @ epoch 93: 0.8567
Client 6 execute local training on 8 samples...
Local loss @ local epoch 0: 0.17113113403320312
Local loss @ local epoch 1: 0.12535281479358673
Local loss @ local epoch 2: 0.0930069163441658
Local loss @ local epoch 3: 0.07282040268182755
Local loss @ local epoch 4: 0.06262725591659546
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.02 seconds!
[tester] 
SST2Metric: acc=0.8474770642201835, hinge=0.8464621500957996, ce=0.525542141547119
Local test acc @ epoch 93: 0.8475
Client 9 execute local training on 8 samples...
Local loss @ local epoch 0: 0.014258510433137417
Local loss @ local epoch 1: 0.00926121138036251
Local loss @ local epoch 2: 0.00633521843701601
Local loss @ local epoch 3: 0.004736938048154116
Local loss @ local epoch 4: 0.003971461206674576
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.99 seconds!
[tester] 
SST2Metric: acc=0.8555045871559633, hinge=0.8183384349039935, ce=0.509922386047488
Local test acc @ epoch 93: 0.8555
Client 5 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0034830449149012566
Local loss @ local epoch 1: 0.0022577799391001463
Local loss @ local epoch 2: 0.0015428813640028238
Local loss @ local epoch 3: 0.0011544821318238974
Local loss @ local epoch 4: 0.0009758754167705774
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.09 seconds!
[tester] 
SST2Metric: acc=0.8486238532110092, hinge=0.8522769177178724, ce=0.5314503183263705
Local test acc @ epoch 93: 0.8486
Client 2 execute local training on 8 samples...
Local loss @ local epoch 0: 0.07444848120212555
Local loss @ local epoch 1: 0.04876595735549927
Local loss @ local epoch 2: 0.032182104885578156
Local loss @ local epoch 3: 0.021716592833399773
Local loss @ local epoch 4: 0.015186851844191551
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.01 seconds!
[tester] 
SST2Metric: acc=0.8543577981651376, hinge=0.8231000445031245, ce=0.5145056260030914
Local test acc @ epoch 93: 0.8544
Client 1 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0006780078983865678
Local loss @ local epoch 1: 0.0006059023435227573
Local loss @ local epoch 2: 0.0005538260447792709
Local loss @ local epoch 3: 0.0005017842631787062
Local loss @ local epoch 4: 0.0004525897675193846
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.07 seconds!
[tester] 
SST2Metric: acc=0.856651376146789, hinge=0.8152513525901585, ce=0.5094299136221498
Local test acc @ epoch 93: 0.8567
Client 4 execute local training on 8 samples...
Local loss @ local epoch 0: 0.03017931617796421
Local loss @ local epoch 1: 0.026117518544197083
Local loss @ local epoch 2: 0.022985637187957764
Local loss @ local epoch 3: 0.02031782642006874
Local loss @ local epoch 4: 0.018087618052959442
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.13 seconds!
[tester] 
SST2Metric: acc=0.8623853211009175, hinge=0.8044711336356785, ce=0.49943811287612166
Local test acc @ epoch 93: 0.8624
Client 7 execute local training on 8 samples...
Local loss @ local epoch 0: 0.007418330758810043
Local loss @ local epoch 1: 0.006523663178086281
Local loss @ local epoch 2: 0.006150919944047928
Local loss @ local epoch 3: 0.005783641245216131
Local loss @ local epoch 4: 0.005380013957619667
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.94 seconds!
[tester] 
SST2Metric: acc=0.8612385321100917, hinge=0.8072400507303553, ce=0.5031631614867632
Local test acc @ epoch 93: 0.8612
Global evaluate on test data...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.34 seconds!
[tester] 
SST2Metric: acc=0.8084862385321101, hinge=0.9350948320069444, ce=0.5266566462750726
Global test acc @ epoch 93: 0.8085
Global epoch 94...
Client 7 execute local training on 8 samples...
Local loss @ local epoch 0: 0.004727277439087629
Local loss @ local epoch 1: 0.004550446756184101
Local loss @ local epoch 2: 0.004327986389398575
Local loss @ local epoch 3: 0.004135261755436659
Local loss @ local epoch 4: 0.003986263647675514
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.01 seconds!
[tester] 
SST2Metric: acc=0.8600917431192661, hinge=0.8154407474425954, ce=0.5041436651895685
Local test acc @ epoch 94: 0.8601
Client 5 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0013992715394124389
Local loss @ local epoch 1: 0.0010510822758078575
Local loss @ local epoch 2: 0.0009178905747830868
Local loss @ local epoch 3: 0.0009145780350081623
Local loss @ local epoch 4: 0.0009461500449106097
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.03 seconds!
[tester] 
SST2Metric: acc=0.8428899082568807, hinge=0.8868497630598349, ce=0.5579673317720293
Local test acc @ epoch 94: 0.8429
Client 2 execute local training on 8 samples...
Local loss @ local epoch 0: 0.10744757950305939
Local loss @ local epoch 1: 0.07118530571460724
Local loss @ local epoch 2: 0.047096848487854004
Local loss @ local epoch 3: 0.0315835177898407
Local loss @ local epoch 4: 0.021749651059508324
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.98 seconds!
[tester] 
SST2Metric: acc=0.8612385321100917, hinge=0.8162604819197173, ce=0.5076753670406323
Local test acc @ epoch 94: 0.8612
Client 4 execute local training on 8 samples...
Local loss @ local epoch 0: 0.04910813644528389
Local loss @ local epoch 1: 0.04171442240476608
Local loss @ local epoch 2: 0.03597770631313324
Local loss @ local epoch 3: 0.031121056526899338
Local loss @ local epoch 4: 0.027042392641305923
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.01 seconds!
[tester] 
SST2Metric: acc=0.8600917431192661, hinge=0.8014823693201083, ce=0.49816502046230926
Local test acc @ epoch 94: 0.8601
Client 8 execute local training on 8 samples...
Local loss @ local epoch 0: 0.01502850279211998
Local loss @ local epoch 1: 0.009766858071088791
Local loss @ local epoch 2: 0.006562636699527502
Local loss @ local epoch 3: 0.0046812985092401505
Local loss @ local epoch 4: 0.0036348076537251472
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.07 seconds!
[tester] 
SST2Metric: acc=0.8428899082568807, hinge=0.9382697907062846, ce=0.6056789896916064
Local test acc @ epoch 94: 0.8429
Client 0 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0030242595821619034
Local loss @ local epoch 1: 0.002133693778887391
Local loss @ local epoch 2: 0.001686233445070684
Local loss @ local epoch 3: 0.001533332047984004
Local loss @ local epoch 4: 0.0015485147014260292
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.01 seconds!
[tester] 
SST2Metric: acc=0.856651376146789, hinge=0.8106005734533345, ce=0.5041156534838208
Local test acc @ epoch 94: 0.8567
Client 1 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0005789948627352715
Local loss @ local epoch 1: 0.0005204094923101366
Local loss @ local epoch 2: 0.0004725109611172229
Local loss @ local epoch 3: 0.00042726731044240296
Local loss @ local epoch 4: 0.0003860970609821379
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.93 seconds!
[tester] 
SST2Metric: acc=0.8532110091743119, hinge=0.8215942288484048, ce=0.5145797704796653
Local test acc @ epoch 94: 0.8532
Client 6 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0911659374833107
Local loss @ local epoch 1: 0.06569276750087738
Local loss @ local epoch 2: 0.050253212451934814
Local loss @ local epoch 3: 0.04277503117918968
Local loss @ local epoch 4: 0.040829479694366455
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.91 seconds!
[tester] 
SST2Metric: acc=0.8555045871559633, hinge=0.8054238605663318, ce=0.4968508766497349
Local test acc @ epoch 94: 0.8555
Client 9 execute local training on 8 samples...
Local loss @ local epoch 0: 0.016903048381209373
Local loss @ local epoch 1: 0.011468160897493362
Local loss @ local epoch 2: 0.008509530685842037
Local loss @ local epoch 3: 0.00713329715654254
Local loss @ local epoch 4: 0.006653905846178532
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.96 seconds!
[tester] 
SST2Metric: acc=0.8532110091743119, hinge=0.8273723594365864, ce=0.5194626780375817
Local test acc @ epoch 94: 0.8532
Client 3 execute local training on 8 samples...
Local loss @ local epoch 0: 0.07104674726724625
Local loss @ local epoch 1: 0.057044144719839096
Local loss @ local epoch 2: 0.05130019411444664
Local loss @ local epoch 3: 0.05113884061574936
Local loss @ local epoch 4: 0.05259569361805916
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.02 seconds!
[tester] 
SST2Metric: acc=0.8589449541284404, hinge=0.802104183156556, ce=0.4949475233256176
Local test acc @ epoch 94: 0.8589
Global evaluate on test data...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.55 seconds!
[tester] 
SST2Metric: acc=0.8107798165137615, hinge=0.9266469194254744, ce=0.5213333551511339
Global test acc @ epoch 94: 0.8108
Global epoch 95...
Client 2 execute local training on 8 samples...
Local loss @ local epoch 0: 0.02323807217180729
Local loss @ local epoch 1: 0.015325626358389854
Local loss @ local epoch 2: 0.010470585897564888
Local loss @ local epoch 3: 0.007610954809933901
Local loss @ local epoch 4: 0.006038757041096687
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.96 seconds!
[tester] 
SST2Metric: acc=0.8405963302752294, hinge=0.9292539710845422, ce=0.5988135773232238
Local test acc @ epoch 95: 0.8406
Client 5 execute local training on 8 samples...
Local loss @ local epoch 0: 0.013446438126266003
Local loss @ local epoch 1: 0.008411582559347153
Local loss @ local epoch 2: 0.005364020820707083
Local loss @ local epoch 3: 0.00352480448782444
Local loss @ local epoch 4: 0.0024070448707789183
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.04 seconds!
[tester] 
SST2Metric: acc=0.8635321100917431, hinge=0.8047531066684548, ce=0.4988083419749949
Local test acc @ epoch 95: 0.8635
Client 1 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0005854907212778926
Local loss @ local epoch 1: 0.0004990700981579721
Local loss @ local epoch 2: 0.00046093747369013727
Local loss @ local epoch 3: 0.00043264703708700836
Local loss @ local epoch 4: 0.00040043104672804475
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.98 seconds!
[tester] 
SST2Metric: acc=0.8532110091743119, hinge=0.8308467669497936, ce=0.522790344034422
Local test acc @ epoch 95: 0.8532
Client 3 execute local training on 8 samples...
Local loss @ local epoch 0: 0.08251631259918213
Local loss @ local epoch 1: 0.06290335953235626
Local loss @ local epoch 2: 0.051927123218774796
Local loss @ local epoch 3: 0.048093996942043304
Local loss @ local epoch 4: 0.048818062990903854
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.97 seconds!
[tester] 
SST2Metric: acc=0.8577981651376146, hinge=0.8053178501511933, ce=0.49629667805830324
Local test acc @ epoch 95: 0.8578
Client 8 execute local training on 8 samples...
Local loss @ local epoch 0: 0.055318742990493774
Local loss @ local epoch 1: 0.036242756992578506
Local loss @ local epoch 2: 0.023811347782611847
Local loss @ local epoch 3: 0.015894176438450813
Local loss @ local epoch 4: 0.01089787669479847
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.03 seconds!
[tester] 
SST2Metric: acc=0.8532110091743119, hinge=0.8399860055074779, ce=0.5302063460240714
Local test acc @ epoch 95: 0.8532
Client 7 execute local training on 8 samples...
Local loss @ local epoch 0: 0.005825492553412914
Local loss @ local epoch 1: 0.004678645171225071
Local loss @ local epoch 2: 0.004347509238868952
Local loss @ local epoch 3: 0.004374452400952578
Local loss @ local epoch 4: 0.004373214673250914
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.98 seconds!
[tester] 
SST2Metric: acc=0.8635321100917431, hinge=0.801667179536382, ce=0.496197997572631
Local test acc @ epoch 95: 0.8635
Client 6 execute local training on 8 samples...
Local loss @ local epoch 0: 0.06842055171728134
Local loss @ local epoch 1: 0.05577424541115761
Local loss @ local epoch 2: 0.051288001239299774
Local loss @ local epoch 3: 0.05056089535355568
Local loss @ local epoch 4: 0.04961731657385826
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.01 seconds!
[tester] 
SST2Metric: acc=0.8532110091743119, hinge=0.8201877567746224, ce=0.5077086640431101
Local test acc @ epoch 95: 0.8532
Client 4 execute local training on 8 samples...
Local loss @ local epoch 0: 0.04870186373591423
Local loss @ local epoch 1: 0.03280868008732796
Local loss @ local epoch 2: 0.02358236163854599
Local loss @ local epoch 3: 0.018897835165262222
Local loss @ local epoch 4: 0.017014101147651672
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.97 seconds!
[tester] 
SST2Metric: acc=0.856651376146789, hinge=0.8123144477332404, ce=0.5076024874285453
Local test acc @ epoch 95: 0.8567
Client 0 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0027600969187915325
Local loss @ local epoch 1: 0.002272282959893346
Local loss @ local epoch 2: 0.0020721228793263435
Local loss @ local epoch 3: 0.0019426426151767373
Local loss @ local epoch 4: 0.0018074101535603404
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.03 seconds!
[tester] 
SST2Metric: acc=0.8520642201834863, hinge=0.8534207759647194, ce=0.5414164132951789
Local test acc @ epoch 95: 0.8521
Client 9 execute local training on 8 samples...
Local loss @ local epoch 0: 0.007688088808208704
Local loss @ local epoch 1: 0.005794020835310221
Local loss @ local epoch 2: 0.004998089279979467
Local loss @ local epoch 3: 0.0046992371790111065
Local loss @ local epoch 4: 0.0044969217851758
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.03 seconds!
[tester] 
SST2Metric: acc=0.8635321100917431, hinge=0.8090739714989968, ce=0.5031668973089102
Local test acc @ epoch 95: 0.8635
Global evaluate on test data...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.44 seconds!
[tester] 
SST2Metric: acc=0.8096330275229358, hinge=0.9431825794211222, ce=0.5348534563112013
Global test acc @ epoch 95: 0.8096
Global epoch 96...
Client 0 execute local training on 8 samples...
Local loss @ local epoch 0: 0.001962965587154031
Local loss @ local epoch 1: 0.001626546960324049
Local loss @ local epoch 2: 0.0015399935655295849
Local loss @ local epoch 3: 0.001523266895674169
Local loss @ local epoch 4: 0.0014792485162615776
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.89 seconds!
[tester] 
SST2Metric: acc=0.8520642201834863, hinge=0.8421584050589745, ce=0.5336215174660821
Local test acc @ epoch 96: 0.8521
Client 6 execute local training on 8 samples...
Local loss @ local epoch 0: 0.1936696171760559
Local loss @ local epoch 1: 0.14148907363414764
Local loss @ local epoch 2: 0.10264390707015991
Local loss @ local epoch 3: 0.07611572742462158
Local loss @ local epoch 4: 0.05999939143657684
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.9 seconds!
[tester] 
SST2Metric: acc=0.856651376146789, hinge=0.8145721536984137, ce=0.5039988881694751
Local test acc @ epoch 96: 0.8567
Client 7 execute local training on 8 samples...
Local loss @ local epoch 0: 0.006296536419540644
Local loss @ local epoch 1: 0.004737677983939648
Local loss @ local epoch 2: 0.004080438520759344
Local loss @ local epoch 3: 0.00400271313264966
Local loss @ local epoch 4: 0.004157057031989098
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.04 seconds!
[tester] 
SST2Metric: acc=0.8577981651376146, hinge=0.8157746768599257, ce=0.5099985474229837
Local test acc @ epoch 96: 0.8578
Client 8 execute local training on 8 samples...
Local loss @ local epoch 0: 0.008211463689804077
Local loss @ local epoch 1: 0.005425998009741306
Local loss @ local epoch 2: 0.003796652425080538
Local loss @ local epoch 3: 0.002914439421147108
Local loss @ local epoch 4: 0.002505526877939701
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.07 seconds!
[tester] 
SST2Metric: acc=0.8428899082568807, hinge=0.9525327646951063, ce=0.6178112291102671
Local test acc @ epoch 96: 0.8429
Client 1 execute local training on 8 samples...
Local loss @ local epoch 0: 0.001872148597612977
Local loss @ local epoch 1: 0.0012360523687675595
Local loss @ local epoch 2: 0.0008537130197510123
Local loss @ local epoch 3: 0.0006360088591463864
Local loss @ local epoch 4: 0.0005233120755292475
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.03 seconds!
[tester] 
SST2Metric: acc=0.856651376146789, hinge=0.8173788377177824, ce=0.5114867703136325
Local test acc @ epoch 96: 0.8567
Client 3 execute local training on 8 samples...
Local loss @ local epoch 0: 0.08800058811903
Local loss @ local epoch 1: 0.06569956988096237
Local loss @ local epoch 2: 0.05167606845498085
Local loss @ local epoch 3: 0.04495766758918762
Local loss @ local epoch 4: 0.04374444857239723
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.97 seconds!
[tester] 
SST2Metric: acc=0.8497706422018348, hinge=0.8401297436121407, ce=0.5228438309640538
Local test acc @ epoch 96: 0.8498
Client 5 execute local training on 8 samples...
Local loss @ local epoch 0: 0.000857843435369432
Local loss @ local epoch 1: 0.0008062486303970218
Local loss @ local epoch 2: 0.0007567123393528163
Local loss @ local epoch 3: 0.0007106627454049885
Local loss @ local epoch 4: 0.0006677831406705081
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.96 seconds!
[tester] 
SST2Metric: acc=0.8509174311926605, hinge=0.842086767794889, ce=0.5259396582393301
Local test acc @ epoch 96: 0.8509
Client 4 execute local training on 8 samples...
Local loss @ local epoch 0: 0.08837473392486572
Local loss @ local epoch 1: 0.06079289689660072
Local loss @ local epoch 2: 0.043112196028232574
Local loss @ local epoch 3: 0.03310443088412285
Local loss @ local epoch 4: 0.028221048414707184
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.97 seconds!
[tester] 
SST2Metric: acc=0.8589449541284404, hinge=0.8144636804904413, ce=0.5112769783844577
Local test acc @ epoch 96: 0.8589
Client 9 execute local training on 8 samples...
Local loss @ local epoch 0: 0.005211574956774712
Local loss @ local epoch 1: 0.004585913848131895
Local loss @ local epoch 2: 0.0040589505806565285
Local loss @ local epoch 3: 0.003595922840759158
Local loss @ local epoch 4: 0.003201061626896262
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.05 seconds!
[tester] 
SST2Metric: acc=0.8612385321100917, hinge=0.828598585834197, ce=0.5222100397975636
Local test acc @ epoch 96: 0.8612
Client 2 execute local training on 8 samples...
Local loss @ local epoch 0: 0.013667870312929153
Local loss @ local epoch 1: 0.009572891518473625
Local loss @ local epoch 2: 0.007392848841845989
Local loss @ local epoch 3: 0.006421281024813652
Local loss @ local epoch 4: 0.006112290546298027
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.14 seconds!
[tester] 
SST2Metric: acc=0.841743119266055, hinge=0.9701270893079426, ce=0.6336882218057418
Local test acc @ epoch 96: 0.8417
Global evaluate on test data...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.38 seconds!
[tester] 
SST2Metric: acc=0.8073394495412844, hinge=0.9758861088151232, ce=0.5606988591009563
Global test acc @ epoch 96: 0.8073
Global epoch 97...
Client 5 execute local training on 8 samples...
Local loss @ local epoch 0: 0.002204533899202943
Local loss @ local epoch 1: 0.001443980261683464
Local loss @ local epoch 2: 0.0010125018889084458
Local loss @ local epoch 3: 0.000794090679846704
Local loss @ local epoch 4: 0.0007134718471206725
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.98 seconds!
[tester] 
SST2Metric: acc=0.8497706422018348, hinge=0.8581516901287464, ce=0.5379261062966463
Local test acc @ epoch 97: 0.8498
Client 2 execute local training on 8 samples...
Local loss @ local epoch 0: 0.079897940158844
Local loss @ local epoch 1: 0.05241748318076134
Local loss @ local epoch 2: 0.03459439426660538
Local loss @ local epoch 3: 0.023287031799554825
Local loss @ local epoch 4: 0.01618042215704918
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.01 seconds!
[tester] 
SST2Metric: acc=0.8577981651376146, hinge=0.8229827890428928, ce=0.5168356490620506
Local test acc @ epoch 97: 0.8578
Client 1 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0005312595749273896
Local loss @ local epoch 1: 0.00047911121509969234
Local loss @ local epoch 2: 0.00043440103763714433
Local loss @ local epoch 3: 0.00039214963908307254
Local loss @ local epoch 4: 0.0003542086633387953
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.96 seconds!
[tester] 
SST2Metric: acc=0.8543577981651376, hinge=0.8221598837627183, ce=0.51749421213086
Local test acc @ epoch 97: 0.8544
Client 3 execute local training on 8 samples...
Local loss @ local epoch 0: 0.07009998708963394
Local loss @ local epoch 1: 0.054553691297769547
Local loss @ local epoch 2: 0.04700661450624466
Local loss @ local epoch 3: 0.04554932564496994
Local loss @ local epoch 4: 0.0470571331679821
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.99 seconds!
[tester] 
SST2Metric: acc=0.8555045871559633, hinge=0.8155257890530683, ce=0.5047307618431226
Local test acc @ epoch 97: 0.8555
Client 7 execute local training on 8 samples...
Local loss @ local epoch 0: 0.008288542740046978
Local loss @ local epoch 1: 0.005854343064129353
Local loss @ local epoch 2: 0.004566873423755169
Local loss @ local epoch 3: 0.004059887025505304
Local loss @ local epoch 4: 0.004031624179333448
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.01 seconds!
[tester] 
SST2Metric: acc=0.856651376146789, hinge=0.8196441505206834, ce=0.5161025215115467
Local test acc @ epoch 97: 0.8567
Client 6 execute local training on 8 samples...
Local loss @ local epoch 0: 0.11333487182855606
Local loss @ local epoch 1: 0.08139827102422714
Local loss @ local epoch 2: 0.06072834134101868
Local loss @ local epoch 3: 0.04936975985765457
Local loss @ local epoch 4: 0.04497774690389633
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.04 seconds!
[tester] 
SST2Metric: acc=0.8532110091743119, hinge=0.8210253920576988, ce=0.5092791568689741
Local test acc @ epoch 97: 0.8532
Client 4 execute local training on 8 samples...
Local loss @ local epoch 0: 0.048335131257772446
Local loss @ local epoch 1: 0.03271404653787613
Local loss @ local epoch 2: 0.023752465844154358
Local loss @ local epoch 3: 0.019306063652038574
Local loss @ local epoch 4: 0.017575271427631378
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.96 seconds!
[tester] 
SST2Metric: acc=0.856651376146789, hinge=0.8184368341067515, ce=0.5136248674310441
Local test acc @ epoch 97: 0.8567
Client 8 execute local training on 8 samples...
Local loss @ local epoch 0: 0.020039822906255722
Local loss @ local epoch 1: 0.013040794059634209
Local loss @ local epoch 2: 0.008725975640118122
Local loss @ local epoch 3: 0.00614920724183321
Local loss @ local epoch 4: 0.004669053014367819
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.97 seconds!
[tester] 
SST2Metric: acc=0.8360091743119266, hinge=0.9886509930595345, ce=0.6461234160708562
Local test acc @ epoch 97: 0.836
Client 9 execute local training on 8 samples...
Local loss @ local epoch 0: 0.023155640810728073
Local loss @ local epoch 1: 0.01475899200886488
Local loss @ local epoch 2: 0.009822456166148186
Local loss @ local epoch 3: 0.0070797959342598915
Local loss @ local epoch 4: 0.00568317761644721
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.91 seconds!
[tester] 
SST2Metric: acc=0.8600917431192661, hinge=0.8152482780294681, ce=0.5123445823697327
Local test acc @ epoch 97: 0.8601
Client 0 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0016220284160226583
Local loss @ local epoch 1: 0.0014366048853844404
Local loss @ local epoch 2: 0.0013905398081988096
Local loss @ local epoch 3: 0.0013402898330241442
Local loss @ local epoch 4: 0.001268158433958888
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.9 seconds!
[tester] 
SST2Metric: acc=0.8543577981651376, hinge=0.8337480446340841, ce=0.5282634392360955
Local test acc @ epoch 97: 0.8544
Global evaluate on test data...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.38 seconds!
[tester] 
SST2Metric: acc=0.8096330275229358, hinge=0.9428133223581752, ce=0.5357102806628639
Global test acc @ epoch 97: 0.8096
Global epoch 98...
Client 7 execute local training on 8 samples...
Local loss @ local epoch 0: 0.003956200089305639
Local loss @ local epoch 1: 0.0037122638896107674
Local loss @ local epoch 2: 0.0035025551915168762
Local loss @ local epoch 3: 0.0033218187745660543
Local loss @ local epoch 4: 0.003161986358463764
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.93 seconds!
[tester] 
SST2Metric: acc=0.8623853211009175, hinge=0.8144694703434585, ce=0.5072680020559613
Local test acc @ epoch 98: 0.8624
Client 3 execute local training on 8 samples...
Local loss @ local epoch 0: 0.06049732863903046
Local loss @ local epoch 1: 0.04732343554496765
Local loss @ local epoch 2: 0.04133469611406326
Local loss @ local epoch 3: 0.04063958302140236
Local loss @ local epoch 4: 0.04218846932053566
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.0 seconds!
[tester] 
SST2Metric: acc=0.8463302752293578, hinge=0.8749343043347018, ce=0.550600367856699
Local test acc @ epoch 98: 0.8463
Client 4 execute local training on 8 samples...
Local loss @ local epoch 0: 0.1079336479306221
Local loss @ local epoch 1: 0.07389797270298004
Local loss @ local epoch 2: 0.05062897875905037
Local loss @ local epoch 3: 0.03618323430418968
Local loss @ local epoch 4: 0.028126688674092293
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.87 seconds!
[tester] 
SST2Metric: acc=0.8623853211009175, hinge=0.8034503282210149, ce=0.5016379048978405
Local test acc @ epoch 98: 0.8624
Client 2 execute local training on 8 samples...
Local loss @ local epoch 0: 0.02127123810350895
Local loss @ local epoch 1: 0.014021094888448715
Local loss @ local epoch 2: 0.009591745212674141
Local loss @ local epoch 3: 0.007000175304710865
Local loss @ local epoch 4: 0.005593014881014824
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.94 seconds!
[tester] 
SST2Metric: acc=0.8428899082568807, hinge=0.936261814395222, ce=0.6067119206396923
Local test acc @ epoch 98: 0.8429
Client 9 execute local training on 8 samples...
Local loss @ local epoch 0: 0.012440096586942673
Local loss @ local epoch 1: 0.008073781616985798
Local loss @ local epoch 2: 0.005655989982187748
Local loss @ local epoch 3: 0.004452310036867857
Local loss @ local epoch 4: 0.003967050462961197
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.84 seconds!
[tester] 
SST2Metric: acc=0.8600917431192661, hinge=0.8148564696311951, ce=0.510318582642016
Local test acc @ epoch 98: 0.8601
Client 1 execute local training on 8 samples...
Local loss @ local epoch 0: 0.00048597020213492215
Local loss @ local epoch 1: 0.00043753147474490106
Local loss @ local epoch 2: 0.0003957980079576373
Local loss @ local epoch 3: 0.0003572973655536771
Local loss @ local epoch 4: 0.00032273787655867636
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.9 seconds!
[tester] 
SST2Metric: acc=0.8600917431192661, hinge=0.8203390739653089, ce=0.5167830079866557
Local test acc @ epoch 98: 0.8601
Client 0 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0016915627056732774
Local loss @ local epoch 1: 0.0015102126635611057
Local loss @ local epoch 2: 0.0014314656145870686
Local loss @ local epoch 3: 0.0013501165667548776
Local loss @ local epoch 4: 0.0012652940349653363
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.89 seconds!
[tester] 
SST2Metric: acc=0.8543577981651376, hinge=0.8321774662087816, ce=0.5279084442868517
Local test acc @ epoch 98: 0.8544
Client 8 execute local training on 8 samples...
Local loss @ local epoch 0: 0.005900234449654818
Local loss @ local epoch 1: 0.003981828689575195
Local loss @ local epoch 2: 0.0029033608734607697
Local loss @ local epoch 3: 0.002369973808526993
Local loss @ local epoch 4: 0.002174234017729759
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.97 seconds!
[tester] 
SST2Metric: acc=0.841743119266055, hinge=0.9892117750207219, ce=0.6488610146944507
Local test acc @ epoch 98: 0.8417
Client 5 execute local training on 8 samples...
Local loss @ local epoch 0: 0.017117368057370186
Local loss @ local epoch 1: 0.010792112909257412
Local loss @ local epoch 2: 0.006915721110999584
Local loss @ local epoch 3: 0.004545093979686499
Local loss @ local epoch 4: 0.0030812015756964684
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.91 seconds!
[tester] 
SST2Metric: acc=0.856651376146789, hinge=0.8290607021489275, ce=0.5225601103896416
Local test acc @ epoch 98: 0.8567
Client 6 execute local training on 8 samples...
Local loss @ local epoch 0: 0.1749725639820099
Local loss @ local epoch 1: 0.12838956713676453
Local loss @ local epoch 2: 0.0950116440653801
Local loss @ local epoch 3: 0.07369419932365417
Local loss @ local epoch 4: 0.062421735376119614
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.87 seconds!
[tester] 
SST2Metric: acc=0.8509174311926605, hinge=0.8548902420822634, ce=0.5372836210443468
Local test acc @ epoch 98: 0.8509
Global evaluate on test data...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.37 seconds!
[tester] 
SST2Metric: acc=0.8073394495412844, hinge=0.9714051336050034, ce=0.5586225842766532
Global test acc @ epoch 98: 0.8073
Global epoch 99...
Client 8 execute local training on 8 samples...
Local loss @ local epoch 0: 0.008625215850770473
Local loss @ local epoch 1: 0.00564992893487215
Local loss @ local epoch 2: 0.003882837016135454
Local loss @ local epoch 3: 0.002892144024372101
Local loss @ local epoch 4: 0.0023928142618387938
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.94 seconds!
[tester] 
SST2Metric: acc=0.8463302752293578, hinge=0.9441805632289396, ce=0.613711267549928
Local test acc @ epoch 99: 0.8463
Client 0 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0029044710099697113
Local loss @ local epoch 1: 0.001981167821213603
Local loss @ local epoch 2: 0.001469029812142253
Local loss @ local epoch 3: 0.0012348983436822891
Local loss @ local epoch 4: 0.001183290733024478
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.0 seconds!
[tester] 
SST2Metric: acc=0.8600917431192661, hinge=0.8211741822028379, ce=0.5150459258093012
Local test acc @ epoch 99: 0.8601
Client 5 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0018213853472843766
Local loss @ local epoch 1: 0.0012105396017432213
Local loss @ local epoch 2: 0.0008685384527780116
Local loss @ local epoch 3: 0.0007037005270831287
Local loss @ local epoch 4: 0.0006541343173012137
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.92 seconds!
[tester] 
SST2Metric: acc=0.8463302752293578, hinge=0.8743223129609309, ce=0.5521570981347745
Local test acc @ epoch 99: 0.8463
Client 2 execute local training on 8 samples...
Local loss @ local epoch 0: 0.07568014413118362
Local loss @ local epoch 1: 0.049519263207912445
Local loss @ local epoch 2: 0.03260282427072525
Local loss @ local epoch 3: 0.02191895619034767
Local loss @ local epoch 4: 0.015244346112012863
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.89 seconds!
[tester] 
SST2Metric: acc=0.856651376146789, hinge=0.8268049039425106, ce=0.5205426981355974
Local test acc @ epoch 99: 0.8567
Client 7 execute local training on 8 samples...
Local loss @ local epoch 0: 0.003955516964197159
Local loss @ local epoch 1: 0.003178270533680916
Local loss @ local epoch 2: 0.0029789998661726713
Local loss @ local epoch 3: 0.003062344389036298
Local loss @ local epoch 4: 0.003125724382698536
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.98 seconds!
[tester] 
SST2Metric: acc=0.856651376146789, hinge=0.8331460762734807, ce=0.521532381825478
Local test acc @ epoch 99: 0.8567
Client 3 execute local training on 8 samples...
Local loss @ local epoch 0: 0.04517845809459686
Local loss @ local epoch 1: 0.03870392218232155
Local loss @ local epoch 2: 0.03773949295282364
Local loss @ local epoch 3: 0.03843259438872337
Local loss @ local epoch 4: 0.03800007328391075
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.89 seconds!
[tester] 
SST2Metric: acc=0.8428899082568807, hinge=0.8879074957392631, ce=0.5613833882583202
Local test acc @ epoch 99: 0.8429
Client 4 execute local training on 8 samples...
Local loss @ local epoch 0: 0.11089581996202469
Local loss @ local epoch 1: 0.07627320289611816
Local loss @ local epoch 2: 0.05254164710640907
Local loss @ local epoch 3: 0.03781995177268982
Local loss @ local epoch 4: 0.029660752043128014
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.94 seconds!
[tester] 
SST2Metric: acc=0.8623853211009175, hinge=0.8106089201542216, ce=0.5068971233342565
Local test acc @ epoch 99: 0.8624
Client 6 execute local training on 8 samples...
Local loss @ local epoch 0: 0.09393001347780228
Local loss @ local epoch 1: 0.07050147652626038
Local loss @ local epoch 2: 0.05762232095003128
Local loss @ local epoch 3: 0.0528545118868351
Local loss @ local epoch 4: 0.052723076194524765
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.93 seconds!
[tester] 
SST2Metric: acc=0.856651376146789, hinge=0.8414904100632449, ce=0.5251777819699582
Local test acc @ epoch 99: 0.8567
Client 1 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0012991306139156222
Local loss @ local epoch 1: 0.0008348547853529453
Local loss @ local epoch 2: 0.0005620268639177084
Local loss @ local epoch 3: 0.0004087076522409916
Local loss @ local epoch 4: 0.000328848313074559
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.92 seconds!
[tester] 
SST2Metric: acc=0.8600917431192661, hinge=0.817452064894755, ce=0.5143148550323697
Local test acc @ epoch 99: 0.8601
Client 9 execute local training on 8 samples...
Local loss @ local epoch 0: 0.006827304605394602
Local loss @ local epoch 1: 0.005951799917966127
Local loss @ local epoch 2: 0.005219690036028624
Local loss @ local epoch 3: 0.004582580178976059
Local loss @ local epoch 4: 0.004041166044771671
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.95 seconds!
[tester] 
SST2Metric: acc=0.856651376146789, hinge=0.8261148422409635, ce=0.5214795627996514
Local test acc @ epoch 99: 0.8567
Global evaluate on test data...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.5 seconds!
[tester] 
SST2Metric: acc=0.8130733944954128, hinge=0.9318735096432748, ce=0.5282263587946671
Global test acc @ epoch 99: 0.8131
Global epoch 100...
Client 2 execute local training on 8 samples...
Local loss @ local epoch 0: 0.021305423229932785
Local loss @ local epoch 1: 0.014036368578672409
Local loss @ local epoch 2: 0.009637145325541496
Local loss @ local epoch 3: 0.007099945563822985
Local loss @ local epoch 4: 0.005754116456955671
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.96 seconds!
[tester] 
SST2Metric: acc=0.8474770642201835, hinge=0.9116394778183841, ce=0.5886842943387536
Local test acc @ epoch 100: 0.8475
Client 5 execute local training on 8 samples...
Local loss @ local epoch 0: 0.007952646352350712
Local loss @ local epoch 1: 0.004991852678358555
Local loss @ local epoch 2: 0.003199332393705845
Local loss @ local epoch 3: 0.00211834697984159
Local loss @ local epoch 4: 0.0014643820468336344
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.89 seconds!
[tester] 
SST2Metric: acc=0.8612385321100917, hinge=0.8179333915404223, ce=0.5106104712128541
Local test acc @ epoch 100: 0.8612
Client 3 execute local training on 8 samples...
Local loss @ local epoch 0: 0.05698446184396744
Local loss @ local epoch 1: 0.043837349861860275
Local loss @ local epoch 2: 0.03751499950885773
Local loss @ local epoch 3: 0.03634822368621826
Local loss @ local epoch 4: 0.03773181140422821
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.02 seconds!
[tester] 
SST2Metric: acc=0.8451834862385321, hinge=0.8788299515433268, ce=0.5539104208262603
Local test acc @ epoch 100: 0.8452
Client 7 execute local training on 8 samples...
Local loss @ local epoch 0: 0.00684356689453125
Local loss @ local epoch 1: 0.004704751074314117
Local loss @ local epoch 2: 0.0035010110586881638
Local loss @ local epoch 3: 0.0029367092065513134
Local loss @ local epoch 4: 0.002797842025756836
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.09 seconds!
[tester] 
SST2Metric: acc=0.8600917431192661, hinge=0.8190642864605703, ce=0.5119914850316207
Local test acc @ epoch 100: 0.8601
Client 8 execute local training on 8 samples...
Local loss @ local epoch 0: 0.00862570758908987
Local loss @ local epoch 1: 0.005626090802252293
Local loss @ local epoch 2: 0.003835278796032071
Local loss @ local epoch 3: 0.0028189511504024267
Local loss @ local epoch 4: 0.0022925459779798985
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.96 seconds!
[tester] 
SST2Metric: acc=0.8474770642201835, hinge=0.9123865259349893, ce=0.5896953360688629
Local test acc @ epoch 100: 0.8475
Client 0 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0022940845228731632
Local loss @ local epoch 1: 0.001602881820872426
Local loss @ local epoch 2: 0.0012433533556759357
Local loss @ local epoch 3: 0.001110018347389996
Local loss @ local epoch 4: 0.0011180000146850944
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.94 seconds!
[tester] 
SST2Metric: acc=0.8577981651376146, hinge=0.8260395972553743, ce=0.5174418029142009
Local test acc @ epoch 100: 0.8578
Client 9 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0036859186366200447
Local loss @ local epoch 1: 0.003180262865498662
Local loss @ local epoch 2: 0.0029370037373155355
Local loss @ local epoch 3: 0.002694318536669016
Local loss @ local epoch 4: 0.002435615286231041
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.95 seconds!
[tester] 
SST2Metric: acc=0.8612385321100917, hinge=0.8508033450316945, ce=0.5409615237058627
Local test acc @ epoch 100: 0.8612
Client 1 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0009052189998328686
Local loss @ local epoch 1: 0.0007445996743626893
Local loss @ local epoch 2: 0.0006775333313271403
Local loss @ local epoch 3: 0.0006374176009558141
Local loss @ local epoch 4: 0.0005918492679484189
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.91 seconds!
[tester] 
SST2Metric: acc=0.8600917431192661, hinge=0.8283654106866329, ce=0.5199606573591521
Local test acc @ epoch 100: 0.8601
Client 6 execute local training on 8 samples...
Local loss @ local epoch 0: 0.12858250737190247
Local loss @ local epoch 1: 0.095559261739254
Local loss @ local epoch 2: 0.07484790682792664
Local loss @ local epoch 3: 0.06458444148302078
Local loss @ local epoch 4: 0.061793625354766846
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.9 seconds!
[tester] 
SST2Metric: acc=0.8474770642201835, hinge=0.879690803108959, ce=0.5567784925266981
Local test acc @ epoch 100: 0.8475
Client 4 execute local training on 8 samples...
Local loss @ local epoch 0: 0.10502450168132782
Local loss @ local epoch 1: 0.07181549817323685
Local loss @ local epoch 2: 0.04923979565501213
Local loss @ local epoch 3: 0.03532245755195618
Local loss @ local epoch 4: 0.027635259553790092
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.96 seconds!
[tester] 
SST2Metric: acc=0.8612385321100917, hinge=0.8131290675576673, ce=0.5107586656882932
Local test acc @ epoch 100: 0.8612
Global evaluate on test data...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.43 seconds!
[tester] 
SST2Metric: acc=0.8107798165137615, hinge=0.9607721509736612, ce=0.5509940343673999
Global test acc @ epoch 100: 0.8108
Global epoch 101...
Client 1 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0005081489798612893
Local loss @ local epoch 1: 0.00045388707076199353
Local loss @ local epoch 2: 0.0004144214326515794
Local loss @ local epoch 3: 0.0003765016735997051
Local loss @ local epoch 4: 0.0003405334718991071
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.03 seconds!
[tester] 
SST2Metric: acc=0.8577981651376146, hinge=0.8239805534345295, ce=0.520978902354185
Local test acc @ epoch 101: 0.8578
Client 8 execute local training on 8 samples...
Local loss @ local epoch 0: 0.009625750593841076
Local loss @ local epoch 1: 0.006307988427579403
Local loss @ local epoch 2: 0.004331104923039675
Local loss @ local epoch 3: 0.003215628210455179
Local loss @ local epoch 4: 0.002643181011080742
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.93 seconds!
[tester] 
SST2Metric: acc=0.8440366972477065, hinge=0.9801475461743293, ce=0.6417764506476062
Local test acc @ epoch 101: 0.844
Client 6 execute local training on 8 samples...
Local loss @ local epoch 0: 0.37475207448005676
Local loss @ local epoch 1: 0.2930642068386078
Local loss @ local epoch 2: 0.22275559604167938
Local loss @ local epoch 3: 0.1652778834104538
Local loss @ local epoch 4: 0.12096312642097473
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.96 seconds!
[tester] 
SST2Metric: acc=0.8600917431192661, hinge=0.8168468929212028, ce=0.5142420631365577
Local test acc @ epoch 101: 0.8601
Client 0 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0015355118084698915
Local loss @ local epoch 1: 0.0012822991702705622
Local loss @ local epoch 2: 0.0012217388721182942
Local loss @ local epoch 3: 0.0012110118987038732
Local loss @ local epoch 4: 0.001175316865555942
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.89 seconds!
[tester] 
SST2Metric: acc=0.8543577981651376, hinge=0.8516310018683793, ce=0.5440677823263879
Local test acc @ epoch 101: 0.8544
Client 2 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0069528683088719845
Local loss @ local epoch 1: 0.005310607608407736
Local loss @ local epoch 2: 0.004630489740520716
Local loss @ local epoch 3: 0.004491996485739946
Local loss @ local epoch 4: 0.004503223579376936
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.99 seconds!
[tester] 
SST2Metric: acc=0.841743119266055, hinge=1.026616606690468, ce=0.6770425275948866
Local test acc @ epoch 101: 0.8417
Client 3 execute local training on 8 samples...
Local loss @ local epoch 0: 0.25007060170173645
Local loss @ local epoch 1: 0.196180060505867
Local loss @ local epoch 2: 0.1500522792339325
Local loss @ local epoch 3: 0.11222058534622192
Local loss @ local epoch 4: 0.0829373300075531
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.99 seconds!
[tester] 
SST2Metric: acc=0.8555045871559633, hinge=0.8295946420606123, ce=0.5253108937056722
Local test acc @ epoch 101: 0.8555
Client 7 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0036120410077273846
Local loss @ local epoch 1: 0.0031518645118921995
Local loss @ local epoch 2: 0.0030891839414834976
Local loss @ local epoch 3: 0.0030460127163678408
Local loss @ local epoch 4: 0.0029162585269659758
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.95 seconds!
[tester] 
SST2Metric: acc=0.8589449541284404, hinge=0.8214542987423206, ce=0.5139161211076949
Local test acc @ epoch 101: 0.8589
Client 5 execute local training on 8 samples...
Local loss @ local epoch 0: 0.001038254820741713
Local loss @ local epoch 1: 0.0007616629009135067
Local loss @ local epoch 2: 0.0006455696420744061
Local loss @ local epoch 3: 0.0006318619707599282
Local loss @ local epoch 4: 0.0006582795176655054
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.0 seconds!
[tester] 
SST2Metric: acc=0.8440366972477065, hinge=0.8943636137137719, ce=0.5677528308454576
Local test acc @ epoch 101: 0.844
Client 9 execute local training on 8 samples...
Local loss @ local epoch 0: 0.01942247897386551
Local loss @ local epoch 1: 0.012418841943144798
Local loss @ local epoch 2: 0.008171606808900833
Local loss @ local epoch 3: 0.005680379923433065
Local loss @ local epoch 4: 0.004283583257347345
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.95 seconds!
[tester] 
SST2Metric: acc=0.8600917431192661, hinge=0.8270851824808558, ce=0.5209169701778291
Local test acc @ epoch 101: 0.8601
Client 4 execute local training on 8 samples...
Local loss @ local epoch 0: 0.040429018437862396
Local loss @ local epoch 1: 0.0341169498860836
Local loss @ local epoch 2: 0.02975078672170639
Local loss @ local epoch 3: 0.026009153574705124
Local loss @ local epoch 4: 0.02272956818342209
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.03 seconds!
[tester] 
SST2Metric: acc=0.8612385321100917, hinge=0.817235374942832, ce=0.5154883188032385
Local test acc @ epoch 101: 0.8612
Global evaluate on test data...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.34 seconds!
[tester] 
SST2Metric: acc=0.8107798165137615, hinge=0.9983936466208292, ce=0.5802891270701833
Global test acc @ epoch 101: 0.8108
Global epoch 102...
Client 6 execute local training on 8 samples...
Local loss @ local epoch 0: 0.12733548879623413
Local loss @ local epoch 1: 0.09182044863700867
Local loss @ local epoch 2: 0.06820505857467651
Local loss @ local epoch 3: 0.05466137081384659
Local loss @ local epoch 4: 0.048857979476451874
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.9 seconds!
[tester] 
SST2Metric: acc=0.8543577981651376, hinge=0.835874723185093, ce=0.5229239796956898
Local test acc @ epoch 102: 0.8544
Client 7 execute local training on 8 samples...
Local loss @ local epoch 0: 0.006149255204945803
Local loss @ local epoch 1: 0.004315529949963093
Local loss @ local epoch 2: 0.0033351602032780647
Local loss @ local epoch 3: 0.002936122240498662
Local loss @ local epoch 4: 0.0029021655209362507
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.93 seconds!
[tester] 
SST2Metric: acc=0.8577981651376146, hinge=0.8225099471730923, ce=0.5189663343467438
Local test acc @ epoch 102: 0.8578
Client 4 execute local training on 8 samples...
Local loss @ local epoch 0: 0.028588751330971718
Local loss @ local epoch 1: 0.024529168382287025
Local loss @ local epoch 2: 0.021244678646326065
Local loss @ local epoch 3: 0.018544651567935944
Local loss @ local epoch 4: 0.016341622918844223
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.9 seconds!
[tester] 
SST2Metric: acc=0.8658256880733946, hinge=0.8098921186606819, ce=0.5093223998323617
Local test acc @ epoch 102: 0.8658
Client 1 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0002588135830592364
Local loss @ local epoch 1: 0.00022234534844756126
Local loss @ local epoch 2: 0.0002076914388453588
Local loss @ local epoch 3: 0.0001990215969271958
Local loss @ local epoch 4: 0.0001885070523712784
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.84 seconds!
[tester] 
SST2Metric: acc=0.8532110091743119, hinge=0.8619712204287905, ce=0.5496332352605025
Local test acc @ epoch 102: 0.8532
Client 5 execute local training on 8 samples...
Local loss @ local epoch 0: 0.005916036199778318
Local loss @ local epoch 1: 0.003644796321168542
Local loss @ local epoch 2: 0.0023175780661404133
Local loss @ local epoch 3: 0.0015530760865658522
Local loss @ local epoch 4: 0.0011212420649826527
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.96 seconds!
[tester] 
SST2Metric: acc=0.856651376146789, hinge=0.8091243549224434, ce=0.5060692641714806
Local test acc @ epoch 102: 0.8567
Client 2 execute local training on 8 samples...
Local loss @ local epoch 0: 0.07611808180809021
Local loss @ local epoch 1: 0.05032656341791153
Local loss @ local epoch 2: 0.03342081606388092
Local loss @ local epoch 3: 0.02253468707203865
Local loss @ local epoch 4: 0.015558919869363308
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.89 seconds!
[tester] 
SST2Metric: acc=0.8577981651376146, hinge=0.8532537246515991, ce=0.5443620571787597
Local test acc @ epoch 102: 0.8578
Client 0 execute local training on 8 samples...
Local loss @ local epoch 0: 0.001695374259725213
Local loss @ local epoch 1: 0.0015009575290605426
Local loss @ local epoch 2: 0.0013472880236804485
Local loss @ local epoch 3: 0.0012254004832357168
Local loss @ local epoch 4: 0.0011325215455144644
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.99 seconds!
[tester] 
SST2Metric: acc=0.856651376146789, hinge=0.8393312721350871, ce=0.5346206691419949
Local test acc @ epoch 102: 0.8567
Client 9 execute local training on 8 samples...
Local loss @ local epoch 0: 0.00483174342662096
Local loss @ local epoch 1: 0.004067687317728996
Local loss @ local epoch 2: 0.003707933472469449
Local loss @ local epoch 3: 0.0034000256564468145
Local loss @ local epoch 4: 0.0030714620370417833
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.95 seconds!
[tester] 
SST2Metric: acc=0.8600917431192661, hinge=0.8361779910708786, ce=0.5291106371906449
Local test acc @ epoch 102: 0.8601
Client 3 execute local training on 8 samples...
Local loss @ local epoch 0: 0.09887457638978958
Local loss @ local epoch 1: 0.07232315093278885
Local loss @ local epoch 2: 0.05368397384881973
Local loss @ local epoch 3: 0.04231153056025505
Local loss @ local epoch 4: 0.03695446625351906
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.92 seconds!
[tester] 
SST2Metric: acc=0.8486238532110092, hinge=0.8801565004871525, ce=0.5590623600756612
Local test acc @ epoch 102: 0.8486
Client 8 execute local training on 8 samples...
Local loss @ local epoch 0: 0.03412662446498871
Local loss @ local epoch 1: 0.022045381367206573
Local loss @ local epoch 2: 0.01435382291674614
Local loss @ local epoch 3: 0.00952935591340065
Local loss @ local epoch 4: 0.006508189719170332
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.0 seconds!
[tester] 
SST2Metric: acc=0.8589449541284404, hinge=0.8369119149555854, ce=0.5295471497133177
Local test acc @ epoch 102: 0.8589
Global evaluate on test data...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.4 seconds!
[tester] 
SST2Metric: acc=0.8084862385321101, hinge=0.9344532631952828, ce=0.5322295852206271
Global test acc @ epoch 102: 0.8085
Global epoch 103...
Client 3 execute local training on 8 samples...
Local loss @ local epoch 0: 0.04626668617129326
Local loss @ local epoch 1: 0.03905244916677475
Local loss @ local epoch 2: 0.03759201243519783
Local loss @ local epoch 3: 0.03833999112248421
Local loss @ local epoch 4: 0.03823470696806908
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.92 seconds!
[tester] 
SST2Metric: acc=0.8555045871559633, hinge=0.8319227773388591, ce=0.5202002926391054
Local test acc @ epoch 103: 0.8555
Client 2 execute local training on 8 samples...
Local loss @ local epoch 0: 0.05335476994514465
Local loss @ local epoch 1: 0.03481548652052879
Local loss @ local epoch 2: 0.022971386089920998
Local loss @ local epoch 3: 0.0155123770236969
Local loss @ local epoch 4: 0.010836831294000149
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.01 seconds!
[tester] 
SST2Metric: acc=0.8543577981651376, hinge=0.8435582515843417, ce=0.5371440499938208
Local test acc @ epoch 103: 0.8544
Client 8 execute local training on 8 samples...
Local loss @ local epoch 0: 0.005917724221944809
Local loss @ local epoch 1: 0.003985648043453693
Local loss @ local epoch 2: 0.0028931687120348215
Local loss @ local epoch 3: 0.0023435400798916817
Local loss @ local epoch 4: 0.0021289533469825983
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.98 seconds!
[tester] 
SST2Metric: acc=0.8405963302752294, hinge=1.0098534956437732, ce=0.6655228914590333
Local test acc @ epoch 103: 0.8406
Client 0 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0035344790667295456
Local loss @ local epoch 1: 0.0023340177722275257
Local loss @ local epoch 2: 0.0016230647452175617
Local loss @ local epoch 3: 0.0012318482622504234
Local loss @ local epoch 4: 0.0010494687594473362
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.91 seconds!
[tester] 
SST2Metric: acc=0.8555045871559633, hinge=0.8350411542785277, ce=0.529795454669511
Local test acc @ epoch 103: 0.8555
Client 7 execute local training on 8 samples...
Local loss @ local epoch 0: 0.00362771307118237
Local loss @ local epoch 1: 0.0028566885739564896
Local loss @ local epoch 2: 0.0026151228230446577
Local loss @ local epoch 3: 0.0026616703253239393
Local loss @ local epoch 4: 0.0027369733434170485
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.97 seconds!
[tester] 
SST2Metric: acc=0.8589449541284404, hinge=0.834847000213938, ce=0.5241303795272028
Local test acc @ epoch 103: 0.8589
Client 4 execute local training on 8 samples...
Local loss @ local epoch 0: 0.04372374713420868
Local loss @ local epoch 1: 0.03203307464718819
Local loss @ local epoch 2: 0.026153113692998886
Local loss @ local epoch 3: 0.023539911955595016
Local loss @ local epoch 4: 0.02218996360898018
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.95 seconds!
[tester] 
SST2Metric: acc=0.8600917431192661, hinge=0.8235589443543635, ce=0.5207361362467589
Local test acc @ epoch 103: 0.8601
Client 1 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0002876170619856566
Local loss @ local epoch 1: 0.0002624134358484298
Local loss @ local epoch 2: 0.00023869465803727508
Local loss @ local epoch 3: 0.00021705830295104533
Local loss @ local epoch 4: 0.00019766911282204092
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.0 seconds!
[tester] 
SST2Metric: acc=0.8555045871559633, hinge=0.8350619958081377, ce=0.5296761245161579
Local test acc @ epoch 103: 0.8555
Client 5 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0034123845398426056
Local loss @ local epoch 1: 0.0021390276961028576
Local loss @ local epoch 2: 0.0014036264037713408
Local loss @ local epoch 3: 0.000995605019852519
Local loss @ local epoch 4: 0.0007869195542298257
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.99 seconds!
[tester] 
SST2Metric: acc=0.8543577981651376, hinge=0.8318854532110582, ce=0.5216110177170442
Local test acc @ epoch 103: 0.8544
Client 9 execute local training on 8 samples...
Local loss @ local epoch 0: 0.02037690579891205
Local loss @ local epoch 1: 0.01346918661147356
Local loss @ local epoch 2: 0.00949329137802124
Local loss @ local epoch 3: 0.0074181039817631245
Local loss @ local epoch 4: 0.006506565026938915
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.04 seconds!
[tester] 
SST2Metric: acc=0.8589449541284404, hinge=0.8301351066302816, ce=0.5268098122756416
Local test acc @ epoch 103: 0.8589
Client 6 execute local training on 8 samples...
Local loss @ local epoch 0: 0.08489547669887543
Local loss @ local epoch 1: 0.06148657575249672
Local loss @ local epoch 2: 0.04765934869647026
Local loss @ local epoch 3: 0.041346944868564606
Local loss @ local epoch 4: 0.040076710283756256
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.95 seconds!
[tester] 
SST2Metric: acc=0.8555045871559633, hinge=0.8215091232586345, ce=0.5149946835809472
Local test acc @ epoch 103: 0.8555
Global evaluate on test data...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.53 seconds!
[tester] 
SST2Metric: acc=0.8142201834862385, hinge=0.9429302942862204, ce=0.5388421458405775
Global test acc @ epoch 103: 0.8142
Global epoch 104...
Client 7 execute local training on 8 samples...
Local loss @ local epoch 0: 0.002870267489925027
Local loss @ local epoch 1: 0.0026858546771109104
Local loss @ local epoch 2: 0.002528955927118659
Local loss @ local epoch 3: 0.0023963963612914085
Local loss @ local epoch 4: 0.002282190602272749
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.93 seconds!
[tester] 
SST2Metric: acc=0.8600917431192661, hinge=0.8239376380902912, ce=0.5180922722590259
Local test acc @ epoch 104: 0.8601
Client 5 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0009126564837060869
Local loss @ local epoch 1: 0.0006761991535313427
Local loss @ local epoch 2: 0.0005777102196589112
Local loss @ local epoch 3: 0.0005669945967383683
Local loss @ local epoch 4: 0.0005885992432013154
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.94 seconds!
[tester] 
SST2Metric: acc=0.8440366972477065, hinge=0.8916264550948362, ce=0.5678649145389967
Local test acc @ epoch 104: 0.844
Client 4 execute local training on 8 samples...
Local loss @ local epoch 0: 0.09184703975915909
Local loss @ local epoch 1: 0.06228799372911453
Local loss @ local epoch 2: 0.04282157123088837
Local loss @ local epoch 3: 0.031233303248882294
Local loss @ local epoch 4: 0.02509586326777935
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.9 seconds!
[tester] 
SST2Metric: acc=0.8600917431192661, hinge=0.8158074143009448, ce=0.5147146719711867
Local test acc @ epoch 104: 0.8601
Client 9 execute local training on 8 samples...
Local loss @ local epoch 0: 0.004907149355858564
Local loss @ local epoch 1: 0.004208966623991728
Local loss @ local epoch 2: 0.0038248212076723576
Local loss @ local epoch 3: 0.003456330159679055
Local loss @ local epoch 4: 0.0030865375883877277
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.95 seconds!
[tester] 
SST2Metric: acc=0.8600917431192661, hinge=0.8395644348422322, ce=0.5355072561160175
Local test acc @ epoch 104: 0.8601
Client 8 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0035873919259756804
Local loss @ local epoch 1: 0.0024786486756056547
Local loss @ local epoch 2: 0.0018871123902499676
Local loss @ local epoch 3: 0.0016324814641848207
Local loss @ local epoch 4: 0.0015781380934640765
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.92 seconds!
[tester] 
SST2Metric: acc=0.8440366972477065, hinge=0.9580363345528962, ce=0.6303295821953141
Local test acc @ epoch 104: 0.844
Client 0 execute local training on 8 samples...
Local loss @ local epoch 0: 0.003173633012920618
Local loss @ local epoch 1: 0.0020980187691748142
Local loss @ local epoch 2: 0.001469484530389309
Local loss @ local epoch 3: 0.0011337650939822197
Local loss @ local epoch 4: 0.0009897372219711542
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.97 seconds!
[tester] 
SST2Metric: acc=0.8623853211009175, hinge=0.840877973295133, ce=0.5350482002239239
Local test acc @ epoch 104: 0.8624
Client 2 execute local training on 8 samples...
Local loss @ local epoch 0: 0.011232785880565643
Local loss @ local epoch 1: 0.008070111274719238
Local loss @ local epoch 2: 0.006486530415713787
Local loss @ local epoch 3: 0.005837734788656235
Local loss @ local epoch 4: 0.005617279559373856
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.08 seconds!
[tester] 
SST2Metric: acc=0.8463302752293578, hinge=0.9500371651091707, ce=0.6227118574665462
Local test acc @ epoch 104: 0.8463
Client 6 execute local training on 8 samples...
Local loss @ local epoch 0: 0.4091353118419647
Local loss @ local epoch 1: 0.3249596953392029
Local loss @ local epoch 2: 0.25138455629348755
Local loss @ local epoch 3: 0.18998314440250397
Local loss @ local epoch 4: 0.14145749807357788
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.11 seconds!
[tester] 
SST2Metric: acc=0.8600917431192661, hinge=0.8266214468610396, ce=0.5229699974401679
Local test acc @ epoch 104: 0.8601
Client 3 execute local training on 8 samples...
Local loss @ local epoch 0: 0.060506757348775864
Local loss @ local epoch 1: 0.045312706381082535
Local loss @ local epoch 2: 0.037026699632406235
Local loss @ local epoch 3: 0.03426504507660866
Local loss @ local epoch 4: 0.03497488051652908
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.95 seconds!
[tester] 
SST2Metric: acc=0.8532110091743119, hinge=0.8729831516195875, ce=0.5536861146283293
Local test acc @ epoch 104: 0.8532
Client 1 execute local training on 8 samples...
Local loss @ local epoch 0: 0.00116959388833493
Local loss @ local epoch 1: 0.0007544942200183868
Local loss @ local epoch 2: 0.0005114962114021182
Local loss @ local epoch 3: 0.00037621925002895296
Local loss @ local epoch 4: 0.0003070682578254491
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.02 seconds!
[tester] 
SST2Metric: acc=0.8589449541284404, hinge=0.825207339650994, ce=0.5232324304644963
Local test acc @ epoch 104: 0.8589
Global evaluate on test data...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.42 seconds!
[tester] 
SST2Metric: acc=0.8073394495412844, hinge=0.9731356753121823, ce=0.5634012841470027
Global test acc @ epoch 104: 0.8073
Global epoch 105...
Client 9 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0034441302996128798
Local loss @ local epoch 1: 0.0030282584484666586
Local loss @ local epoch 2: 0.0027396557852625847
Local loss @ local epoch 3: 0.0024562531616538763
Local loss @ local epoch 4: 0.002196839079260826
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.93 seconds!
[tester] 
SST2Metric: acc=0.8623853211009175, hinge=0.8518463746396774, ce=0.5453344952916394
Local test acc @ epoch 105: 0.8624
Client 7 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0043163904920220375
Local loss @ local epoch 1: 0.003013490466400981
Local loss @ local epoch 2: 0.0023289646487683058
Local loss @ local epoch 3: 0.0020642809104174376
Local loss @ local epoch 4: 0.002063806401565671
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.95 seconds!
[tester] 
SST2Metric: acc=0.8486238532110092, hinge=0.872430592501929, ce=0.5566925554517058
Local test acc @ epoch 105: 0.8486
Client 3 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0424429252743721
Local loss @ local epoch 1: 0.03538094460964203
Local loss @ local epoch 2: 0.03378138318657875
Local loss @ local epoch 3: 0.03460036218166351
Local loss @ local epoch 4: 0.03482089936733246
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.03 seconds!
[tester] 
SST2Metric: acc=0.841743119266055, hinge=0.9492085365527266, ce=0.6133120961972179
Local test acc @ epoch 105: 0.8417
Client 0 execute local training on 8 samples...
Local loss @ local epoch 0: 0.007093420717865229
Local loss @ local epoch 1: 0.004516626708209515
Local loss @ local epoch 2: 0.002937381388619542
Local loss @ local epoch 3: 0.0019881161861121655
Local loss @ local epoch 4: 0.001428429502993822
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.08 seconds!
[tester] 
SST2Metric: acc=0.8589449541284404, hinge=0.8456977381618744, ce=0.5355494828804486
Local test acc @ epoch 105: 0.8589
Client 5 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0008134057279676199
Local loss @ local epoch 1: 0.000603740569204092
Local loss @ local epoch 2: 0.0005149058997631073
Local loss @ local epoch 3: 0.0005039522657170892
Local loss @ local epoch 4: 0.0005229557864367962
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.04 seconds!
[tester] 
SST2Metric: acc=0.8428899082568807, hinge=0.9118142750011672, ce=0.5862055583088018
Local test acc @ epoch 105: 0.8429
Client 4 execute local training on 8 samples...
Local loss @ local epoch 0: 0.12552474439144135
Local loss @ local epoch 1: 0.08789922297000885
Local loss @ local epoch 2: 0.061445996165275574
Local loss @ local epoch 3: 0.04472552239894867
Local loss @ local epoch 4: 0.03534962981939316
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.98 seconds!
[tester] 
SST2Metric: acc=0.8600917431192661, hinge=0.8316316290186085, ce=0.527323474243247
Local test acc @ epoch 105: 0.8601
Client 2 execute local training on 8 samples...
Local loss @ local epoch 0: 0.014818623661994934
Local loss @ local epoch 1: 0.009970543906092644
Local loss @ local epoch 2: 0.007191110868006945
Local loss @ local epoch 3: 0.005750604439526796
Local loss @ local epoch 4: 0.005141741596162319
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.04 seconds!
[tester] 
SST2Metric: acc=0.8451834862385321, hinge=0.9400548371699972, ce=0.6154713644627418
Local test acc @ epoch 105: 0.8452
Client 1 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0010962180094793439
Local loss @ local epoch 1: 0.0007346693892031908
Local loss @ local epoch 2: 0.0005204153130762279
Local loss @ local epoch 3: 0.00040307670133188367
Local loss @ local epoch 4: 0.00034770547063089907
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.04 seconds!
[tester] 
SST2Metric: acc=0.8600917431192661, hinge=0.8294179838731748, ce=0.5269179382830604
Local test acc @ epoch 105: 0.8601
Client 8 execute local training on 8 samples...
Local loss @ local epoch 0: 0.007310524582862854
Local loss @ local epoch 1: 0.004762026481330395
Local loss @ local epoch 2: 0.0032379652839154005
Local loss @ local epoch 3: 0.0023682769387960434
Local loss @ local epoch 4: 0.0019111577421426773
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.09 seconds!
[tester] 
SST2Metric: acc=0.8440366972477065, hinge=0.9506023683405798, ce=0.6240053862594164
Local test acc @ epoch 105: 0.844
Client 6 execute local training on 8 samples...
Local loss @ local epoch 0: 0.38742589950561523
Local loss @ local epoch 1: 0.30552640557289124
Local loss @ local epoch 2: 0.23464249074459076
Local loss @ local epoch 3: 0.17619195580482483
Local loss @ local epoch 4: 0.13064487278461456
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.06 seconds!
[tester] 
SST2Metric: acc=0.8635321100917431, hinge=0.8263290528583964, ce=0.5240089106820727
Local test acc @ epoch 105: 0.8635
Global evaluate on test data...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.3 seconds!
[tester] 
SST2Metric: acc=0.8142201834862385, hinge=0.9654897909372224, ce=0.5584988831563793
Global test acc @ epoch 105: 0.8142
Global epoch 106...
Client 4 execute local training on 8 samples...
Local loss @ local epoch 0: 0.03573249280452728
Local loss @ local epoch 1: 0.03024391643702984
Local loss @ local epoch 2: 0.02635347843170166
Local loss @ local epoch 3: 0.0230196975171566
Local loss @ local epoch 4: 0.02013963647186756
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.1 seconds!
[tester] 
SST2Metric: acc=0.8635321100917431, hinge=0.8190271925488743, ce=0.5191053178395282
Local test acc @ epoch 106: 0.8635
Client 2 execute local training on 8 samples...
Local loss @ local epoch 0: 0.01741739921271801
Local loss @ local epoch 1: 0.011469779536128044
Local loss @ local epoch 2: 0.007851283065974712
Local loss @ local epoch 3: 0.005745660979300737
Local loss @ local epoch 4: 0.004613338969647884
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.98 seconds!
[tester] 
SST2Metric: acc=0.841743119266055, hinge=0.9466240462086616, ce=0.6208801843315938
Local test acc @ epoch 106: 0.8417
Client 7 execute local training on 8 samples...
Local loss @ local epoch 0: 0.00835744570940733
Local loss @ local epoch 1: 0.005513333715498447
Local loss @ local epoch 2: 0.0038152963388711214
Local loss @ local epoch 3: 0.0028633649926632643
Local loss @ local epoch 4: 0.0023949919268488884
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.06 seconds!
[tester] 
SST2Metric: acc=0.8635321100917431, hinge=0.8195281355479441, ce=0.5194700352040946
Local test acc @ epoch 106: 0.8635
Client 8 execute local training on 8 samples...
Local loss @ local epoch 0: 0.010539427399635315
Local loss @ local epoch 1: 0.006798801943659782
Local loss @ local epoch 2: 0.004515068605542183
Local loss @ local epoch 3: 0.003156759077683091
Local loss @ local epoch 4: 0.002376638352870941
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.0 seconds!
[tester] 
SST2Metric: acc=0.8428899082568807, hinge=0.9299772523958748, ce=0.6084140631143423
Local test acc @ epoch 106: 0.8429
Client 6 execute local training on 8 samples...
Local loss @ local epoch 0: 0.31323498487472534
Local loss @ local epoch 1: 0.2410101443529129
Local loss @ local epoch 2: 0.18093112111091614
Local loss @ local epoch 3: 0.13365815579891205
Local loss @ local epoch 4: 0.0988055169582367
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.96 seconds!
[tester] 
SST2Metric: acc=0.8612385321100917, hinge=0.8177519869913749, ce=0.5181244880675526
Local test acc @ epoch 106: 0.8612
Client 1 execute local training on 8 samples...
Local loss @ local epoch 0: 0.00033331059967167675
Local loss @ local epoch 1: 0.0002830189187079668
Local loss @ local epoch 2: 0.0002611695963423699
Local loss @ local epoch 3: 0.0002472705964464694
Local loss @ local epoch 4: 0.00023157567193266004
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.07 seconds!
[tester] 
SST2Metric: acc=0.8555045871559633, hinge=0.8481749527224707, ce=0.5429519989267162
Local test acc @ epoch 106: 0.8555
Client 0 execute local training on 8 samples...
Local loss @ local epoch 0: 0.001437861705198884
Local loss @ local epoch 1: 0.0012778816744685173
Local loss @ local epoch 2: 0.0011568402405828238
Local loss @ local epoch 3: 0.0010568522848188877
Local loss @ local epoch 4: 0.000977269490249455
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.99 seconds!
[tester] 
SST2Metric: acc=0.8555045871559633, hinge=0.8468214339072552, ce=0.5431225814636574
Local test acc @ epoch 106: 0.8555
Client 9 execute local training on 8 samples...
Local loss @ local epoch 0: 0.003840293735265732
Local loss @ local epoch 1: 0.0032919105142354965
Local loss @ local epoch 2: 0.0029985918663442135
Local loss @ local epoch 3: 0.0027225676458328962
Local loss @ local epoch 4: 0.0024429243057966232
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.96 seconds!
[tester] 
SST2Metric: acc=0.8612385321100917, hinge=0.8490856861302613, ce=0.5430195335945736
Local test acc @ epoch 106: 0.8612
Client 5 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0014501974219456315
Local loss @ local epoch 1: 0.0009589088149368763
Local loss @ local epoch 2: 0.0006757215014658868
Local loss @ local epoch 3: 0.0005294844158925116
Local loss @ local epoch 4: 0.00047292897943407297
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.0 seconds!
[tester] 
SST2Metric: acc=0.8474770642201835, hinge=0.8956810069193534, ce=0.5754441912611733
Local test acc @ epoch 106: 0.8475
Client 3 execute local training on 8 samples...
Local loss @ local epoch 0: 0.036514051258563995
Local loss @ local epoch 1: 0.03297056257724762
Local loss @ local epoch 2: 0.03303259611129761
Local loss @ local epoch 3: 0.03270316496491432
Local loss @ local epoch 4: 0.031382154673337936
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.0 seconds!
[tester] 
SST2Metric: acc=0.838302752293578, hinge=0.9350928973167314, ce=0.6043203957336143
Local test acc @ epoch 106: 0.8383
Global evaluate on test data...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.4 seconds!
[tester] 
SST2Metric: acc=0.8084862385321101, hinge=0.9698081660434741, ce=0.5619858815025008
Global test acc @ epoch 106: 0.8085
Global epoch 107...
Client 8 execute local training on 8 samples...
Local loss @ local epoch 0: 0.006776466965675354
Local loss @ local epoch 1: 0.004412636626511812
Local loss @ local epoch 2: 0.002999479416757822
Local loss @ local epoch 3: 0.002193151041865349
Local loss @ local epoch 4: 0.0017693988047540188
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.99 seconds!
[tester] 
SST2Metric: acc=0.8440366972477065, hinge=0.9472446605699871, ce=0.6220673830788438
Local test acc @ epoch 107: 0.844
Client 5 execute local training on 8 samples...
Local loss @ local epoch 0: 0.007128675002604723
Local loss @ local epoch 1: 0.004477192182093859
Local loss @ local epoch 2: 0.0028674304485321045
Local loss @ local epoch 3: 0.0018921877490356565
Local loss @ local epoch 4: 0.0012973099946975708
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.98 seconds!
[tester] 
SST2Metric: acc=0.8589449541284404, hinge=0.8348167277530792, ce=0.5306675699630266
Local test acc @ epoch 107: 0.8589
Client 7 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0022800276055932045
Local loss @ local epoch 1: 0.0020264338236302137
Local loss @ local epoch 2: 0.002029137685894966
Local loss @ local epoch 3: 0.002020469633862376
Local loss @ local epoch 4: 0.0019407530780881643
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.97 seconds!
[tester] 
SST2Metric: acc=0.856651376146789, hinge=0.8444984567548157, ce=0.5354077084168339
Local test acc @ epoch 107: 0.8567
Client 9 execute local training on 8 samples...
Local loss @ local epoch 0: 0.004721532110124826
Local loss @ local epoch 1: 0.003336756955832243
Local loss @ local epoch 2: 0.002664327621459961
Local loss @ local epoch 3: 0.0024205707013607025
Local loss @ local epoch 4: 0.0023687314242124557
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.01 seconds!
[tester] 
SST2Metric: acc=0.8577981651376146, hinge=0.8632169579147199, ce=0.5584211607774455
Local test acc @ epoch 107: 0.8578
Client 6 execute local training on 8 samples...
Local loss @ local epoch 0: 0.26169058680534363
Local loss @ local epoch 1: 0.19991616904735565
Local loss @ local epoch 2: 0.1505642831325531
Local loss @ local epoch 3: 0.11384417861700058
Local loss @ local epoch 4: 0.08908218145370483
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.98 seconds!
[tester] 
SST2Metric: acc=0.8543577981651376, hinge=0.8573757677996924, ce=0.5452551681611371
Local test acc @ epoch 107: 0.8544
Client 2 execute local training on 8 samples...
Local loss @ local epoch 0: 0.03953105956315994
Local loss @ local epoch 1: 0.02559528686106205
Local loss @ local epoch 2: 0.0168900266289711
Local loss @ local epoch 3: 0.011572934687137604
Local loss @ local epoch 4: 0.00838612299412489
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.0 seconds!
[tester] 
SST2Metric: acc=0.8577981651376146, hinge=0.8617545210440224, ce=0.5570954249212156
Local test acc @ epoch 107: 0.8578
Client 1 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0005138469859957695
Local loss @ local epoch 1: 0.00040447316132485867
Local loss @ local epoch 2: 0.00035660708090290427
Local loss @ local epoch 3: 0.00033929612254723907
Local loss @ local epoch 4: 0.000326834328006953
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.02 seconds!
[tester] 
SST2Metric: acc=0.8635321100917431, hinge=0.8263424245315955, ce=0.5262271883433424
Local test acc @ epoch 107: 0.8635
Client 0 execute local training on 8 samples...
Local loss @ local epoch 0: 0.001418306608684361
Local loss @ local epoch 1: 0.0010775398695841432
Local loss @ local epoch 2: 0.0009473731624893844
Local loss @ local epoch 3: 0.0009380764095112681
Local loss @ local epoch 4: 0.0009621523786336184
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.06 seconds!
[tester] 
SST2Metric: acc=0.8555045871559633, hinge=0.8766863946247538, ce=0.5689191690070977
Local test acc @ epoch 107: 0.8555
Client 4 execute local training on 8 samples...
Local loss @ local epoch 0: 0.04421248286962509
Local loss @ local epoch 1: 0.0322093591094017
Local loss @ local epoch 2: 0.02596733719110489
Local loss @ local epoch 3: 0.023158371448516846
Local loss @ local epoch 4: 0.021816875785589218
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.97 seconds!
[tester] 
SST2Metric: acc=0.8623853211009175, hinge=0.8264347174025457, ce=0.524976498672179
Local test acc @ epoch 107: 0.8624
Client 3 execute local training on 8 samples...
Local loss @ local epoch 0: 0.04066644608974457
Local loss @ local epoch 1: 0.0346742682158947
Local loss @ local epoch 2: 0.03375338390469551
Local loss @ local epoch 3: 0.03437738120555878
Local loss @ local epoch 4: 0.03399626165628433
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.0 seconds!
[tester] 
SST2Metric: acc=0.8577981651376146, hinge=0.8582970586118348, ce=0.5444038305426264
Local test acc @ epoch 107: 0.8578
Global evaluate on test data...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.34 seconds!
[tester] 
SST2Metric: acc=0.8084862385321101, hinge=0.9734603601311325, ce=0.5657682381068365
Global test acc @ epoch 107: 0.8085
Global epoch 108...
Client 9 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0030021234415471554
Local loss @ local epoch 1: 0.002631744835525751
Local loss @ local epoch 2: 0.0023897727951407433
Local loss @ local epoch 3: 0.0021494561806321144
Local loss @ local epoch 4: 0.0019239732064306736
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.98 seconds!
[tester] 
SST2Metric: acc=0.8577981651376146, hinge=0.8593672807883779, ce=0.5535048809588178
Local test acc @ epoch 108: 0.8578
Client 8 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0022820057347416878
Local loss @ local epoch 1: 0.0016456800512969494
Local loss @ local epoch 2: 0.0013400521129369736
Local loss @ local epoch 3: 0.0012449098285287619
Local loss @ local epoch 4: 0.0012541606556624174
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.01 seconds!
[tester] 
SST2Metric: acc=0.8440366972477065, hinge=0.9571168212168807, ce=0.632728990466695
Local test acc @ epoch 108: 0.844
Client 7 execute local training on 8 samples...
Local loss @ local epoch 0: 0.018360672518610954
Local loss @ local epoch 1: 0.011931898072361946
Local loss @ local epoch 2: 0.007849237881600857
Local loss @ local epoch 3: 0.00530483340844512
Local loss @ local epoch 4: 0.0037370813079178333
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.0 seconds!
[tester] 
SST2Metric: acc=0.8589449541284404, hinge=0.8579358755175127, ce=0.550469364951698
Local test acc @ epoch 108: 0.8589
Client 5 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0010844286298379302
Local loss @ local epoch 1: 0.000738875474780798
Local loss @ local epoch 2: 0.0005489523755386472
Local loss @ local epoch 3: 0.00046433854731731117
Local loss @ local epoch 4: 0.0004477767797652632
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.07 seconds!
[tester] 
SST2Metric: acc=0.8463302752293578, hinge=0.9184032429795747, ce=0.5947061574381324
Local test acc @ epoch 108: 0.8463
Client 1 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0014443020336329937
Local loss @ local epoch 1: 0.0009524938650429249
Local loss @ local epoch 2: 0.0006757885566912591
Local loss @ local epoch 3: 0.0005337622133083642
Local loss @ local epoch 4: 0.000472461775643751
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.05 seconds!
[tester] 
SST2Metric: acc=0.8612385321100917, hinge=0.851849185354119, ce=0.5465064525598641
Local test acc @ epoch 108: 0.8612
Client 3 execute local training on 8 samples...
Local loss @ local epoch 0: 0.10474423319101334
Local loss @ local epoch 1: 0.07638345658779144
Local loss @ local epoch 2: 0.05580102652311325
Local loss @ local epoch 3: 0.042459674179553986
Local loss @ local epoch 4: 0.03521451726555824
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.05 seconds!
[tester] 
SST2Metric: acc=0.8463302752293578, hinge=0.8981540148411322, ce=0.5779331848132125
Local test acc @ epoch 108: 0.8463
Client 0 execute local training on 8 samples...
Local loss @ local epoch 0: 0.003998733591288328
Local loss @ local epoch 1: 0.002565467730164528
Local loss @ local epoch 2: 0.0017075344221666455
Local loss @ local epoch 3: 0.0012140036560595036
Local loss @ local epoch 4: 0.0009492181707173586
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.0 seconds!
[tester] 
SST2Metric: acc=0.8600917431192661, hinge=0.8499159767813639, ce=0.5442083525517315
Local test acc @ epoch 108: 0.8601
Client 6 execute local training on 8 samples...
Local loss @ local epoch 0: 0.16678769886493683
Local loss @ local epoch 1: 0.12416631728410721
Local loss @ local epoch 2: 0.09425679594278336
Local loss @ local epoch 3: 0.07603027671575546
Local loss @ local epoch 4: 0.06741725653409958
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.96 seconds!
[tester] 
SST2Metric: acc=0.8463302752293578, hinge=0.8891100328449809, ce=0.5706427960358603
Local test acc @ epoch 108: 0.8463
Client 2 execute local training on 8 samples...
Local loss @ local epoch 0: 0.06141405552625656
Local loss @ local epoch 1: 0.03998638316988945
Local loss @ local epoch 2: 0.02626628428697586
Local loss @ local epoch 3: 0.017646340653300285
Local loss @ local epoch 4: 0.012268138118088245
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.01 seconds!
[tester] 
SST2Metric: acc=0.8555045871559633, hinge=0.8475550180728283, ce=0.5433577202853785
Local test acc @ epoch 108: 0.8555
Client 4 execute local training on 8 samples...
Local loss @ local epoch 0: 0.03101482056081295
Local loss @ local epoch 1: 0.026470709592103958
Local loss @ local epoch 2: 0.02331402152776718
Local loss @ local epoch 3: 0.020567381754517555
Local loss @ local epoch 4: 0.018172526732087135
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.97 seconds!
[tester] 
SST2Metric: acc=0.8612385321100917, hinge=0.8253586098141626, ce=0.5262155736966909
Local test acc @ epoch 108: 0.8612
Global evaluate on test data...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.6 seconds!
[tester] 
SST2Metric: acc=0.8084862385321101, hinge=0.9772917804641461, ce=0.5701934254638543
Global test acc @ epoch 108: 0.8085
Global epoch 109...
Client 2 execute local training on 8 samples...
Local loss @ local epoch 0: 0.015021676197648048
Local loss @ local epoch 1: 0.010083187371492386
Local loss @ local epoch 2: 0.007261993829160929
Local loss @ local epoch 3: 0.00579533725976944
Local loss @ local epoch 4: 0.005154639016836882
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.07 seconds!
[tester] 
SST2Metric: acc=0.8451834862385321, hinge=0.9285304853948978, ce=0.6098729271470802
Local test acc @ epoch 109: 0.8452
Client 0 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0015427523758262396
Local loss @ local epoch 1: 0.001098652952350676
Local loss @ local epoch 2: 0.0008764490485191345
Local loss @ local epoch 3: 0.000806384370662272
Local loss @ local epoch 4: 0.000826136558316648
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.11 seconds!
[tester] 
SST2Metric: acc=0.8589449541284404, hinge=0.8464442991882289, ce=0.5405434046872227
Local test acc @ epoch 109: 0.8589
Client 5 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0009710373706184328
Local loss @ local epoch 1: 0.0006672494928352535
Local loss @ local epoch 2: 0.0005068727186881006
Local loss @ local epoch 3: 0.0004436270392034203
Local loss @ local epoch 4: 0.0004412995476741344
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.03 seconds!
[tester] 
SST2Metric: acc=0.8463302752293578, hinge=0.9047905469159467, ce=0.5831620241755645
Local test acc @ epoch 109: 0.8463
Client 6 execute local training on 8 samples...
Local loss @ local epoch 0: 0.07189732044935226
Local loss @ local epoch 1: 0.06589876115322113
Local loss @ local epoch 2: 0.060564249753952026
Local loss @ local epoch 3: 0.05558403953909874
Local loss @ local epoch 4: 0.05107772350311279
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.04 seconds!
[tester] 
SST2Metric: acc=0.8555045871559633, hinge=0.8528167964941865, ce=0.5421729004734149
Local test acc @ epoch 109: 0.8555
Client 8 execute local training on 8 samples...
Local loss @ local epoch 0: 0.033166639506816864
Local loss @ local epoch 1: 0.021428311243653297
Local loss @ local epoch 2: 0.013952900655567646
Local loss @ local epoch 3: 0.009261700324714184
Local loss @ local epoch 4: 0.006321483291685581
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.13 seconds!
[tester] 
SST2Metric: acc=0.8555045871559633, hinge=0.8524823563361387, ce=0.5495451841215815
Local test acc @ epoch 109: 0.8555
Client 3 execute local training on 8 samples...
Local loss @ local epoch 0: 0.08201997727155685
Local loss @ local epoch 1: 0.059526488184928894
Local loss @ local epoch 2: 0.04468560591340065
Local loss @ local epoch 3: 0.036474332213401794
Local loss @ local epoch 4: 0.03343415632843971
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.03 seconds!
[tester] 
SST2Metric: acc=0.856651376146789, hinge=0.8533314202630192, ce=0.5422173856315416
Local test acc @ epoch 109: 0.8567
Client 4 execute local training on 8 samples...
Local loss @ local epoch 0: 0.041122812777757645
Local loss @ local epoch 1: 0.028047971427440643
Local loss @ local epoch 2: 0.020833292976021767
Local loss @ local epoch 3: 0.01745923049747944
Local loss @ local epoch 4: 0.016241617500782013
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.06 seconds!
[tester] 
SST2Metric: acc=0.8589449541284404, hinge=0.8398781522698359, ce=0.5379295807226065
Local test acc @ epoch 109: 0.8589
Client 7 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0023326799273490906
Local loss @ local epoch 1: 0.00218235794454813
Local loss @ local epoch 2: 0.002053318079560995
Local loss @ local epoch 3: 0.0019360119476914406
Local loss @ local epoch 4: 0.0018431481439620256
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.98 seconds!
[tester] 
SST2Metric: acc=0.8646788990825688, hinge=0.832009828008643, ce=0.5320539613072269
Local test acc @ epoch 109: 0.8647
Client 9 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0033811500761657953
Local loss @ local epoch 1: 0.0028509488329291344
Local loss @ local epoch 2: 0.0026178068947046995
Local loss @ local epoch 3: 0.0024138360749930143
Local loss @ local epoch 4: 0.0021856073290109634
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.12 seconds!
[tester] 
SST2Metric: acc=0.8577981651376146, hinge=0.859650703471735, ce=0.5568289415649974
Local test acc @ epoch 109: 0.8578
Client 1 execute local training on 8 samples...
Local loss @ local epoch 0: 0.00043521594488993287
Local loss @ local epoch 1: 0.00037126560346223414
Local loss @ local epoch 2: 0.00034303971915505826
Local loss @ local epoch 3: 0.00032175425440073013
Local loss @ local epoch 4: 0.00029686212656088173
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.02 seconds!
[tester] 
SST2Metric: acc=0.8600917431192661, hinge=0.8381976634263992, ce=0.5378734834640129
Local test acc @ epoch 109: 0.8601
Global evaluate on test data...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.5 seconds!
[tester] 
SST2Metric: acc=0.8130733944954128, hinge=0.9547894435738205, ce=0.5531708748601035
Global test acc @ epoch 109: 0.8131
Global epoch 110...
Client 2 execute local training on 8 samples...
Local loss @ local epoch 0: 0.014751608483493328
Local loss @ local epoch 1: 0.009769503027200699
Local loss @ local epoch 2: 0.0067943730391561985
Local loss @ local epoch 3: 0.005121463909745216
Local loss @ local epoch 4: 0.004281990695744753
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.01 seconds!
[tester] 
SST2Metric: acc=0.8451834862385321, hinge=0.9454515737677933, ce=0.6230711256506465
Local test acc @ epoch 110: 0.8452
Client 9 execute local training on 8 samples...
Local loss @ local epoch 0: 0.005945843178778887
Local loss @ local epoch 1: 0.0039113303646445274
Local loss @ local epoch 2: 0.002829603850841522
Local loss @ local epoch 3: 0.0023340315092355013
Local loss @ local epoch 4: 0.0021716870833188295
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.94 seconds!
[tester] 
SST2Metric: acc=0.8600917431192661, hinge=0.8500791942581124, ce=0.5447413546453473
Local test acc @ epoch 110: 0.8601
Client 8 execute local training on 8 samples...
Local loss @ local epoch 0: 0.004234811756759882
Local loss @ local epoch 1: 0.0027860880363732576
Local loss @ local epoch 2: 0.0019372210372239351
Local loss @ local epoch 3: 0.0014741693157702684
Local loss @ local epoch 4: 0.0012558912858366966
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.04 seconds!
[tester] 
SST2Metric: acc=0.8474770642201835, hinge=0.9519886842014593, ce=0.6293976458000877
Local test acc @ epoch 110: 0.8475
Client 7 execute local training on 8 samples...
Local loss @ local epoch 0: 0.011706140823662281
Local loss @ local epoch 1: 0.007605713792145252
Local loss @ local epoch 2: 0.005052688531577587
Local loss @ local epoch 3: 0.0035002962686121464
Local loss @ local epoch 4: 0.0025833938270807266
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.0 seconds!
[tester] 
SST2Metric: acc=0.8577981651376146, hinge=0.8520863247573922, ce=0.545906773789726
Local test acc @ epoch 110: 0.8578
Client 0 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0008269789395853877
Local loss @ local epoch 1: 0.0007391143590211868
Local loss @ local epoch 2: 0.0007376656867563725
Local loss @ local epoch 3: 0.0007288575870916247
Local loss @ local epoch 4: 0.0006962466286495328
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.05 seconds!
[tester] 
SST2Metric: acc=0.856651376146789, hinge=0.8656586087351545, ce=0.5587524906434429
Local test acc @ epoch 110: 0.8567
Client 5 execute local training on 8 samples...
Local loss @ local epoch 0: 0.001464297529309988
Local loss @ local epoch 1: 0.0009566498338244855
Local loss @ local epoch 2: 0.0006592812715098262
Local loss @ local epoch 3: 0.0004982068785466254
Local loss @ local epoch 4: 0.00042559168650768697
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.08 seconds!
[tester] 
SST2Metric: acc=0.8497706422018348, hinge=0.8998503125862244, ce=0.5814132122888973
Local test acc @ epoch 110: 0.8498
Client 1 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0010136201744899154
Local loss @ local epoch 1: 0.0006816925597377121
Local loss @ local epoch 2: 0.000498731154948473
Local loss @ local epoch 3: 0.00040969811379909515
Local loss @ local epoch 4: 0.0003757968661375344
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.01 seconds!
[tester] 
SST2Metric: acc=0.8555045871559633, hinge=0.8591370816351077, ce=0.5533940781603691
Local test acc @ epoch 110: 0.8555
Client 3 execute local training on 8 samples...
Local loss @ local epoch 0: 0.09943968802690506
Local loss @ local epoch 1: 0.07219997048377991
Local loss @ local epoch 2: 0.052704475820064545
Local loss @ local epoch 3: 0.040289606899023056
Local loss @ local epoch 4: 0.03374214097857475
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.96 seconds!
[tester] 
SST2Metric: acc=0.8497706422018348, hinge=0.8854615431313121, ce=0.5686817646850063
Local test acc @ epoch 110: 0.8498
Client 6 execute local training on 8 samples...
Local loss @ local epoch 0: 0.059412792325019836
Local loss @ local epoch 1: 0.05440627783536911
Local loss @ local epoch 2: 0.04987229034304619
Local loss @ local epoch 3: 0.045735813677310944
Local loss @ local epoch 4: 0.041987523436546326
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.93 seconds!
[tester] 
SST2Metric: acc=0.8555045871559633, hinge=0.8443739776491025, ce=0.5373307375545473
Local test acc @ epoch 110: 0.8555
Client 4 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0339030958712101
Local loss @ local epoch 1: 0.023264428600668907
Local loss @ local epoch 2: 0.017573414370417595
Local loss @ local epoch 3: 0.015063272789120674
Local loss @ local epoch 4: 0.014285625889897346
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.01 seconds!
[tester] 
SST2Metric: acc=0.8577981651376146, hinge=0.8518892182122677, ce=0.5481329861010528
Local test acc @ epoch 110: 0.8578
Global evaluate on test data...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.46 seconds!
[tester] 
SST2Metric: acc=0.8096330275229358, hinge=0.9853703108949399, ce=0.5775923655216727
Global test acc @ epoch 110: 0.8096
Global epoch 111...
Client 9 execute local training on 8 samples...
Local loss @ local epoch 0: 0.002463256474584341
Local loss @ local epoch 1: 0.0021523330360651016
Local loss @ local epoch 2: 0.0019799345172941685
Local loss @ local epoch 3: 0.001802851795218885
Local loss @ local epoch 4: 0.0016251062043011189
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.05 seconds!
[tester] 
SST2Metric: acc=0.8577981651376146, hinge=0.8726752373056674, ce=0.5661990250009027
Local test acc @ epoch 111: 0.8578
Client 7 execute local training on 8 samples...
Local loss @ local epoch 0: 0.003608205821365118
Local loss @ local epoch 1: 0.0025261526461690664
Local loss @ local epoch 2: 0.0019605981651693583
Local loss @ local epoch 3: 0.001745493384078145
Local loss @ local epoch 4: 0.0017495218198746443
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.96 seconds!
[tester] 
SST2Metric: acc=0.8486238532110092, hinge=0.8805708700637205, ce=0.5672633925344958
Local test acc @ epoch 111: 0.8486
Client 1 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0007414451101794839
Local loss @ local epoch 1: 0.0005202757893130183
Local loss @ local epoch 2: 0.0004056165344081819
Local loss @ local epoch 3: 0.00035709727671928704
Local loss @ local epoch 4: 0.0003427449846640229
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.0 seconds!
[tester] 
SST2Metric: acc=0.8555045871559633, hinge=0.8622016616917532, ce=0.5585253555083342
Local test acc @ epoch 111: 0.8555
Client 5 execute local training on 8 samples...
Local loss @ local epoch 0: 0.001503245672211051
Local loss @ local epoch 1: 0.0009748466545715928
Local loss @ local epoch 2: 0.0006638498161919415
Local loss @ local epoch 3: 0.0004921749350614846
Local loss @ local epoch 4: 0.000410045642638579
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.01 seconds!
[tester] 
SST2Metric: acc=0.8532110091743119, hinge=0.8822267022701579, ce=0.5685784144496862
Local test acc @ epoch 111: 0.8532
Client 4 execute local training on 8 samples...
Local loss @ local epoch 0: 0.07561636716127396
Local loss @ local epoch 1: 0.05177392438054085
Local loss @ local epoch 2: 0.037044815719127655
Local loss @ local epoch 3: 0.029049307107925415
Local loss @ local epoch 4: 0.025324318557977676
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.98 seconds!
[tester] 
SST2Metric: acc=0.8555045871559633, hinge=0.8483063324328957, ce=0.546299287758836
Local test acc @ epoch 111: 0.8555
Client 8 execute local training on 8 samples...
Local loss @ local epoch 0: 0.005254202522337437
Local loss @ local epoch 1: 0.0034278975799679756
Local loss @ local epoch 2: 0.002340688370168209
Local loss @ local epoch 3: 0.0017257011495530605
Local loss @ local epoch 4: 0.0014087792951613665
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.11 seconds!
[tester] 
SST2Metric: acc=0.8451834862385321, hinge=0.9738932756382391, ce=0.6473160774599619
Local test acc @ epoch 111: 0.8452
Client 0 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0016559493960812688
Local loss @ local epoch 1: 0.0011394999455660582
Local loss @ local epoch 2: 0.0008570316131226718
Local loss @ local epoch 3: 0.0007333268877118826
Local loss @ local epoch 4: 0.000712265376932919
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.0 seconds!
[tester] 
SST2Metric: acc=0.8577981651376146, hinge=0.8549125064677054, ce=0.5508091124722525
Local test acc @ epoch 111: 0.8578
Client 2 execute local training on 8 samples...
Local loss @ local epoch 0: 0.00978199951350689
Local loss @ local epoch 1: 0.006735701113939285
Local loss @ local epoch 2: 0.0050597647204995155
Local loss @ local epoch 3: 0.0042690010741353035
Local loss @ local epoch 4: 0.004005649592727423
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.96 seconds!
[tester] 
SST2Metric: acc=0.8451834862385321, hinge=0.9809136793974342, ce=0.652373005473896
Local test acc @ epoch 111: 0.8452
Client 3 execute local training on 8 samples...
Local loss @ local epoch 0: 0.23757465183734894
Local loss @ local epoch 1: 0.18772383034229279
Local loss @ local epoch 2: 0.14417248964309692
Local loss @ local epoch 3: 0.10789792984724045
Local loss @ local epoch 4: 0.07948686927556992
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.99 seconds!
[tester] 
SST2Metric: acc=0.8577981651376146, hinge=0.8465748366412766, ce=0.543886729603267
Local test acc @ epoch 111: 0.8578
Client 6 execute local training on 8 samples...
Local loss @ local epoch 0: 0.10289499163627625
Local loss @ local epoch 1: 0.0752430409193039
Local loss @ local epoch 2: 0.05828123912215233
Local loss @ local epoch 3: 0.050005920231342316
Local loss @ local epoch 4: 0.047764405608177185
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.11 seconds!
[tester] 
SST2Metric: acc=0.856651376146789, hinge=0.872677438712995, ce=0.5588731379104017
Local test acc @ epoch 111: 0.8567
Global evaluate on test data...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.58 seconds!
[tester] 
SST2Metric: acc=0.8084862385321101, hinge=0.9955757057994877, ce=0.5861623847212027
Global test acc @ epoch 111: 0.8085
Global epoch 112...
Client 8 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0044991616159677505
Local loss @ local epoch 1: 0.002946268767118454
Local loss @ local epoch 2: 0.0020285898353904486
Local loss @ local epoch 3: 0.001517998636700213
Local loss @ local epoch 4: 0.0012651849538087845
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.96 seconds!
[tester] 
SST2Metric: acc=0.8474770642201835, hinge=0.9642450668669622, ce=0.6396182522284767
Local test acc @ epoch 112: 0.8475
Client 7 execute local training on 8 samples...
Local loss @ local epoch 0: 0.009897381998598576
Local loss @ local epoch 1: 0.0064407829195261
Local loss @ local epoch 2: 0.004303949419409037
Local loss @ local epoch 3: 0.003018907504156232
Local loss @ local epoch 4: 0.0022763304878026247
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.02 seconds!
[tester] 
SST2Metric: acc=0.8612385321100917, hinge=0.8544563508908684, ce=0.5493442964622081
Local test acc @ epoch 112: 0.8612
Client 0 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0007841803599148989
Local loss @ local epoch 1: 0.0006798139656893909
Local loss @ local epoch 2: 0.0006709144217893481
Local loss @ local epoch 3: 0.0006736576906405389
Local loss @ local epoch 4: 0.000652258051559329
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.93 seconds!
[tester] 
SST2Metric: acc=0.856651376146789, hinge=0.8741191420533242, ce=0.568415677478914
Local test acc @ epoch 112: 0.8567
Client 1 execute local training on 8 samples...
Local loss @ local epoch 0: 0.00044069666182622313
Local loss @ local epoch 1: 0.0003648832207545638
Local loss @ local epoch 2: 0.0003326223522890359
Local loss @ local epoch 3: 0.0003145259106531739
Local loss @ local epoch 4: 0.0002942323626484722
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.98 seconds!
[tester] 
SST2Metric: acc=0.8612385321100917, hinge=0.8503243647037296, ce=0.5463623228393989
Local test acc @ epoch 112: 0.8612
Client 5 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0008675376302562654
Local loss @ local epoch 1: 0.000593583972658962
Local loss @ local epoch 2: 0.0004495706525631249
Local loss @ local epoch 3: 0.00039287813706323504
Local loss @ local epoch 4: 0.0003907863865606487
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.94 seconds!
[tester] 
SST2Metric: acc=0.8463302752293578, hinge=0.9051513633596788, ce=0.5860370900759627
Local test acc @ epoch 112: 0.8463
Client 9 execute local training on 8 samples...
Local loss @ local epoch 0: 0.010140231810510159
Local loss @ local epoch 1: 0.006466434802860022
Local loss @ local epoch 2: 0.004264567978680134
Local loss @ local epoch 3: 0.002986478852108121
Local loss @ local epoch 4: 0.002280608518049121
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.95 seconds!
[tester] 
SST2Metric: acc=0.8589449541284404, hinge=0.8588012105554615, ce=0.5541317713708048
Local test acc @ epoch 112: 0.8589
Client 2 execute local training on 8 samples...
Local loss @ local epoch 0: 0.01029772125184536
Local loss @ local epoch 1: 0.006980123929679394
Local loss @ local epoch 2: 0.005097602028399706
Local loss @ local epoch 3: 0.004144936800003052
Local loss @ local epoch 4: 0.0037670349702239037
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.13 seconds!
[tester] 
SST2Metric: acc=0.8451834862385321, hinge=0.981797986347741, ce=0.6537018754834124
Local test acc @ epoch 112: 0.8452
Client 4 execute local training on 8 samples...
Local loss @ local epoch 0: 0.09304752945899963
Local loss @ local epoch 1: 0.06348644196987152
Local loss @ local epoch 2: 0.04460933059453964
Local loss @ local epoch 3: 0.033524785190820694
Local loss @ local epoch 4: 0.027695413678884506
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.07 seconds!
[tester] 
SST2Metric: acc=0.856651376146789, hinge=0.8545686176063818, ce=0.5514138134000417
Local test acc @ epoch 112: 0.8567
Client 3 execute local training on 8 samples...
Local loss @ local epoch 0: 0.06785500049591064
Local loss @ local epoch 1: 0.049864448606967926
Local loss @ local epoch 2: 0.03905273228883743
Local loss @ local epoch 3: 0.034215424209833145
Local loss @ local epoch 4: 0.033650048077106476
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.98 seconds!
[tester] 
SST2Metric: acc=0.8543577981651376, hinge=0.8815677446236304, ce=0.5665816220405743
Local test acc @ epoch 112: 0.8544
Client 6 execute local training on 8 samples...
Local loss @ local epoch 0: 0.050176143646240234
Local loss @ local epoch 1: 0.0460929311811924
Local loss @ local epoch 2: 0.043133966624736786
Local loss @ local epoch 3: 0.039908137172460556
Local loss @ local epoch 4: 0.03683491051197052
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.94 seconds!
[tester] 
SST2Metric: acc=0.8589449541284404, hinge=0.8438055410024223, ce=0.539119784772175
Local test acc @ epoch 112: 0.8589
Global evaluate on test data...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.37 seconds!
[tester] 
SST2Metric: acc=0.8073394495412844, hinge=0.9828681260894198, ce=0.5773233045180918
Global test acc @ epoch 112: 0.8073
Global epoch 113...
Client 7 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0018264846876263618
Local loss @ local epoch 1: 0.0017344558145850897
Local loss @ local epoch 2: 0.0017042645486071706
Local loss @ local epoch 3: 0.0016305366298183799
Local loss @ local epoch 4: 0.0015583165222778916
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.03 seconds!
[tester] 
SST2Metric: acc=0.8600917431192661, hinge=0.8548863639525317, ce=0.5501282486662487
Local test acc @ epoch 113: 0.8601
Client 2 execute local training on 8 samples...
Local loss @ local epoch 0: 0.016443811357021332
Local loss @ local epoch 1: 0.010769188404083252
Local loss @ local epoch 2: 0.007361527532339096
Local loss @ local epoch 3: 0.00540508609265089
Local loss @ local epoch 4: 0.004362801089882851
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.98 seconds!
[tester] 
SST2Metric: acc=0.8451834862385321, hinge=0.9334521170603026, ce=0.6169760696620786
Local test acc @ epoch 113: 0.8452
Client 9 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0031570896971970797
Local loss @ local epoch 1: 0.0021888844203203917
Local loss @ local epoch 2: 0.0017312357667833567
Local loss @ local epoch 3: 0.0015737450448796153
Local loss @ local epoch 4: 0.0015528327785432339
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.0 seconds!
[tester] 
SST2Metric: acc=0.8600917431192661, hinge=0.8690465467238645, ce=0.5620691242947942
Local test acc @ epoch 113: 0.8601
Client 6 execute local training on 8 samples...
Local loss @ local epoch 0: 0.16854062676429749
Local loss @ local epoch 1: 0.1246413141489029
Local loss @ local epoch 2: 0.0936020016670227
Local loss @ local epoch 3: 0.0743165835738182
Local loss @ local epoch 4: 0.06472838670015335
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.06 seconds!
[tester] 
SST2Metric: acc=0.8463302752293578, hinge=0.9126338355858391, ce=0.593319306928673
Local test acc @ epoch 113: 0.8463
Client 0 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0030249785631895065
Local loss @ local epoch 1: 0.0019362961174920201
Local loss @ local epoch 2: 0.0012832884676754475
Local loss @ local epoch 3: 0.0009049665532074869
Local loss @ local epoch 4: 0.0006981128244660795
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.08 seconds!
[tester] 
SST2Metric: acc=0.8589449541284404, hinge=0.8665844394799767, ce=0.5599019687954943
Local test acc @ epoch 113: 0.8589
Client 3 execute local training on 8 samples...
Local loss @ local epoch 0: 0.09374312311410904
Local loss @ local epoch 1: 0.0680866539478302
Local loss @ local epoch 2: 0.0501161627471447
Local loss @ local epoch 3: 0.03909539058804512
Local loss @ local epoch 4: 0.03376156464219093
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.97 seconds!
[tester] 
SST2Metric: acc=0.8463302752293578, hinge=0.9108256011381062, ce=0.5914067895986874
Local test acc @ epoch 113: 0.8463
Client 8 execute local training on 8 samples...
Local loss @ local epoch 0: 0.02597159892320633
Local loss @ local epoch 1: 0.0166989304125309
Local loss @ local epoch 2: 0.01083892397582531
Local loss @ local epoch 3: 0.007178287021815777
Local loss @ local epoch 4: 0.004887908231467009
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.95 seconds!
[tester] 
SST2Metric: acc=0.8589449541284404, hinge=0.8671140711788737, ce=0.5601311607511689
Local test acc @ epoch 113: 0.8589
Client 5 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0009835334494709969
Local loss @ local epoch 1: 0.0006618459592573345
Local loss @ local epoch 2: 0.0004820901667699218
Local loss @ local epoch 3: 0.0003975942963734269
Local loss @ local epoch 4: 0.00037545710802078247
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.0 seconds!
[tester] 
SST2Metric: acc=0.8463302752293578, hinge=0.9184728206024257, ce=0.5979404333618965
Local test acc @ epoch 113: 0.8463
Client 1 execute local training on 8 samples...
Local loss @ local epoch 0: 0.00110969552770257
Local loss @ local epoch 1: 0.0007233154028654099
Local loss @ local epoch 2: 0.0005002394318580627
Local loss @ local epoch 3: 0.00037987148971296847
Local loss @ local epoch 4: 0.00032215117244049907
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.03 seconds!
[tester] 
SST2Metric: acc=0.8577981651376146, hinge=0.8640284360548772, ce=0.5585021528769434
Local test acc @ epoch 113: 0.8578
Client 4 execute local training on 8 samples...
Local loss @ local epoch 0: 0.03518759086728096
Local loss @ local epoch 1: 0.029964648187160492
Local loss @ local epoch 2: 0.025786323472857475
Local loss @ local epoch 3: 0.022303346544504166
Local loss @ local epoch 4: 0.019435536116361618
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.96 seconds!
[tester] 
SST2Metric: acc=0.8577981651376146, hinge=0.8450826205518267, ce=0.5452324400455403
Local test acc @ epoch 113: 0.8578
Global evaluate on test data...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.16 seconds!
[tester] 
SST2Metric: acc=0.8084862385321101, hinge=0.9732347290723695, ce=0.5714176566972303
Global test acc @ epoch 113: 0.8085
Global epoch 114...
Client 9 execute local training on 8 samples...
Local loss @ local epoch 0: 0.002406666288152337
Local loss @ local epoch 1: 0.0018208685796707869
Local loss @ local epoch 2: 0.001586527330800891
Local loss @ local epoch 3: 0.0015247665578499436
Local loss @ local epoch 4: 0.0014901041286066175
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.98 seconds!
[tester] 
SST2Metric: acc=0.8532110091743119, hinge=0.8936494841761545, ce=0.5875446399494391
Local test acc @ epoch 114: 0.8532
Client 4 execute local training on 8 samples...
Local loss @ local epoch 0: 0.054528385400772095
Local loss @ local epoch 1: 0.04337714985013008
Local loss @ local epoch 2: 0.037506915628910065
Local loss @ local epoch 3: 0.0336628220975399
Local loss @ local epoch 4: 0.03021937794983387
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.06 seconds!
[tester] 
SST2Metric: acc=0.8612385321100917, hinge=0.855087165165385, ce=0.5511910571661188
Local test acc @ epoch 114: 0.8612
Client 8 execute local training on 8 samples...
Local loss @ local epoch 0: 0.005981962662190199
Local loss @ local epoch 1: 0.0038580785039812326
Local loss @ local epoch 2: 0.0025704505387693644
Local loss @ local epoch 3: 0.0018120736349374056
Local loss @ local epoch 4: 0.001384455943480134
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.0 seconds!
[tester] 
SST2Metric: acc=0.8474770642201835, hinge=0.945648525696282, ce=0.6275886009236207
Local test acc @ epoch 114: 0.8475
Client 2 execute local training on 8 samples...
Local loss @ local epoch 0: 0.004047338850796223
Local loss @ local epoch 1: 0.0036284958478063345
Local loss @ local epoch 2: 0.0033038791734725237
Local loss @ local epoch 3: 0.0030340002849698067
Local loss @ local epoch 4: 0.00281125889159739
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.91 seconds!
[tester] 
SST2Metric: acc=0.841743119266055, hinge=1.0270995695930007, ce=0.6902766371321536
Local test acc @ epoch 114: 0.8417
Client 5 execute local training on 8 samples...
Local loss @ local epoch 0: 0.010924229398369789
Local loss @ local epoch 1: 0.0067880600690841675
Local loss @ local epoch 2: 0.004297444596886635
Local loss @ local epoch 3: 0.002797770546749234
Local loss @ local epoch 4: 0.0018833217909559608
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.88 seconds!
[tester] 
SST2Metric: acc=0.8577981651376146, hinge=0.8627631053191807, ce=0.5601523334228267
Local test acc @ epoch 114: 0.8578
Client 3 execute local training on 8 samples...
Local loss @ local epoch 0: 0.07205027341842651
Local loss @ local epoch 1: 0.05271504819393158
Local loss @ local epoch 2: 0.040739431977272034
Local loss @ local epoch 3: 0.03496778756380081
Local loss @ local epoch 4: 0.033773958683013916
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.95 seconds!
[tester] 
SST2Metric: acc=0.8577981651376146, hinge=0.8703183374547083, ce=0.5595199766119199
Local test acc @ epoch 114: 0.8578
Client 1 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0006396393873728812
Local loss @ local epoch 1: 0.0004192613414488733
Local loss @ local epoch 2: 0.0002902935666497797
Local loss @ local epoch 3: 0.00021975321578793228
Local loss @ local epoch 4: 0.00018540800374466926
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.97 seconds!
[tester] 
SST2Metric: acc=0.856651376146789, hinge=0.863131278015058, ce=0.560136599251931
Local test acc @ epoch 114: 0.8567
Client 7 execute local training on 8 samples...
Local loss @ local epoch 0: 0.001931578735820949
Local loss @ local epoch 1: 0.0017668143846094608
Local loss @ local epoch 2: 0.0017053590854629874
Local loss @ local epoch 3: 0.0016217547236010432
Local loss @ local epoch 4: 0.0015301910461857915
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.95 seconds!
[tester] 
SST2Metric: acc=0.8623853211009175, hinge=0.8509951310420255, ce=0.5492087471880416
Local test acc @ epoch 114: 0.8624
Client 6 execute local training on 8 samples...
Local loss @ local epoch 0: 0.08253371715545654
Local loss @ local epoch 1: 0.06228528544306755
Local loss @ local epoch 2: 0.05160975456237793
Local loss @ local epoch 3: 0.04795737937092781
Local loss @ local epoch 4: 0.047911252826452255
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.95 seconds!
[tester] 
SST2Metric: acc=0.856651376146789, hinge=0.8799185789779785, ce=0.567264087428232
Local test acc @ epoch 114: 0.8567
Client 0 execute local training on 8 samples...
Local loss @ local epoch 0: 0.004549192264676094
Local loss @ local epoch 1: 0.002900846069678664
Local loss @ local epoch 2: 0.001902252435684204
Local loss @ local epoch 3: 0.0013118120841681957
Local loss @ local epoch 4: 0.0009734336053952575
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.05 seconds!
[tester] 
SST2Metric: acc=0.8577981651376146, hinge=0.8554329881700901, ce=0.5542994047521236
Local test acc @ epoch 114: 0.8578
Global evaluate on test data...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.49 seconds!
[tester] 
SST2Metric: acc=0.8107798165137615, hinge=0.9907757355532515, ce=0.5851417108622611
Global test acc @ epoch 114: 0.8108
Global epoch 115...
Client 6 execute local training on 8 samples...
Local loss @ local epoch 0: 0.1112942323088646
Local loss @ local epoch 1: 0.0804937481880188
Local loss @ local epoch 2: 0.06070658937096596
Local loss @ local epoch 3: 0.050028614699840546
Local loss @ local epoch 4: 0.04606177657842636
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.96 seconds!
[tester] 
SST2Metric: acc=0.856651376146789, hinge=0.871903147719322, ce=0.5608988729646801
Local test acc @ epoch 115: 0.8567
Client 9 execute local training on 8 samples...
Local loss @ local epoch 0: 0.009102895855903625
Local loss @ local epoch 1: 0.005862835794687271
Local loss @ local epoch 2: 0.003964046481996775
Local loss @ local epoch 3: 0.0029122198466211557
Local loss @ local epoch 4: 0.002385540632531047
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.99 seconds!
[tester] 
SST2Metric: acc=0.856651376146789, hinge=0.863287395977099, ce=0.5610098234469117
Local test acc @ epoch 115: 0.8567
Client 7 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0017951722256839275
Local loss @ local epoch 1: 0.001605133875273168
Local loss @ local epoch 2: 0.0015830212505534291
Local loss @ local epoch 3: 0.0015466922195628285
Local loss @ local epoch 4: 0.0014727022498846054
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.98 seconds!
[tester] 
SST2Metric: acc=0.8600917431192661, hinge=0.8531898224572523, ce=0.5508755134425816
Local test acc @ epoch 115: 0.8601
Client 3 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0491851307451725
Local loss @ local epoch 1: 0.038480330258607864
Local loss @ local epoch 2: 0.03398093953728676
Local loss @ local epoch 3: 0.03378602862358093
Local loss @ local epoch 4: 0.035107970237731934
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.95 seconds!
[tester] 
SST2Metric: acc=0.8497706422018348, hinge=0.9004214382773146, ce=0.5838388356331036
Local test acc @ epoch 115: 0.8498
Client 5 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0004251096397638321
Local loss @ local epoch 1: 0.0003742519475053996
Local loss @ local epoch 2: 0.00036969021311961114
Local loss @ local epoch 3: 0.00036488447221927345
Local loss @ local epoch 4: 0.000347964814864099
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.0 seconds!
[tester] 
SST2Metric: acc=0.8589449541284404, hinge=0.8693387489253228, ce=0.5608762608634932
Local test acc @ epoch 115: 0.8589
Client 2 execute local training on 8 samples...
Local loss @ local epoch 0: 0.03775719180703163
Local loss @ local epoch 1: 0.024456966668367386
Local loss @ local epoch 2: 0.016069553792476654
Local loss @ local epoch 3: 0.0108482139185071
Local loss @ local epoch 4: 0.007611514069139957
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.98 seconds!
[tester] 
SST2Metric: acc=0.8532110091743119, hinge=0.8857782268469486, ce=0.5810492812042283
Local test acc @ epoch 115: 0.8532
Client 4 execute local training on 8 samples...
Local loss @ local epoch 0: 0.030987977981567383
Local loss @ local epoch 1: 0.02537309005856514
Local loss @ local epoch 2: 0.02236853912472725
Local loss @ local epoch 3: 0.020057471469044685
Local loss @ local epoch 4: 0.01785690151154995
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.04 seconds!
[tester] 
SST2Metric: acc=0.8577981651376146, hinge=0.8470579443174765, ce=0.5486252166261659
Local test acc @ epoch 115: 0.8578
Client 1 execute local training on 8 samples...
Local loss @ local epoch 0: 0.00021513209503609687
Local loss @ local epoch 1: 0.00018232301226817071
Local loss @ local epoch 2: 0.00016827957006171346
Local loss @ local epoch 3: 0.00016097973275464028
Local loss @ local epoch 4: 0.0001530406007077545
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.94 seconds!
[tester] 
SST2Metric: acc=0.8555045871559633, hinge=0.8909095205845089, ce=0.5835124830449152
Local test acc @ epoch 115: 0.8555
Client 0 execute local training on 8 samples...
Local loss @ local epoch 0: 0.001106408890336752
Local loss @ local epoch 1: 0.0009621527278795838
Local loss @ local epoch 2: 0.0008469767635688186
Local loss @ local epoch 3: 0.0007535344921052456
Local loss @ local epoch 4: 0.0006808277685195208
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.0 seconds!
[tester] 
SST2Metric: acc=0.8532110091743119, hinge=0.8851462660032675, ce=0.5798217684972803
Local test acc @ epoch 115: 0.8532
Client 8 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0033315704204142094
Local loss @ local epoch 1: 0.002242476912215352
Local loss @ local epoch 2: 0.0016271586064249277
Local loss @ local epoch 3: 0.001318861497566104
Local loss @ local epoch 4: 0.0012017071712762117
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.88 seconds!
[tester] 
SST2Metric: acc=0.8405963302752294, hinge=1.0408710989109966, ce=0.703549132475085
Local test acc @ epoch 115: 0.8406
Global evaluate on test data...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.24 seconds!
[tester] 
SST2Metric: acc=0.8096330275229358, hinge=0.9646548735439231, ce=0.5658511809017851
Global test acc @ epoch 115: 0.8096
Global epoch 116...
Client 0 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0008891252218745649
Local loss @ local epoch 1: 0.0007094262400642037
Local loss @ local epoch 2: 0.0006487841601483524
Local loss @ local epoch 3: 0.0006403368897736073
Local loss @ local epoch 4: 0.0006331129698082805
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.01 seconds!
[tester] 
SST2Metric: acc=0.8520642201834863, hinge=0.8973519941535565, ce=0.5907362997475819
Local test acc @ epoch 116: 0.8521
Client 4 execute local training on 8 samples...
Local loss @ local epoch 0: 0.03762711584568024
Local loss @ local epoch 1: 0.02917911857366562
Local loss @ local epoch 2: 0.025079241022467613
Local loss @ local epoch 3: 0.022861367091536522
Local loss @ local epoch 4: 0.02101505547761917
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.06 seconds!
[tester] 
SST2Metric: acc=0.8623853211009175, hinge=0.8492263848081641, ce=0.5493156523967093
Local test acc @ epoch 116: 0.8624
Client 3 execute local training on 8 samples...
Local loss @ local epoch 0: 0.043035537004470825
Local loss @ local epoch 1: 0.034905679523944855
Local loss @ local epoch 2: 0.03239016234874725
Local loss @ local epoch 3: 0.033001210540533066
Local loss @ local epoch 4: 0.03373542055487633
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.02 seconds!
[tester] 
SST2Metric: acc=0.856651376146789, hinge=0.8757845062181491, ce=0.5639363370358905
Local test acc @ epoch 116: 0.8567
Client 7 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0033428571186959743
Local loss @ local epoch 1: 0.0023145063314586878
Local loss @ local epoch 2: 0.0017465241253376007
Local loss @ local epoch 3: 0.0014912585029378533
Local loss @ local epoch 4: 0.0014389173593372107
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.02 seconds!
[tester] 
SST2Metric: acc=0.856651376146789, hinge=0.8590043018990701, ce=0.557145074067035
Local test acc @ epoch 116: 0.8567
Client 1 execute local training on 8 samples...
Local loss @ local epoch 0: 0.00022759527200832963
Local loss @ local epoch 1: 0.0002078926918329671
Local loss @ local epoch 2: 0.0001904090604512021
Local loss @ local epoch 3: 0.00017392358859069645
Local loss @ local epoch 4: 0.00015865884779486805
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.94 seconds!
[tester] 
SST2Metric: acc=0.856651376146789, hinge=0.8682547323747513, ce=0.5647859247363975
Local test acc @ epoch 116: 0.8567
Client 9 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0031675014179199934
Local loss @ local epoch 1: 0.002775939181447029
Local loss @ local epoch 2: 0.0024488430935889482
Local loss @ local epoch 3: 0.002159849740564823
Local loss @ local epoch 4: 0.0019128646235913038
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.95 seconds!
[tester] 
SST2Metric: acc=0.856651376146789, hinge=0.87411244610034, ce=0.5713403306088684
Local test acc @ epoch 116: 0.8567
Client 5 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0012909196084365249
Local loss @ local epoch 1: 0.0008351912838406861
Local loss @ local epoch 2: 0.0005668082158081234
Local loss @ local epoch 3: 0.00041811418486759067
Local loss @ local epoch 4: 0.0003459714353084564
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.94 seconds!
[tester] 
SST2Metric: acc=0.8509174311926605, hinge=0.8884369204623983, ce=0.5765208148804959
Local test acc @ epoch 116: 0.8509
Client 8 execute local training on 8 samples...
Local loss @ local epoch 0: 0.019464630633592606
Local loss @ local epoch 1: 0.012475919909775257
Local loss @ local epoch 2: 0.008092573843896389
Local loss @ local epoch 3: 0.00537086371332407
Local loss @ local epoch 4: 0.0036780317313969135
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.01 seconds!
[tester] 
SST2Metric: acc=0.856651376146789, hinge=0.8786103755509088, ce=0.5748432203877659
Local test acc @ epoch 116: 0.8567
Client 6 execute local training on 8 samples...
Local loss @ local epoch 0: 0.17756123840808868
Local loss @ local epoch 1: 0.1303505301475525
Local loss @ local epoch 2: 0.09580306708812714
Local loss @ local epoch 3: 0.07288402318954468
Local loss @ local epoch 4: 0.05975990369915962
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.92 seconds!
[tester] 
SST2Metric: acc=0.8532110091743119, hinge=0.8884737617378935, ce=0.5766724594067697
Local test acc @ epoch 116: 0.8532
Client 2 execute local training on 8 samples...
Local loss @ local epoch 0: 0.04076292738318443
Local loss @ local epoch 1: 0.026404961943626404
Local loss @ local epoch 2: 0.01732616312801838
Local loss @ local epoch 3: 0.011658402159810066
Local loss @ local epoch 4: 0.008129452355206013
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.93 seconds!
[tester] 
SST2Metric: acc=0.8543577981651376, hinge=0.8790270790047602, ce=0.5757060419976895
Local test acc @ epoch 116: 0.8544
Global evaluate on test data...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.41 seconds!
[tester] 
SST2Metric: acc=0.8119266055045872, hinge=0.9659457834215339, ce=0.5673241680295745
Global test acc @ epoch 116: 0.8119
Global epoch 117...
Client 4 execute local training on 8 samples...
Local loss @ local epoch 0: 0.023895906284451485
Local loss @ local epoch 1: 0.020084531977772713
Local loss @ local epoch 2: 0.01786361262202263
Local loss @ local epoch 3: 0.015959512442350388
Local loss @ local epoch 4: 0.014185018837451935
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.99 seconds!
[tester] 
SST2Metric: acc=0.8600917431192661, hinge=0.8546941318643202, ce=0.5543192872895598
Local test acc @ epoch 117: 0.8601
Client 3 execute local training on 8 samples...
Local loss @ local epoch 0: 0.040980830788612366
Local loss @ local epoch 1: 0.034384965896606445
Local loss @ local epoch 2: 0.03302421420812607
Local loss @ local epoch 3: 0.03370260074734688
Local loss @ local epoch 4: 0.033646177500486374
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.97 seconds!
[tester] 
SST2Metric: acc=0.8612385321100917, hinge=0.8493339201999367, ce=0.5464521526100559
Local test acc @ epoch 117: 0.8612
Client 5 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0004417334566824138
Local loss @ local epoch 1: 0.0003843469312414527
Local loss @ local epoch 2: 0.00037621171213686466
Local loss @ local epoch 3: 0.0003697933570947498
Local loss @ local epoch 4: 0.0003513224655762315
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.97 seconds!
[tester] 
SST2Metric: acc=0.8577981651376146, hinge=0.8783307737166729, ce=0.5664978302067889
Local test acc @ epoch 117: 0.8578
Client 1 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0006890837103128433
Local loss @ local epoch 1: 0.0004461776989046484
Local loss @ local epoch 2: 0.0003020810545422137
Local loss @ local epoch 3: 0.00022055767476558685
Local loss @ local epoch 4: 0.000177777445060201
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.89 seconds!
[tester] 
SST2Metric: acc=0.8600917431192661, hinge=0.8618753299527212, ce=0.5602242471532759
Local test acc @ epoch 117: 0.8601
Client 2 execute local training on 8 samples...
Local loss @ local epoch 0: 0.01172985602170229
Local loss @ local epoch 1: 0.007712469901889563
Local loss @ local epoch 2: 0.005248855333775282
Local loss @ local epoch 3: 0.0037919185124337673
Local loss @ local epoch 4: 0.0029825004749000072
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.91 seconds!
[tester] 
SST2Metric: acc=0.841743119266055, hinge=1.016426783089244, ce=0.683843907033815
Local test acc @ epoch 117: 0.8417
Client 8 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0016034920699894428
Local loss @ local epoch 1: 0.0014096228405833244
Local loss @ local epoch 2: 0.0013050924753770232
Local loss @ local epoch 3: 0.0012059169821441174
Local loss @ local epoch 4: 0.001105251256376505
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.98 seconds!
[tester] 
SST2Metric: acc=0.8428899082568807, hinge=1.0368006043477889, ce=0.7011712515308518
Local test acc @ epoch 117: 0.8429
Client 7 execute local training on 8 samples...
Local loss @ local epoch 0: 0.010538747534155846
Local loss @ local epoch 1: 0.006821122486144304
Local loss @ local epoch 2: 0.004509147256612778
Local loss @ local epoch 3: 0.0030945895705372095
Local loss @ local epoch 4: 0.002243648748844862
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.97 seconds!
[tester] 
SST2Metric: acc=0.8577981651376146, hinge=0.874132638130713, ce=0.5714758687716462
Local test acc @ epoch 117: 0.8578
Client 0 execute local training on 8 samples...
Local loss @ local epoch 0: 0.000592192867770791
Local loss @ local epoch 1: 0.0005551279755309224
Local loss @ local epoch 2: 0.000525758892763406
Local loss @ local epoch 3: 0.0004972273018211126
Local loss @ local epoch 4: 0.000473452644655481
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.03 seconds!
[tester] 
SST2Metric: acc=0.8555045871559633, hinge=0.8826390097447492, ce=0.5763950684129735
Local test acc @ epoch 117: 0.8555
Client 6 execute local training on 8 samples...
Local loss @ local epoch 0: 0.1866283118724823
Local loss @ local epoch 1: 0.13724622130393982
Local loss @ local epoch 2: 0.10092496871948242
Local loss @ local epoch 3: 0.07669837772846222
Local loss @ local epoch 4: 0.06272238492965698
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.05 seconds!
[tester] 
SST2Metric: acc=0.8474770642201835, hinge=0.9081661680696207, ce=0.5924472317351627
Local test acc @ epoch 117: 0.8475
Client 9 execute local training on 8 samples...
Local loss @ local epoch 0: 0.007041939068585634
Local loss @ local epoch 1: 0.0045204320922493935
Local loss @ local epoch 2: 0.0030344135593622923
Local loss @ local epoch 3: 0.0021997347939759493
Local loss @ local epoch 4: 0.0017699716845527291
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.95 seconds!
[tester] 
SST2Metric: acc=0.8543577981651376, hinge=0.880048179161658, ce=0.5751958934120555
Local test acc @ epoch 117: 0.8544
Global evaluate on test data...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.33 seconds!
[tester] 
SST2Metric: acc=0.8107798165137615, hinge=0.9878923570344208, ce=0.5843281294496884
Global test acc @ epoch 117: 0.8108
Global epoch 118...
Client 9 execute local training on 8 samples...
Local loss @ local epoch 0: 0.00228931475430727
Local loss @ local epoch 1: 0.0020241811871528625
Local loss @ local epoch 2: 0.0018088816432282329
Local loss @ local epoch 3: 0.0016101569635793567
Local loss @ local epoch 4: 0.0014380490174517035
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.99 seconds!
[tester] 
SST2Metric: acc=0.8555045871559633, hinge=0.8835378490183332, ce=0.5788976152820263
Local test acc @ epoch 118: 0.8555
Client 6 execute local training on 8 samples...
Local loss @ local epoch 0: 0.1920640766620636
Local loss @ local epoch 1: 0.14187920093536377
Local loss @ local epoch 2: 0.10451270639896393
Local loss @ local epoch 3: 0.07915152609348297
Local loss @ local epoch 4: 0.06411152333021164
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.99 seconds!
[tester] 
SST2Metric: acc=0.8520642201834863, hinge=0.8981877093741654, ce=0.5853304982734332
Local test acc @ epoch 118: 0.8521
Client 4 execute local training on 8 samples...
Local loss @ local epoch 0: 0.05729188770055771
Local loss @ local epoch 1: 0.03847294673323631
Local loss @ local epoch 2: 0.027240382507443428
Local loss @ local epoch 3: 0.02131275273859501
Local loss @ local epoch 4: 0.018661975860595703
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.02 seconds!
[tester] 
SST2Metric: acc=0.856651376146789, hinge=0.8643425593682386, ce=0.5633395073810964
Local test acc @ epoch 118: 0.8567
Client 1 execute local training on 8 samples...
Local loss @ local epoch 0: 0.00019119995704386383
Local loss @ local epoch 1: 0.00017515946819912642
Local loss @ local epoch 2: 0.00016013183631002903
Local loss @ local epoch 3: 0.0001461020583519712
Local loss @ local epoch 4: 0.00013321872393134981
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.01 seconds!
[tester] 
SST2Metric: acc=0.8577981651376146, hinge=0.8725421384933891, ce=0.5700013802952609
Local test acc @ epoch 118: 0.8578
Client 8 execute local training on 8 samples...
Local loss @ local epoch 0: 0.007593909278512001
Local loss @ local epoch 1: 0.004892441444098949
Local loss @ local epoch 2: 0.0032431157305836678
Local loss @ local epoch 3: 0.0022595252376049757
Local loss @ local epoch 4: 0.0016900708433240652
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.98 seconds!
[tester] 
SST2Metric: acc=0.841743119266055, hinge=1.0253285365093738, ce=0.6927906697618784
Local test acc @ epoch 118: 0.8417
Client 0 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0019157392671331763
Local loss @ local epoch 1: 0.0012472022790461779
Local loss @ local epoch 2: 0.0008843546384014189
Local loss @ local epoch 3: 0.0007069673738442361
Local loss @ local epoch 4: 0.0006385563756339252
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.96 seconds!
[tester] 
SST2Metric: acc=0.8543577981651376, hinge=0.8768330190433274, ce=0.5744761339381579
Local test acc @ epoch 118: 0.8544
Client 5 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0012432937510311604
Local loss @ local epoch 1: 0.0007984269759617746
Local loss @ local epoch 2: 0.0005372343584895134
Local loss @ local epoch 3: 0.00039196896250359714
Local loss @ local epoch 4: 0.0003199323546141386
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.1 seconds!
[tester] 
SST2Metric: acc=0.856651376146789, hinge=0.8852851913062805, ce=0.5752454600812239
Local test acc @ epoch 118: 0.8567
Client 2 execute local training on 8 samples...
Local loss @ local epoch 0: 0.039451539516448975
Local loss @ local epoch 1: 0.025593258440494537
Local loss @ local epoch 2: 0.01681683585047722
Local loss @ local epoch 3: 0.011323552578687668
Local loss @ local epoch 4: 0.00789159070700407
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.96 seconds!
[tester] 
SST2Metric: acc=0.8532110091743119, hinge=0.8819566944596964, ce=0.5792793284541203
Local test acc @ epoch 118: 0.8532
Client 7 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0018474218668416142
Local loss @ local epoch 1: 0.0014583682641386986
Local loss @ local epoch 2: 0.0013349740765988827
Local loss @ local epoch 3: 0.0013571479357779026
Local loss @ local epoch 4: 0.001395004102960229
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.92 seconds!
[tester] 
SST2Metric: acc=0.856651376146789, hinge=0.8687815991563534, ce=0.564292846657544
Local test acc @ epoch 118: 0.8567
Client 3 execute local training on 8 samples...
Local loss @ local epoch 0: 0.03782546520233154
Local loss @ local epoch 1: 0.031559962779283524
Local loss @ local epoch 2: 0.03023548610508442
Local loss @ local epoch 3: 0.030951732769608498
Local loss @ local epoch 4: 0.0310339517891407
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.01 seconds!
[tester] 
SST2Metric: acc=0.8497706422018348, hinge=0.914000022438688, ce=0.5970457688234576
Local test acc @ epoch 118: 0.8498
Global evaluate on test data...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.33 seconds!
[tester] 
SST2Metric: acc=0.8096330275229358, hinge=0.9747035038580588, ce=0.5751630712840963
Global test acc @ epoch 118: 0.8096
Global epoch 119...
Client 8 execute local training on 8 samples...
Local loss @ local epoch 0: 0.006370315328240395
Local loss @ local epoch 1: 0.004092248622328043
Local loss @ local epoch 2: 0.0027013132348656654
Local loss @ local epoch 3: 0.0018695711623877287
Local loss @ local epoch 4: 0.001385135343298316
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.97 seconds!
[tester] 
SST2Metric: acc=0.8451834862385321, hinge=0.9597797020586258, ce=0.641611554824313
Local test acc @ epoch 119: 0.8452
Client 1 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0005442386609502137
Local loss @ local epoch 1: 0.0003705694689415395
Local loss @ local epoch 2: 0.000267475814325735
Local loss @ local epoch 3: 0.0002117995172739029
Local loss @ local epoch 4: 0.0001866238162619993
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.0 seconds!
[tester] 
SST2Metric: acc=0.8589449541284404, hinge=0.8648819842743217, ce=0.5641729921821308
Local test acc @ epoch 119: 0.8589
Client 6 execute local training on 8 samples...
Local loss @ local epoch 0: 0.09583849459886551
Local loss @ local epoch 1: 0.069810651242733
Local loss @ local epoch 2: 0.05393107235431671
Local loss @ local epoch 3: 0.04620448872447014
Local loss @ local epoch 4: 0.044094037264585495
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.03 seconds!
[tester] 
SST2Metric: acc=0.8577981651376146, hinge=0.8857167793796696, ce=0.574483747156166
Local test acc @ epoch 119: 0.8578
Client 5 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0003322907432448119
Local loss @ local epoch 1: 0.0003164778172504157
Local loss @ local epoch 2: 0.00030017041717655957
Local loss @ local epoch 3: 0.0002833970356732607
Local loss @ local epoch 4: 0.0002687828382477164
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.95 seconds!
[tester] 
SST2Metric: acc=0.856651376146789, hinge=0.8868378298271686, ce=0.5768487947595673
Local test acc @ epoch 119: 0.8567
Client 0 execute local training on 8 samples...
Local loss @ local epoch 0: 0.003022528951987624
Local loss @ local epoch 1: 0.0019306718604639173
Local loss @ local epoch 2: 0.0012734311167150736
Local loss @ local epoch 3: 0.0008891551406122744
Local loss @ local epoch 4: 0.0006742707337252796
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.1 seconds!
[tester] 
SST2Metric: acc=0.856651376146789, hinge=0.8777264904811841, ce=0.5761620972685755
Local test acc @ epoch 119: 0.8567
Client 7 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0015308998990803957
Local loss @ local epoch 1: 0.0013279548147693276
Local loss @ local epoch 2: 0.0013125411933287978
Local loss @ local epoch 3: 0.0013213710626587272
Local loss @ local epoch 4: 0.0012826693709939718
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.92 seconds!
[tester] 
SST2Metric: acc=0.8600917431192661, hinge=0.8684131165436648, ce=0.5663729444678303
Local test acc @ epoch 119: 0.8601
Client 3 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0437915064394474
Local loss @ local epoch 1: 0.034141432493925095
Local loss @ local epoch 2: 0.030166059732437134
Local loss @ local epoch 3: 0.030060095712542534
Local loss @ local epoch 4: 0.03126984089612961
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.96 seconds!
[tester] 
SST2Metric: acc=0.8497706422018348, hinge=0.922078691764709, ce=0.6039518280154513
Local test acc @ epoch 119: 0.8498
Client 2 execute local training on 8 samples...
Local loss @ local epoch 0: 0.07262647151947021
Local loss @ local epoch 1: 0.04745088517665863
Local loss @ local epoch 2: 0.031172502785921097
Local loss @ local epoch 3: 0.020826000720262527
Local loss @ local epoch 4: 0.014266586862504482
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.96 seconds!
[tester] 
SST2Metric: acc=0.8589449541284404, hinge=0.8692900768113793, ce=0.5684106762258233
Local test acc @ epoch 119: 0.8589
Client 9 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0018658273620530963
Local loss @ local epoch 1: 0.0016168688889592886
Local loss @ local epoch 2: 0.0014923573471605778
Local loss @ local epoch 3: 0.0013666388113051653
Local loss @ local epoch 4: 0.0012337706284597516
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.97 seconds!
[tester] 
SST2Metric: acc=0.856651376146789, hinge=0.8956870811248044, ce=0.5920963298371585
Local test acc @ epoch 119: 0.8567
Client 4 execute local training on 8 samples...
Local loss @ local epoch 0: 0.037705130875110626
Local loss @ local epoch 1: 0.03184378892183304
Local loss @ local epoch 2: 0.027516070753335953
Local loss @ local epoch 3: 0.023787468671798706
Local loss @ local epoch 4: 0.02058180421590805
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.99 seconds!
[tester] 
SST2Metric: acc=0.856651376146789, hinge=0.8684697657003315, ce=0.568555370774072
Local test acc @ epoch 119: 0.8567
Global evaluate on test data...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.32 seconds!
[tester] 
SST2Metric: acc=0.8153669724770642, hinge=0.9682648868188946, ce=0.5714528146178521
Global test acc @ epoch 119: 0.8154
Global epoch 120...
Client 6 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0824476107954979
Local loss @ local epoch 1: 0.0616583414375782
Local loss @ local epoch 2: 0.05030056834220886
Local loss @ local epoch 3: 0.0460318922996521
Local loss @ local epoch 4: 0.045724403113126755
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.91 seconds!
[tester] 
SST2Metric: acc=0.8543577981651376, hinge=0.8977878723942906, ce=0.5844134881856441
Local test acc @ epoch 120: 0.8544
Client 9 execute local training on 8 samples...
Local loss @ local epoch 0: 0.00978049822151661
Local loss @ local epoch 1: 0.006238894071429968
Local loss @ local epoch 2: 0.00412728451192379
Local loss @ local epoch 3: 0.002912501571699977
Local loss @ local epoch 4: 0.002251840429380536
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.0 seconds!
[tester] 
SST2Metric: acc=0.856651376146789, hinge=0.8685555642624514, ce=0.5686666153594553
Local test acc @ epoch 120: 0.8567
Client 7 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0013567634159699082
Local loss @ local epoch 1: 0.0012991473777219653
Local loss @ local epoch 2: 0.0012464458122849464
Local loss @ local epoch 3: 0.001184990513138473
Local loss @ local epoch 4: 0.0011374307796359062
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.89 seconds!
[tester] 
SST2Metric: acc=0.8600917431192661, hinge=0.8681340608574929, ce=0.5676079397073759
Local test acc @ epoch 120: 0.8601
Client 1 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0002236899163108319
Local loss @ local epoch 1: 0.0001931216538650915
Local loss @ local epoch 2: 0.00017822573136072606
Local loss @ local epoch 3: 0.00016782741295173764
Local loss @ local epoch 4: 0.00015647827240172774
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.96 seconds!
[tester] 
SST2Metric: acc=0.8555045871559633, hinge=0.8859654134566631, ce=0.5831580884262563
Local test acc @ epoch 120: 0.8555
Client 2 execute local training on 8 samples...
Local loss @ local epoch 0: 0.007314545568078756
Local loss @ local epoch 1: 0.00492157693952322
Local loss @ local epoch 2: 0.0035151492338627577
Local loss @ local epoch 3: 0.0027595802675932646
Local loss @ local epoch 4: 0.002427601721137762
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.1 seconds!
[tester] 
SST2Metric: acc=0.8428899082568807, hinge=1.0569014456294, ce=0.7204677987373118
Local test acc @ epoch 120: 0.8429
Client 0 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0017265520291402936
Local loss @ local epoch 1: 0.0011351859429851174
Local loss @ local epoch 2: 0.0008007618598639965
Local loss @ local epoch 3: 0.0006287345895543694
Local loss @ local epoch 4: 0.0005580129800364375
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.07 seconds!
[tester] 
SST2Metric: acc=0.8555045871559633, hinge=0.8896220513713469, ce=0.5868707670911093
Local test acc @ epoch 120: 0.8555
Client 5 execute local training on 8 samples...
Local loss @ local epoch 0: 0.001115195918828249
Local loss @ local epoch 1: 0.0007180872489698231
Local loss @ local epoch 2: 0.00048100732965394855
Local loss @ local epoch 3: 0.0003454738762229681
Local loss @ local epoch 4: 0.0002742511569522321
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.96 seconds!
[tester] 
SST2Metric: acc=0.8555045871559633, hinge=0.885895983341637, ce=0.577988213527341
Local test acc @ epoch 120: 0.8555
Client 4 execute local training on 8 samples...
Local loss @ local epoch 0: 0.04782319813966751
Local loss @ local epoch 1: 0.03205180540680885
Local loss @ local epoch 2: 0.022925324738025665
Local loss @ local epoch 3: 0.018276937305927277
Local loss @ local epoch 4: 0.0162599328905344
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.99 seconds!
[tester] 
SST2Metric: acc=0.8577981651376146, hinge=0.8730556978545058, ce=0.5727284135595455
Local test acc @ epoch 120: 0.8578
Client 3 execute local training on 8 samples...
Local loss @ local epoch 0: 0.05595824494957924
Local loss @ local epoch 1: 0.04162479192018509
Local loss @ local epoch 2: 0.033852774649858475
Local loss @ local epoch 3: 0.03123999759554863
Local loss @ local epoch 4: 0.031854718923568726
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.91 seconds!
[tester] 
SST2Metric: acc=0.8543577981651376, hinge=0.873675663126718, ce=0.567650632644147
Local test acc @ epoch 120: 0.8544
Client 8 execute local training on 8 samples...
Local loss @ local epoch 0: 0.03105059638619423
Local loss @ local epoch 1: 0.020045828074216843
Local loss @ local epoch 2: 0.01303846575319767
Local loss @ local epoch 3: 0.008637262508273125
Local loss @ local epoch 4: 0.005872203037142754
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.95 seconds!
[tester] 
SST2Metric: acc=0.8555045871559633, hinge=0.8818935164071005, ce=0.579797207308696
Local test acc @ epoch 120: 0.8555
Global evaluate on test data...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.19 seconds!
[tester] 
SST2Metric: acc=0.8107798165137615, hinge=0.9722849026732489, ce=0.5755760877258614
Global test acc @ epoch 120: 0.8108
Global epoch 121...
Client 4 execute local training on 8 samples...
Local loss @ local epoch 0: 0.017927365377545357
Local loss @ local epoch 1: 0.01543665211647749
Local loss @ local epoch 2: 0.013524875976145267
Local loss @ local epoch 3: 0.011886595748364925
Local loss @ local epoch 4: 0.010526364669203758
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.88 seconds!
[tester] 
SST2Metric: acc=0.8600917431192661, hinge=0.8575622073007286, ce=0.5602669902989607
Local test acc @ epoch 121: 0.8601
Client 0 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0014568845508620143
Local loss @ local epoch 1: 0.0010739219142124057
Local loss @ local epoch 2: 0.0008939089020714164
Local loss @ local epoch 3: 0.0008234373526647687
Local loss @ local epoch 4: 0.0007897724281065166
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.91 seconds!
[tester] 
SST2Metric: acc=0.8555045871559633, hinge=0.9273642691176965, ce=0.6172578316210602
Local test acc @ epoch 121: 0.8555
Client 7 execute local training on 8 samples...
Local loss @ local epoch 0: 0.002273598685860634
Local loss @ local epoch 1: 0.0016607805155217648
Local loss @ local epoch 2: 0.0013746563345193863
Local loss @ local epoch 3: 0.0012985951034352183
Local loss @ local epoch 4: 0.0013243097346276045
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.96 seconds!
[tester] 
SST2Metric: acc=0.8646788990825688, hinge=0.8528602350195613, ce=0.5576903716924839
Local test acc @ epoch 121: 0.8647
Client 3 execute local training on 8 samples...
Local loss @ local epoch 0: 0.03498267009854317
Local loss @ local epoch 1: 0.03112749010324478
Local loss @ local epoch 2: 0.031046191230416298
Local loss @ local epoch 3: 0.03094109334051609
Local loss @ local epoch 4: 0.029797183349728584
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.03 seconds!
[tester] 
SST2Metric: acc=0.856651376146789, hinge=0.8699136316229444, ce=0.5651983627883949
Local test acc @ epoch 121: 0.8567
Client 6 execute local training on 8 samples...
Local loss @ local epoch 0: 0.04412328824400902
Local loss @ local epoch 1: 0.04035006836056709
Local loss @ local epoch 2: 0.03694537281990051
Local loss @ local epoch 3: 0.03382936120033264
Local loss @ local epoch 4: 0.031031981110572815
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.95 seconds!
[tester] 
SST2Metric: acc=0.8600917431192661, hinge=0.8485610828213735, ce=0.5522374920585724
Local test acc @ epoch 121: 0.8601
Client 8 execute local training on 8 samples...
Local loss @ local epoch 0: 0.036159493029117584
Local loss @ local epoch 1: 0.02345922589302063
Local loss @ local epoch 2: 0.015308918431401253
Local loss @ local epoch 3: 0.010165557265281677
Local loss @ local epoch 4: 0.0069283731281757355
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.99 seconds!
[tester] 
SST2Metric: acc=0.8577981651376146, hinge=0.9139634091919715, ce=0.605960251596917
Local test acc @ epoch 121: 0.8578
Client 9 execute local training on 8 samples...
Local loss @ local epoch 0: 0.003957645036280155
Local loss @ local epoch 1: 0.0030547198839485645
Local loss @ local epoch 2: 0.0026800939813256264
Local loss @ local epoch 3: 0.0025151753798127174
Local loss @ local epoch 4: 0.0023697211872786283
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.99 seconds!
[tester] 
SST2Metric: acc=0.8577981651376146, hinge=0.8633374488408413, ce=0.5662335894933535
Local test acc @ epoch 121: 0.8578
Client 5 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0005784398526884615
Local loss @ local epoch 1: 0.0003921985626220703
Local loss @ local epoch 2: 0.00029094700585119426
Local loss @ local epoch 3: 0.00024680898059159517
Local loss @ local epoch 4: 0.00023934774799272418
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.92 seconds!
[tester] 
SST2Metric: acc=0.8555045871559633, hinge=0.9016102199707556, ce=0.58921688114514
Local test acc @ epoch 121: 0.8555
Client 1 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0006688171997666359
Local loss @ local epoch 1: 0.00043092406122013927
Local loss @ local epoch 2: 0.00028864029445685446
Local loss @ local epoch 3: 0.00020666304044425488
Local loss @ local epoch 4: 0.00016195903299376369
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.88 seconds!
[tester] 
SST2Metric: acc=0.8577981651376146, hinge=0.867306747567763, ce=0.5694207252565837
Local test acc @ epoch 121: 0.8578
Client 2 execute local training on 8 samples...
Local loss @ local epoch 0: 0.011194531805813313
Local loss @ local epoch 1: 0.007349877152591944
Local loss @ local epoch 2: 0.004995507653802633
Local loss @ local epoch 3: 0.003602806478738785
Local loss @ local epoch 4: 0.002825603587552905
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.93 seconds!
[tester] 
SST2Metric: acc=0.8405963302752294, hinge=1.0049464446962426, ce=0.6795270677426376
Local test acc @ epoch 121: 0.8406
Global evaluate on test data...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.19 seconds!
[tester] 
SST2Metric: acc=0.8130733944954128, hinge=0.9534983577531412, ce=0.5617181160722201
Global test acc @ epoch 121: 0.8131
Global epoch 122...
Client 1 execute local training on 8 samples...
Local loss @ local epoch 0: 0.00013959115312900394
Local loss @ local epoch 1: 0.00012350731412880123
Local loss @ local epoch 2: 0.00011524003639351577
Local loss @ local epoch 3: 0.00010937044862657785
Local loss @ local epoch 4: 0.00010293525701854378
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.94 seconds!
[tester] 
SST2Metric: acc=0.8577981651376146, hinge=0.8944771410128393, ce=0.5907856352867451
Local test acc @ epoch 122: 0.8578
Client 4 execute local training on 8 samples...
Local loss @ local epoch 0: 0.012926587834954262
Local loss @ local epoch 1: 0.010223429650068283
Local loss @ local epoch 2: 0.009189561009407043
Local loss @ local epoch 3: 0.008793397806584835
Local loss @ local epoch 4: 0.008395388722419739
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.03 seconds!
[tester] 
SST2Metric: acc=0.8589449541284404, hinge=0.8507023954063381, ce=0.555646782192517
Local test acc @ epoch 122: 0.8589
Client 3 execute local training on 8 samples...
Local loss @ local epoch 0: 0.03762437403202057
Local loss @ local epoch 1: 0.03385594114661217
Local loss @ local epoch 2: 0.03359131142497063
Local loss @ local epoch 3: 0.033051252365112305
Local loss @ local epoch 4: 0.03160057216882706
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.92 seconds!
[tester] 
SST2Metric: acc=0.8612385321100917, hinge=0.8562277429694429, ce=0.5599449538258507
Local test acc @ epoch 122: 0.8612
Client 7 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0017134608933702111
Local loss @ local epoch 1: 0.001477609621360898
Local loss @ local epoch 2: 0.0014049711171537638
Local loss @ local epoch 3: 0.001352395280264318
Local loss @ local epoch 4: 0.0012783089186996222
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.02 seconds!
[tester] 
SST2Metric: acc=0.8600917431192661, hinge=0.8707044012502793, ce=0.5711020306804764
Local test acc @ epoch 122: 0.8601
Client 5 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0007752994424663484
Local loss @ local epoch 1: 0.000506394833792001
Local loss @ local epoch 2: 0.0003524341154843569
Local loss @ local epoch 3: 0.00027272768784314394
Local loss @ local epoch 4: 0.00024068176571745425
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.94 seconds!
[tester] 
SST2Metric: acc=0.856651376146789, hinge=0.8840866371852543, ce=0.5760610316302777
Local test acc @ epoch 122: 0.8567
Client 8 execute local training on 8 samples...
Local loss @ local epoch 0: 0.02906867489218712
Local loss @ local epoch 1: 0.01873127557337284
Local loss @ local epoch 2: 0.012170487083494663
Local loss @ local epoch 3: 0.00805877335369587
Local loss @ local epoch 4: 0.005479061976075172
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.0 seconds!
[tester] 
SST2Metric: acc=0.8612385321100917, hinge=0.8743118360775326, ce=0.5751949835327291
Local test acc @ epoch 122: 0.8612
Client 6 execute local training on 8 samples...
Local loss @ local epoch 0: 0.1167483776807785
Local loss @ local epoch 1: 0.08406940847635269
Local loss @ local epoch 2: 0.06210131570696831
Local loss @ local epoch 3: 0.04918761923909187
Local loss @ local epoch 4: 0.04325145483016968
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.91 seconds!
[tester] 
SST2Metric: acc=0.8577981651376146, hinge=0.8795888099921952, ce=0.5730148965670138
Local test acc @ epoch 122: 0.8578
Client 2 execute local training on 8 samples...
Local loss @ local epoch 0: 0.04926576465368271
Local loss @ local epoch 1: 0.032053057104349136
Local loss @ local epoch 2: 0.021060477942228317
Local loss @ local epoch 3: 0.014120429754257202
Local loss @ local epoch 4: 0.009736749343574047
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.91 seconds!
[tester] 
SST2Metric: acc=0.856651376146789, hinge=0.8812013487203405, ce=0.579925562341862
Local test acc @ epoch 122: 0.8567
Client 9 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0021948290523141623
Local loss @ local epoch 1: 0.0019281150307506323
Local loss @ local epoch 2: 0.0017269495874643326
Local loss @ local epoch 3: 0.001537203905172646
Local loss @ local epoch 4: 0.0013678943505510688
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.02 seconds!
[tester] 
SST2Metric: acc=0.8577981651376146, hinge=0.8859010670983464, ce=0.5855452440878796
Local test acc @ epoch 122: 0.8578
Client 0 execute local training on 8 samples...
Local loss @ local epoch 0: 0.00047991774044930935
Local loss @ local epoch 1: 0.0004603122069966048
Local loss @ local epoch 2: 0.00044655019883066416
Local loss @ local epoch 3: 0.0004259694251231849
Local loss @ local epoch 4: 0.0004077036282978952
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.96 seconds!
[tester] 
SST2Metric: acc=0.8589449541284404, hinge=0.893301222439206, ce=0.5905586975875499
Local test acc @ epoch 122: 0.8589
Global evaluate on test data...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.35 seconds!
[tester] 
SST2Metric: acc=0.8142201834862385, hinge=0.9536823191500585, ce=0.5622488825289328
Global test acc @ epoch 122: 0.8142
Global epoch 123...
Client 7 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0011482108384370804
Local loss @ local epoch 1: 0.001093956409022212
Local loss @ local epoch 2: 0.001039842958562076
Local loss @ local epoch 3: 0.000989712541922927
Local loss @ local epoch 4: 0.0009491225937381387
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.9 seconds!
[tester] 
SST2Metric: acc=0.8623853211009175, hinge=0.8644817560364347, ce=0.5670588282065108
Local test acc @ epoch 123: 0.8624
Client 5 execute local training on 8 samples...
Local loss @ local epoch 0: 0.00033590980456210673
Local loss @ local epoch 1: 0.00025497531169094145
Local loss @ local epoch 2: 0.00022093266306910664
Local loss @ local epoch 3: 0.0002177917049266398
Local loss @ local epoch 4: 0.00022607430582866073
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.0 seconds!
[tester] 
SST2Metric: acc=0.8497706422018348, hinge=0.9144400931006178, ce=0.601050049747413
Local test acc @ epoch 123: 0.8498
Client 8 execute local training on 8 samples...
Local loss @ local epoch 0: 0.022212984040379524
Local loss @ local epoch 1: 0.014243840239942074
Local loss @ local epoch 2: 0.009232189506292343
Local loss @ local epoch 3: 0.006111103110015392
Local loss @ local epoch 4: 0.004161321558058262
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.98 seconds!
[tester] 
SST2Metric: acc=0.8589449541284404, hinge=0.8773278034881714, ce=0.5783450921656726
Local test acc @ epoch 123: 0.8589
Client 2 execute local training on 8 samples...
Local loss @ local epoch 0: 0.009656080976128578
Local loss @ local epoch 1: 0.006465050391852856
Local loss @ local epoch 2: 0.004648478236049414
Local loss @ local epoch 3: 0.0037045408971607685
Local loss @ local epoch 4: 0.003290437860414386
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.12 seconds!
[tester] 
SST2Metric: acc=0.8440366972477065, hinge=0.989114615211793, ce=0.6697777745154673
Local test acc @ epoch 123: 0.844
Client 9 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0036362814716994762
Local loss @ local epoch 1: 0.00234625325538218
Local loss @ local epoch 2: 0.0016414709389209747
Local loss @ local epoch 3: 0.0012959673767909408
Local loss @ local epoch 4: 0.0011626529740169644
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.0 seconds!
[tester] 
SST2Metric: acc=0.8600917431192661, hinge=0.8860956614990847, ce=0.585317800939457
Local test acc @ epoch 123: 0.8601
Client 0 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0004759922157973051
Local loss @ local epoch 1: 0.0004490794090088457
Local loss @ local epoch 2: 0.0004425313090905547
Local loss @ local epoch 3: 0.0004248783807270229
Local loss @ local epoch 4: 0.0004045524983666837
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.01 seconds!
[tester] 
SST2Metric: acc=0.8589449541284404, hinge=0.8958490918809121, ce=0.5930903715176568
Local test acc @ epoch 123: 0.8589
Client 3 execute local training on 8 samples...
Local loss @ local epoch 0: 0.11228813976049423
Local loss @ local epoch 1: 0.08156131207942963
Local loss @ local epoch 2: 0.05837074667215347
Local loss @ local epoch 3: 0.0422663576900959
Local loss @ local epoch 4: 0.032166123390197754
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.06 seconds!
[tester] 
SST2Metric: acc=0.8509174311926605, hinge=0.9171805338028374, ce=0.6039499176878145
Local test acc @ epoch 123: 0.8509
Client 4 execute local training on 8 samples...
Local loss @ local epoch 0: 0.06339509785175323
Local loss @ local epoch 1: 0.0423603355884552
Local loss @ local epoch 2: 0.029405636712908745
Local loss @ local epoch 3: 0.022260472178459167
Local loss @ local epoch 4: 0.018798405304551125
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.96 seconds!
[tester] 
SST2Metric: acc=0.8589449541284404, hinge=0.8748017235086598, ce=0.576152038012328
Local test acc @ epoch 123: 0.8589
Client 6 execute local training on 8 samples...
Local loss @ local epoch 0: 0.11240091919898987
Local loss @ local epoch 1: 0.08131080865859985
Local loss @ local epoch 2: 0.06104297563433647
Local loss @ local epoch 3: 0.04981274902820587
Local loss @ local epoch 4: 0.04534367099404335
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.0 seconds!
[tester] 
SST2Metric: acc=0.856651376146789, hinge=0.8985033113201824, ce=0.588126971911577
Local test acc @ epoch 123: 0.8567
Client 1 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0004945186083205044
Local loss @ local epoch 1: 0.0003228767600376159
Local loss @ local epoch 2: 0.00022053449356462806
Local loss @ local epoch 3: 0.00016261165728792548
Local loss @ local epoch 4: 0.0001323988544754684
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.96 seconds!
[tester] 
SST2Metric: acc=0.8589449541284404, hinge=0.8730007400479886, ce=0.5742641119908546
Local test acc @ epoch 123: 0.8589
Global evaluate on test data...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.4 seconds!
[tester] 
SST2Metric: acc=0.8142201834862385, hinge=0.973233943958895, ce=0.5785858605576064
Global test acc @ epoch 123: 0.8142
Global epoch 124...
Client 0 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0006703143590129912
Local loss @ local epoch 1: 0.0005058260867372155
Local loss @ local epoch 2: 0.000438860384747386
Local loss @ local epoch 3: 0.0004327974747866392
Local loss @ local epoch 4: 0.0004490683786571026
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.94 seconds!
[tester] 
SST2Metric: acc=0.856651376146789, hinge=0.9170463392767337, ce=0.6124030203219272
Local test acc @ epoch 124: 0.8567
Client 1 execute local training on 8 samples...
Local loss @ local epoch 0: 0.00030809573945589364
Local loss @ local epoch 1: 0.00023128563771024346
Local loss @ local epoch 2: 0.00019165041157975793
Local loss @ local epoch 3: 0.000176188099430874
Local loss @ local epoch 4: 0.0001719150022836402
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.95 seconds!
[tester] 
SST2Metric: acc=0.8600917431192661, hinge=0.870378290567923, ce=0.5725845009902355
Local test acc @ epoch 124: 0.8601
Client 6 execute local training on 8 samples...
Local loss @ local epoch 0: 0.08828709274530411
Local loss @ local epoch 1: 0.06480880826711655
Local loss @ local epoch 2: 0.0510747991502285
Local loss @ local epoch 3: 0.04498688504099846
Local loss @ local epoch 4: 0.04383307322859764
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.99 seconds!
[tester] 
SST2Metric: acc=0.8543577981651376, hinge=0.9075120444144678, ce=0.5952133665618555
Local test acc @ epoch 124: 0.8544
Client 8 execute local training on 8 samples...
Local loss @ local epoch 0: 0.035315003246068954
Local loss @ local epoch 1: 0.02285635471343994
Local loss @ local epoch 2: 0.014879624359309673
Local loss @ local epoch 3: 0.009848572313785553
Local loss @ local epoch 4: 0.006676302291452885
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.07 seconds!
[tester] 
SST2Metric: acc=0.8589449541284404, hinge=0.8734826616464405, ce=0.5755212207940522
Local test acc @ epoch 124: 0.8589
Client 4 execute local training on 8 samples...
Local loss @ local epoch 0: 0.017789898440241814
Local loss @ local epoch 1: 0.015220816247165203
Local loss @ local epoch 2: 0.013137143105268478
Local loss @ local epoch 3: 0.011428785510361195
Local loss @ local epoch 4: 0.010043896734714508
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.02 seconds!
[tester] 
SST2Metric: acc=0.8600917431192661, hinge=0.8603430080031036, ce=0.5653154240227454
Local test acc @ epoch 124: 0.8601
Client 5 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0005427711876109242
Local loss @ local epoch 1: 0.00036943735904060304
Local loss @ local epoch 2: 0.00027852985658682883
Local loss @ local epoch 3: 0.0002422420948278159
Local loss @ local epoch 4: 0.00023929176677484065
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.94 seconds!
[tester] 
SST2Metric: acc=0.8577981651376146, hinge=0.8960447787144862, ce=0.5864528116266512
Local test acc @ epoch 124: 0.8578
Client 9 execute local training on 8 samples...
Local loss @ local epoch 0: 0.011343764141201973
Local loss @ local epoch 1: 0.0072355750016868114
Local loss @ local epoch 2: 0.00478358194231987
Local loss @ local epoch 3: 0.0033721239306032658
Local loss @ local epoch 4: 0.0026028857100754976
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.05 seconds!
[tester] 
SST2Metric: acc=0.8589449541284404, hinge=0.870863757412368, ce=0.5735449316893243
Local test acc @ epoch 124: 0.8589
Client 2 execute local training on 8 samples...
Local loss @ local epoch 0: 0.010844353586435318
Local loss @ local epoch 1: 0.007102071773260832
Local loss @ local epoch 2: 0.004805079661309719
Local loss @ local epoch 3: 0.0034383435267955065
Local loss @ local epoch 4: 0.002665397245436907
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.93 seconds!
[tester] 
SST2Metric: acc=0.8428899082568807, hinge=1.007238752934911, ce=0.6841612278972332
Local test acc @ epoch 124: 0.8429
Client 7 execute local training on 8 samples...
Local loss @ local epoch 0: 0.00388403725810349
Local loss @ local epoch 1: 0.002553301863372326
Local loss @ local epoch 2: 0.0017560035921633244
Local loss @ local epoch 3: 0.0013037765165790915
Local loss @ local epoch 4: 0.0010737711563706398
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.01 seconds!
[tester] 
SST2Metric: acc=0.8589449541284404, hinge=0.8704186029937289, ce=0.5736179810811428
Local test acc @ epoch 124: 0.8589
Client 3 execute local training on 8 samples...
Local loss @ local epoch 0: 0.04431524500250816
Local loss @ local epoch 1: 0.03352420777082443
Local loss @ local epoch 2: 0.028384318575263023
Local loss @ local epoch 3: 0.02738140895962715
Local loss @ local epoch 4: 0.02845236100256443
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.95 seconds!
[tester] 
SST2Metric: acc=0.8555045871559633, hinge=0.8818886367552871, ce=0.5766062017761432
Local test acc @ epoch 124: 0.8555
Global evaluate on test data...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.37 seconds!
[tester] 
SST2Metric: acc=0.8165137614678899, hinge=0.9524945895606225, ce=0.5632497595981993
Global test acc @ epoch 124: 0.8165
Global epoch 125...
Client 9 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0023606133181601763
Local loss @ local epoch 1: 0.0019185871351510286
Local loss @ local epoch 2: 0.0017463640542700887
Local loss @ local epoch 3: 0.0016399401938542724
Local loss @ local epoch 4: 0.0015138140879571438
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.01 seconds!
[tester] 
SST2Metric: acc=0.8543577981651376, hinge=0.8996349354402735, ce=0.5981668860470255
Local test acc @ epoch 125: 0.8544
Client 7 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0014551208587363362
Local loss @ local epoch 1: 0.0010836308356374502
Local loss @ local epoch 2: 0.0009209866402670741
Local loss @ local epoch 3: 0.0008970415801741183
Local loss @ local epoch 4: 0.0009379219845868647
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.06 seconds!
[tester] 
SST2Metric: acc=0.856651376146789, hinge=0.8806544736437841, ce=0.5783109719301104
Local test acc @ epoch 125: 0.8567
Client 6 execute local training on 8 samples...
Local loss @ local epoch 0: 0.061122484505176544
Local loss @ local epoch 1: 0.0504394955933094
Local loss @ local epoch 2: 0.046663496643304825
Local loss @ local epoch 3: 0.04558705911040306
Local loss @ local epoch 4: 0.04407021403312683
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.96 seconds!
[tester] 
SST2Metric: acc=0.8589449541284404, hinge=0.8994710192494436, ce=0.5905477217344648
Local test acc @ epoch 125: 0.8589
Client 8 execute local training on 8 samples...
Local loss @ local epoch 0: 0.03380003198981285
Local loss @ local epoch 1: 0.02185528725385666
Local loss @ local epoch 2: 0.014221526682376862
Local loss @ local epoch 3: 0.009412911720573902
Local loss @ local epoch 4: 0.0063835810869932175
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.99 seconds!
[tester] 
SST2Metric: acc=0.8577981651376146, hinge=0.8768708592434542, ce=0.5796920679104589
Local test acc @ epoch 125: 0.8578
Client 1 execute local training on 8 samples...
Local loss @ local epoch 0: 0.00014062125410418957
Local loss @ local epoch 1: 0.00012952425458934158
Local loss @ local epoch 2: 0.00011987178004346788
Local loss @ local epoch 3: 0.00011062163684982806
Local loss @ local epoch 4: 0.00010177366493735462
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.97 seconds!
[tester] 
SST2Metric: acc=0.8577981651376146, hinge=0.8888674575254458, ce=0.5882351190575885
Local test acc @ epoch 125: 0.8578
Client 5 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0008583569433540106
Local loss @ local epoch 1: 0.0005517667159438133
Local loss @ local epoch 2: 0.00037196287303231657
Local loss @ local epoch 3: 0.0002723522193264216
Local loss @ local epoch 4: 0.00022340675059240311
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.09 seconds!
[tester] 
SST2Metric: acc=0.856651376146789, hinge=0.8813641087724529, ce=0.5783571123301453
Local test acc @ epoch 125: 0.8567
Client 3 execute local training on 8 samples...
Local loss @ local epoch 0: 0.03109842725098133
Local loss @ local epoch 1: 0.028474029153585434
Local loss @ local epoch 2: 0.02847398817539215
Local loss @ local epoch 3: 0.027829904109239578
Local loss @ local epoch 4: 0.026539314538240433
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.1 seconds!
[tester] 
SST2Metric: acc=0.8589449541284404, hinge=0.8770246556319228, ce=0.5754282839323657
Local test acc @ epoch 125: 0.8589
Client 0 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0017816396430134773
Local loss @ local epoch 1: 0.001153229153715074
Local loss @ local epoch 2: 0.0007819884922355413
Local loss @ local epoch 3: 0.0005741548957303166
Local loss @ local epoch 4: 0.00046910485252738
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.91 seconds!
[tester] 
SST2Metric: acc=0.8532110091743119, hinge=0.9019804482066304, ce=0.599275727744306
Local test acc @ epoch 125: 0.8532
Client 4 execute local training on 8 samples...
Local loss @ local epoch 0: 0.02071790024638176
Local loss @ local epoch 1: 0.016707200556993484
Local loss @ local epoch 2: 0.014735925011336803
Local loss @ local epoch 3: 0.013362023048102856
Local loss @ local epoch 4: 0.01203601062297821
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.05 seconds!
[tester] 
SST2Metric: acc=0.8612385321100917, hinge=0.8619245396021309, ce=0.568840067641016
Local test acc @ epoch 125: 0.8612
Client 2 execute local training on 8 samples...
Local loss @ local epoch 0: 0.016132300719618797
Local loss @ local epoch 1: 0.01048120204359293
Local loss @ local epoch 2: 0.006949147675186396
Local loss @ local epoch 3: 0.004771563224494457
Local loss @ local epoch 4: 0.003445591777563095
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.01 seconds!
[tester] 
SST2Metric: acc=0.8474770642201835, hinge=0.9708089334975689, ce=0.65649551023464
Local test acc @ epoch 125: 0.8475
Global evaluate on test data...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.43 seconds!
[tester] 
SST2Metric: acc=0.8142201834862385, hinge=0.9579386033049417, ce=0.5686269871401387
Global test acc @ epoch 125: 0.8142
Global epoch 126...
Client 6 execute local training on 8 samples...
Local loss @ local epoch 0: 0.06857139617204666
Local loss @ local epoch 1: 0.05062112957239151
Local loss @ local epoch 2: 0.040898438543081284
Local loss @ local epoch 3: 0.03725765272974968
Local loss @ local epoch 4: 0.03705042228102684
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.05 seconds!
[tester] 
SST2Metric: acc=0.8555045871559633, hinge=0.89066855281318, ce=0.5838033772108931
Local test acc @ epoch 126: 0.8555
Client 9 execute local training on 8 samples...
Local loss @ local epoch 0: 0.009214039891958237
Local loss @ local epoch 1: 0.005891080014407635
Local loss @ local epoch 2: 0.0039254650473594666
Local loss @ local epoch 3: 0.0028117098845541477
Local loss @ local epoch 4: 0.0022238055244088173
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.97 seconds!
[tester] 
SST2Metric: acc=0.8589449541284404, hinge=0.8799833373739085, ce=0.5812128333472583
Local test acc @ epoch 126: 0.8589
Client 4 execute local training on 8 samples...
Local loss @ local epoch 0: 0.012604226358234882
Local loss @ local epoch 1: 0.010883789509534836
Local loss @ local epoch 2: 0.009517153725028038
Local loss @ local epoch 3: 0.008378678001463413
Local loss @ local epoch 4: 0.007444337010383606
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.97 seconds!
[tester] 
SST2Metric: acc=0.8600917431192661, hinge=0.8677970839476367, ce=0.5720644337372105
Local test acc @ epoch 126: 0.8601
Client 0 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0014944660943001509
Local loss @ local epoch 1: 0.0011000533122569323
Local loss @ local epoch 2: 0.0009107384830713272
Local loss @ local epoch 3: 0.0008293458959087729
Local loss @ local epoch 4: 0.0007826280780136585
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.99 seconds!
[tester] 
SST2Metric: acc=0.8600917431192661, hinge=0.9385425818896075, ce=0.630067516912402
Local test acc @ epoch 126: 0.8601
Client 5 execute local training on 8 samples...
Local loss @ local epoch 0: 0.001891814754344523
Local loss @ local epoch 1: 0.0011766623938456178
Local loss @ local epoch 2: 0.0007515309262089431
Local loss @ local epoch 3: 0.0005010116146877408
Local loss @ local epoch 4: 0.0003543276689015329
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.95 seconds!
[tester] 
SST2Metric: acc=0.8600917431192661, hinge=0.8588335728700008, ce=0.5658667763993368
Local test acc @ epoch 126: 0.8601
Client 8 execute local training on 8 samples...
Local loss @ local epoch 0: 0.01599276438355446
Local loss @ local epoch 1: 0.010245462879538536
Local loss @ local epoch 2: 0.006651913281530142
Local loss @ local epoch 3: 0.004427432082593441
Local loss @ local epoch 4: 0.00304940203204751
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.08 seconds!
[tester] 
SST2Metric: acc=0.856651376146789, hinge=0.9292630910053166, ce=0.6234588516263185
Local test acc @ epoch 126: 0.8567
Client 1 execute local training on 8 samples...
Local loss @ local epoch 0: 0.00015473489474970847
Local loss @ local epoch 1: 0.00012406652967911214
Local loss @ local epoch 2: 0.00010871444828808308
Local loss @ local epoch 3: 0.00010320373257854953
Local loss @ local epoch 4: 0.00010119203943759203
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.0 seconds!
[tester] 
SST2Metric: acc=0.8600917431192661, hinge=0.8734870927596311, ce=0.5765903918135996
Local test acc @ epoch 126: 0.8601
Client 3 execute local training on 8 samples...
Local loss @ local epoch 0: 0.03655761107802391
Local loss @ local epoch 1: 0.03018612042069435
Local loss @ local epoch 2: 0.028657035902142525
Local loss @ local epoch 3: 0.029311390593647957
Local loss @ local epoch 4: 0.029533594846725464
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.95 seconds!
[tester] 
SST2Metric: acc=0.8600917431192661, hinge=0.8678760084263776, ce=0.5694300763550911
Local test acc @ epoch 126: 0.8601
Client 2 execute local training on 8 samples...
Local loss @ local epoch 0: 0.03883320838212967
Local loss @ local epoch 1: 0.025270864367485046
Local loss @ local epoch 2: 0.016623174771666527
Local loss @ local epoch 3: 0.01116208452731371
Local loss @ local epoch 4: 0.0077077168971300125
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.91 seconds!
[tester] 
SST2Metric: acc=0.8577981651376146, hinge=0.9086594280846622, ce=0.6051015717549081
Local test acc @ epoch 126: 0.8578
Client 7 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0012629871489480138
Local loss @ local epoch 1: 0.0010370585368946195
Local loss @ local epoch 2: 0.0009769329335540533
Local loss @ local epoch 3: 0.000980533892288804
Local loss @ local epoch 4: 0.0009702015086077154
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.91 seconds!
[tester] 
SST2Metric: acc=0.8623853211009175, hinge=0.8637905556947814, ce=0.5708307314627011
Local test acc @ epoch 126: 0.8624
Global evaluate on test data...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.25 seconds!
[tester] 
SST2Metric: acc=0.8176605504587156, hinge=0.9465929230145358, ce=0.5606590072793134
Global test acc @ epoch 126: 0.8177
Global epoch 127...
Client 5 execute local training on 8 samples...
Local loss @ local epoch 0: 0.00047367086517624557
Local loss @ local epoch 1: 0.0003224777174182236
Local loss @ local epoch 2: 0.00024102807219605893
Local loss @ local epoch 3: 0.00020649652287829667
Local loss @ local epoch 4: 0.00020158510596957058
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.95 seconds!
[tester] 
SST2Metric: acc=0.8577981651376146, hinge=0.897279419209979, ce=0.589419881773711
Local test acc @ epoch 127: 0.8578
Client 7 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0027773650363087654
Local loss @ local epoch 1: 0.0018629877595230937
Local loss @ local epoch 2: 0.0013218863168731332
Local loss @ local epoch 3: 0.0010290510253980756
Local loss @ local epoch 4: 0.0009003448649309576
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.95 seconds!
[tester] 
SST2Metric: acc=0.8589449541284404, hinge=0.8723114399461571, ce=0.577184375810932
Local test acc @ epoch 127: 0.8589
Client 2 execute local training on 8 samples...
Local loss @ local epoch 0: 0.010309291072189808
Local loss @ local epoch 1: 0.006754457950592041
Local loss @ local epoch 2: 0.004591906443238258
Local loss @ local epoch 3: 0.0033236697781831026
Local loss @ local epoch 4: 0.0026250132359564304
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.98 seconds!
[tester] 
SST2Metric: acc=0.841743119266055, hinge=1.0001333068817033, ce=0.6805480939447998
Local test acc @ epoch 127: 0.8417
Client 3 execute local training on 8 samples...
Local loss @ local epoch 0: 0.17334096133708954
Local loss @ local epoch 1: 0.1308407336473465
Local loss @ local epoch 2: 0.09610337764024734
Local loss @ local epoch 3: 0.06941336393356323
Local loss @ local epoch 4: 0.05031394585967064
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.02 seconds!
[tester] 
SST2Metric: acc=0.8600917431192661, hinge=0.8697185288055227, ce=0.5748046621192436
Local test acc @ epoch 127: 0.8601
Client 1 execute local training on 8 samples...
Local loss @ local epoch 0: 0.00012019842688459903
Local loss @ local epoch 1: 0.00010735997057054192
Local loss @ local epoch 2: 0.00010029895202023908
Local loss @ local epoch 3: 9.501027670921758e-05
Local loss @ local epoch 4: 8.933457866078243e-05
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.97 seconds!
[tester] 
SST2Metric: acc=0.8589449541284404, hinge=0.8984370916535002, ce=0.5968657948551064
Local test acc @ epoch 127: 0.8589
Client 9 execute local training on 8 samples...
Local loss @ local epoch 0: 0.002688071923330426
Local loss @ local epoch 1: 0.0023019404616206884
Local loss @ local epoch 2: 0.002083210274577141
Local loss @ local epoch 3: 0.0018779997481033206
Local loss @ local epoch 4: 0.0016742669977247715
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.95 seconds!
[tester] 
SST2Metric: acc=0.8600917431192661, hinge=0.8843843655301891, ce=0.586672554894603
Local test acc @ epoch 127: 0.8601
Client 0 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0005196916172280908
Local loss @ local epoch 1: 0.00044373871060088277
Local loss @ local epoch 2: 0.0004235985688865185
Local loss @ local epoch 3: 0.00041617362876422703
Local loss @ local epoch 4: 0.00040128588443621993
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.98 seconds!
[tester] 
SST2Metric: acc=0.8555045871559633, hinge=0.9163799886036357, ce=0.6137960320313909
Local test acc @ epoch 127: 0.8555
Client 8 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0015850868076086044
Local loss @ local epoch 1: 0.001105892239138484
Local loss @ local epoch 2: 0.0008522706339135766
Local loss @ local epoch 3: 0.0007460273918695748
Local loss @ local epoch 4: 0.0007261107093654573
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.98 seconds!
[tester] 
SST2Metric: acc=0.8463302752293578, hinge=1.0531466790568937, ce=0.7242268888308699
Local test acc @ epoch 127: 0.8463
Client 4 execute local training on 8 samples...
Local loss @ local epoch 0: 0.12360793352127075
Local loss @ local epoch 1: 0.08412513136863708
Local loss @ local epoch 2: 0.05740782991051674
Local loss @ local epoch 3: 0.0403616726398468
Local loss @ local epoch 4: 0.03011152520775795
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.92 seconds!
[tester] 
SST2Metric: acc=0.8555045871559633, hinge=0.9018036922730437, ce=0.601719674795018
Local test acc @ epoch 127: 0.8555
Client 6 execute local training on 8 samples...
Local loss @ local epoch 0: 0.17443130910396576
Local loss @ local epoch 1: 0.12698683142662048
Local loss @ local epoch 2: 0.09187422692775726
Local loss @ local epoch 3: 0.06793773919343948
Local loss @ local epoch 4: 0.053330253809690475
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.02 seconds!
[tester] 
SST2Metric: acc=0.8555045871559633, hinge=0.9030644602458412, ce=0.5964298491091724
Local test acc @ epoch 127: 0.8555
Global evaluate on test data...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.53 seconds!
[tester] 
SST2Metric: acc=0.8096330275229358, hinge=1.0103538364445397, ce=0.6107035983244914
Global test acc @ epoch 127: 0.8096
Global epoch 128...
Client 2 execute local training on 8 samples...
Local loss @ local epoch 0: 0.006563297938555479
Local loss @ local epoch 1: 0.004407653119415045
Local loss @ local epoch 2: 0.0031518531031906605
Local loss @ local epoch 3: 0.002484528347849846
Local loss @ local epoch 4: 0.002194327302277088
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.98 seconds!
[tester] 
SST2Metric: acc=0.8440366972477065, hinge=1.0355880387605878, ce=0.7083113363656894
Local test acc @ epoch 128: 0.844
Client 4 execute local training on 8 samples...
Local loss @ local epoch 0: 0.07121363282203674
Local loss @ local epoch 1: 0.04730970412492752
Local loss @ local epoch 2: 0.031921256333589554
Local loss @ local epoch 3: 0.022519519552588463
Local loss @ local epoch 4: 0.017102280631661415
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.05 seconds!
[tester] 
SST2Metric: acc=0.8589449541284404, hinge=0.8870645330859981, ce=0.5879985846590022
Local test acc @ epoch 128: 0.8589
Client 3 execute local training on 8 samples...
Local loss @ local epoch 0: 0.05546995997428894
Local loss @ local epoch 1: 0.04008598253130913
Local loss @ local epoch 2: 0.030987856909632683
Local loss @ local epoch 3: 0.02690698206424713
Local loss @ local epoch 4: 0.026346702128648758
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.95 seconds!
[tester] 
SST2Metric: acc=0.8555045871559633, hinge=0.8865546610103835, ce=0.5825360788629441
Local test acc @ epoch 128: 0.8555
Client 9 execute local training on 8 samples...
Local loss @ local epoch 0: 0.006344234570860863
Local loss @ local epoch 1: 0.0041058249771595
Local loss @ local epoch 2: 0.0028138807974755764
Local loss @ local epoch 3: 0.002118317410349846
Local loss @ local epoch 4: 0.0017902364488691092
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.93 seconds!
[tester] 
SST2Metric: acc=0.8555045871559633, hinge=0.8928385521840612, ce=0.5923716303814467
Local test acc @ epoch 128: 0.8555
Client 8 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0038874796591699123
Local loss @ local epoch 1: 0.002527036936953664
Local loss @ local epoch 2: 0.0017119200201705098
Local loss @ local epoch 3: 0.0012435505632311106
Local loss @ local epoch 4: 0.000992887420579791
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.91 seconds!
[tester] 
SST2Metric: acc=0.841743119266055, hinge=1.0317576349875248, ce=0.7055364861033413
Local test acc @ epoch 128: 0.8417
Client 0 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0009664727840572596
Local loss @ local epoch 1: 0.0006591230630874634
Local loss @ local epoch 2: 0.00048821611562743783
Local loss @ local epoch 3: 0.00040810558130033314
Local loss @ local epoch 4: 0.0003858859126921743
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.93 seconds!
[tester] 
SST2Metric: acc=0.856651376146789, hinge=0.8951208715865372, ce=0.5955162236951356
Local test acc @ epoch 128: 0.8567
Client 5 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0005990325007587671
Local loss @ local epoch 1: 0.00039476004894822836
Local loss @ local epoch 2: 0.0002738601469900459
Local loss @ local epoch 3: 0.0002079180267173797
Local loss @ local epoch 4: 0.00017800292698666453
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.97 seconds!
[tester] 
SST2Metric: acc=0.8543577981651376, hinge=0.9090093770705232, ce=0.6000115660095583
Local test acc @ epoch 128: 0.8544
Client 7 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0012279843213036656
Local loss @ local epoch 1: 0.0009291025344282389
Local loss @ local epoch 2: 0.0008034492493607104
Local loss @ local epoch 3: 0.0007931977743282914
Local loss @ local epoch 4: 0.0008316771709360182
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.0 seconds!
[tester] 
SST2Metric: acc=0.8612385321100917, hinge=0.8881542394194034, ce=0.5897371060327881
Local test acc @ epoch 128: 0.8612
Client 1 execute local training on 8 samples...
Local loss @ local epoch 0: 0.00016628412413410842
Local loss @ local epoch 1: 0.00015204434748739004
Local loss @ local epoch 2: 0.0001396961451973766
Local loss @ local epoch 3: 0.00012801869888789952
Local loss @ local epoch 4: 0.00011692215775838122
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.01 seconds!
[tester] 
SST2Metric: acc=0.8600917431192661, hinge=0.8891832899609837, ce=0.5908119691879736
Local test acc @ epoch 128: 0.8601
Client 6 execute local training on 8 samples...
Local loss @ local epoch 0: 0.1001129150390625
Local loss @ local epoch 1: 0.07124729454517365
Local loss @ local epoch 2: 0.052345119416713715
Local loss @ local epoch 3: 0.04155505448579788
Local loss @ local epoch 4: 0.03681924194097519
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.95 seconds!
[tester] 
SST2Metric: acc=0.8555045871559633, hinge=0.8916020747445045, ce=0.5871961579095774
Local test acc @ epoch 128: 0.8555
Global evaluate on test data...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 78.41 seconds!
[tester] 
SST2Metric: acc=0.8130733944954128, hinge=0.9933560906200234, ce=0.5977456230422431
Global test acc @ epoch 128: 0.8131
Global epoch 129...
Client 3 execute local training on 8 samples...
Local loss @ local epoch 0: 0.052016738802194595
Local loss @ local epoch 1: 0.037580251693725586
Local loss @ local epoch 2: 0.029168298467993736
Local loss @ local epoch 3: 0.0255165696144104
Local loss @ local epoch 4: 0.02515537105500698
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.91 seconds!
[tester] 
SST2Metric: acc=0.8577981651376146, hinge=0.8992576722158204, ce=0.591856002065069
Local test acc @ epoch 129: 0.8578
Client 4 execute local training on 8 samples...
Local loss @ local epoch 0: 0.03315671533346176
Local loss @ local epoch 1: 0.021610237658023834
Local loss @ local epoch 2: 0.014976974576711655
Local loss @ local epoch 3: 0.011533236131072044
Local loss @ local epoch 4: 0.010000239126384258
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.98 seconds!
[tester] 
SST2Metric: acc=0.8600917431192661, hinge=0.8768731921637823, ce=0.5796664190842288
Local test acc @ epoch 129: 0.8601
Client 5 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0005184556357562542
Local loss @ local epoch 1: 0.00034595801844261587
Local loss @ local epoch 2: 0.00024981467868201435
Local loss @ local epoch 3: 0.00020418928761500865
Local loss @ local epoch 4: 0.00019114775932393968
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.97 seconds!
[tester] 
SST2Metric: acc=0.8589449541284404, hinge=0.8975229024066838, ce=0.5900921730419146
Local test acc @ epoch 129: 0.8589
Client 0 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0033333732280880213
Local loss @ local epoch 1: 0.002117109252139926
Local loss @ local epoch 2: 0.0013780143344774842
Local loss @ local epoch 3: 0.0009365992737002671
Local loss @ local epoch 4: 0.0006775693036615849
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.95 seconds!
[tester] 
SST2Metric: acc=0.8589449541284404, hinge=0.8822182185879541, ce=0.5844465454110749
Local test acc @ epoch 129: 0.8589
Client 9 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0018740921514108777
Local loss @ local epoch 1: 0.001653508748859167
Local loss @ local epoch 2: 0.0014684179332107306
Local loss @ local epoch 3: 0.0013017633464187384
Local loss @ local epoch 4: 0.0011579938000068069
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 77.01 seconds!
[tester] 
SST2Metric: acc=0.8577981651376146, hinge=0.8985421452500405, ce=0.5989903435687115
Local test acc @ epoch 129: 0.8578
Client 6 execute local training on 8 samples...
Local loss @ local epoch 0: 0.16413813829421997
Local loss @ local epoch 1: 0.11957850307226181
Local loss @ local epoch 2: 0.08707248419523239
Local loss @ local epoch 3: 0.06543247401714325
Local loss @ local epoch 4: 0.052816107869148254
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.99 seconds!
[tester] 
SST2Metric: acc=0.856651376146789, hinge=0.9104820750995514, ce=0.6019391532661073
Local test acc @ epoch 129: 0.8567
Client 7 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0013501645298674703
Local loss @ local epoch 1: 0.0009898769203573465
Local loss @ local epoch 2: 0.0008182256133295596
Local loss @ local epoch 3: 0.0007767231436446309
Local loss @ local epoch 4: 0.0008071264601312578
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 76.96 seconds!
[tester] 
SST2Metric: acc=0.8623853211009175, hinge=0.8880648414749618, ce=0.5903426360700988
Local test acc @ epoch 129: 0.8624
Client 8 execute local training on 8 samples...
Local loss @ local epoch 0: 0.003149808384478092
Local loss @ local epoch 1: 0.0020445119589567184
Local loss @ local epoch 2: 0.0013815558049827814
Local loss @ local epoch 3: 0.000999846262857318
Local loss @ local epoch 4: 0.0007951451698318124
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
