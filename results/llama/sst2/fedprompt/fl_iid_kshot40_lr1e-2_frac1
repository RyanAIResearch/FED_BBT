Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:13<00:13, 13.56s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:18<00:00,  8.42s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:18<00:00,  9.19s/it]
Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at meta-llama/Llama-2-7b-hf and are newly initialized: ['score.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Found cached dataset glue (/root/.cache/huggingface/datasets/glue/sst2/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)
Map:   0%|          | 0/67349 [00:00<?, ? examples/s]Map:   1%|          | 587/67349 [00:00<00:11, 5817.45 examples/s]Map:   2%|▏         | 1320/67349 [00:00<00:12, 5176.95 examples/s]Map:   3%|▎         | 1902/67349 [00:00<00:12, 5428.02 examples/s]Map:   4%|▍         | 2748/67349 [00:00<00:11, 5520.21 examples/s]Map:   5%|▌         | 3598/67349 [00:00<00:11, 5575.89 examples/s]Map:   6%|▌         | 4159/67349 [00:00<00:11, 5582.59 examples/s]Map:   7%|▋         | 4755/67349 [00:00<00:11, 5685.89 examples/s]Map:   8%|▊         | 5619/67349 [00:01<00:10, 5711.57 examples/s]Map:  10%|▉         | 6475/67349 [00:01<00:10, 5693.53 examples/s]Map:  11%|█         | 7189/67349 [00:01<00:11, 5327.91 examples/s]Map:  12%|█▏        | 7779/67349 [00:01<00:10, 5459.97 examples/s]Map:  13%|█▎        | 8618/67349 [00:01<00:10, 5502.10 examples/s]Map:  14%|█▎        | 9211/67349 [00:01<00:11, 4970.14 examples/s]Map:  15%|█▍        | 9802/67349 [00:01<00:11, 5188.52 examples/s]Map:  16%|█▌        | 10547/67349 [00:01<00:11, 5104.10 examples/s]Map:  16%|█▋        | 11087/67349 [00:02<00:10, 5174.85 examples/s]Map:  18%|█▊        | 11863/67349 [00:02<00:10, 5169.13 examples/s]Map:  19%|█▉        | 12649/67349 [00:02<00:10, 5189.82 examples/s]Map:  20%|█▉        | 13198/67349 [00:02<00:10, 5258.39 examples/s]Map:  20%|██        | 13761/67349 [00:02<00:10, 5350.15 examples/s]Map:  21%|██        | 14306/67349 [00:02<00:09, 5375.13 examples/s]Map:  22%|██▏       | 14905/67349 [00:02<00:09, 5542.06 examples/s]Map:  23%|██▎       | 15702/67349 [00:02<00:09, 5454.18 examples/s]Map:  24%|██▍       | 16357/67349 [00:03<00:10, 5053.18 examples/s]Map:  25%|██▌       | 16953/67349 [00:03<00:09, 5274.11 examples/s]Map:  26%|██▌       | 17512/67349 [00:03<00:09, 5354.83 examples/s]Map:  27%|██▋       | 18326/67349 [00:03<00:09, 5377.03 examples/s]Map:  28%|██▊       | 18923/67349 [00:03<00:08, 5524.96 examples/s]Map:  29%|██▉       | 19488/67349 [00:03<00:08, 5555.37 examples/s]Map:  30%|██▉       | 20050/67349 [00:03<00:08, 5569.19 examples/s]Map:  31%|███       | 20624/67349 [00:03<00:08, 5615.40 examples/s]Map:  32%|███▏      | 21380/67349 [00:03<00:08, 5394.27 examples/s]Map:  33%|███▎      | 21975/67349 [00:04<00:08, 5537.79 examples/s]Map:  34%|███▍      | 22762/67349 [00:04<00:08, 5430.15 examples/s]Map:  35%|███▍      | 23547/67349 [00:04<00:08, 5360.94 examples/s]Map:  36%|███▌      | 24108/67349 [00:04<00:07, 5420.94 examples/s]Map:  37%|███▋      | 24706/67349 [00:04<00:07, 5562.88 examples/s]Map:  38%|███▊      | 25298/67349 [00:04<00:07, 5572.78 examples/s]Map:  38%|███▊      | 25866/67349 [00:04<00:08, 5174.16 examples/s]Map:  40%|███▉      | 26687/67349 [00:04<00:07, 5276.62 examples/s]Map:  41%|████      | 27519/67349 [00:05<00:07, 5364.86 examples/s]Map:  42%|████▏     | 28298/67349 [00:05<00:07, 5303.89 examples/s]Map:  43%|████▎     | 28864/67349 [00:05<00:07, 5386.66 examples/s]Map:  44%|████▍     | 29585/67349 [00:05<00:07, 5189.66 examples/s]Map:  45%|████▍     | 30109/67349 [00:05<00:07, 5199.25 examples/s]Map:  46%|████▌     | 30702/67349 [00:05<00:06, 5386.31 examples/s]Map:  47%|████▋     | 31350/67349 [00:05<00:07, 5005.01 examples/s]Map:  47%|████▋     | 31926/67349 [00:05<00:06, 5121.48 examples/s]Map:  48%|████▊     | 32458/67349 [00:06<00:06, 5171.71 examples/s]Map:  49%|████▉     | 33000/67349 [00:06<00:06, 5018.26 examples/s]Map:  50%|█████     | 33734/67349 [00:06<00:06, 4970.56 examples/s]Map:  51%|█████     | 34298/67349 [00:06<00:06, 5103.08 examples/s]Map:  52%|█████▏    | 34897/67349 [00:06<00:06, 5337.60 examples/s]Map:  53%|█████▎    | 35650/67349 [00:06<00:06, 5219.87 examples/s]Map:  54%|█████▍    | 36398/67349 [00:06<00:06, 5136.55 examples/s]Map:  55%|█████▍    | 36961/67349 [00:06<00:05, 5249.55 examples/s]Map:  56%|█████▌    | 37773/67349 [00:07<00:05, 5303.59 examples/s]Map:  57%|█████▋    | 38545/67349 [00:07<00:05, 5248.49 examples/s]Map:  58%|█████▊    | 39077/67349 [00:07<00:06, 4666.58 examples/s]Map:  59%|█████▉    | 39730/67349 [00:07<00:06, 4566.98 examples/s]Map:  60%|██████    | 40463/67349 [00:07<00:05, 4662.98 examples/s]Map:  61%|██████    | 41145/67349 [00:07<00:05, 4624.53 examples/s]Map:  62%|██████▏   | 41704/67349 [00:07<00:05, 4836.21 examples/s]Map:  63%|██████▎   | 42293/67349 [00:08<00:05, 4995.11 examples/s]Map:  64%|██████▍   | 43000/67349 [00:08<00:05, 4809.10 examples/s]Map:  65%|██████▍   | 43509/67349 [00:08<00:04, 4873.49 examples/s]Map:  65%|██████▌   | 44049/67349 [00:08<00:04, 5005.45 examples/s]Map:  66%|██████▌   | 44604/67349 [00:08<00:04, 5150.17 examples/s]Map:  67%|██████▋   | 45368/67349 [00:08<00:04, 5117.23 examples/s]Map:  68%|██████▊   | 45968/67349 [00:08<00:04, 5341.66 examples/s]Map:  69%|██████▉   | 46685/67349 [00:08<00:04, 5136.12 examples/s]Map:  70%|███████   | 47212/67349 [00:09<00:03, 5166.52 examples/s]Map:  71%|███████   | 47795/67349 [00:09<00:03, 5338.94 examples/s]Map:  72%|███████▏  | 48356/67349 [00:09<00:03, 5411.06 examples/s]Map:  73%|███████▎  | 48956/67349 [00:09<00:03, 5574.39 examples/s]Map:  74%|███████▍  | 49820/67349 [00:09<00:03, 5640.55 examples/s]Map:  75%|███████▌  | 50608/67349 [00:09<00:03, 5500.62 examples/s]Map:  76%|███████▋  | 51403/67349 [00:09<00:02, 5431.35 examples/s]Map:  77%|███████▋  | 52000/67349 [00:09<00:02, 5472.35 examples/s]Map:  78%|███████▊  | 52604/67349 [00:09<00:02, 5612.87 examples/s]Map:  79%|███████▉  | 53433/67349 [00:10<00:02, 5580.77 examples/s]Map:  80%|████████  | 54000/67349 [00:10<00:02, 5578.51 examples/s]Map:  81%|████████▏ | 54774/67349 [00:10<00:02, 5430.03 examples/s]Map:  82%|████████▏ | 55526/67349 [00:10<00:02, 5266.10 examples/s]Map:  83%|████████▎ | 56083/67349 [00:10<00:02, 5336.40 examples/s]Map:  84%|████████▍ | 56681/67349 [00:10<00:01, 5499.24 examples/s]Map:  85%|████████▌ | 57486/67349 [00:10<00:01, 5448.95 examples/s]Map:  87%|████████▋ | 58296/67349 [00:11<00:01, 5390.44 examples/s]Map:  88%|████████▊ | 59045/67349 [00:11<00:01, 5260.74 examples/s]Map:  89%|████████▉ | 59801/67349 [00:11<00:01, 5188.18 examples/s]Map:  90%|████████▉ | 60349/67349 [00:11<00:01, 5254.09 examples/s]Map:  90%|█████████ | 60949/67349 [00:11<00:01, 5436.83 examples/s]Map:  92%|█████████▏| 61685/67349 [00:11<00:01, 5219.78 examples/s]Map:  93%|█████████▎| 62448/67349 [00:11<00:00, 5173.38 examples/s]Map:  94%|█████████▎| 63000/67349 [00:11<00:00, 5094.99 examples/s]Map:  95%|█████████▍| 63748/67349 [00:12<00:00, 5038.56 examples/s]Map:  96%|█████████▌| 64515/67349 [00:12<00:00, 5057.60 examples/s]Map:  97%|█████████▋| 65044/67349 [00:12<00:00, 4503.31 examples/s]Map:  97%|█████████▋| 65613/67349 [00:12<00:00, 4772.26 examples/s]Map:  98%|█████████▊| 66171/67349 [00:12<00:00, 4969.32 examples/s]Map:  99%|█████████▉| 66745/67349 [00:12<00:00, 5157.69 examples/s]Map: 100%|██████████| 67349/67349 [00:12<00:00, 4744.78 examples/s]                                                                   Example in train set:
{'sentence': 'hide new secretions from the parental units ', 'label': 0, 'idx': 0, 'input_text': 'iedER statfigme von interroidater their bet ein}\\"> sub op donty try Pro tra sameep two nameoldlet simsp avbreblemey could cor accayscreurrsi constues}$View act bo ко som aboutland . hide new secretions from the parental units  . The sentiment is', 'target_text': 'negative'}
Map:   0%|          | 0/67349 [00:00<?, ? examples/s]Map:   3%|▎         | 2000/67349 [00:00<00:04, 13999.45 examples/s]Map:   6%|▌         | 4000/67349 [00:00<00:04, 13567.29 examples/s]Map:   9%|▉         | 6000/67349 [00:00<00:04, 14268.31 examples/s]Map:  12%|█▏        | 8000/67349 [00:00<00:06, 9846.72 examples/s] Map:  15%|█▍        | 10000/67349 [00:00<00:04, 11692.78 examples/s]Map:  18%|█▊        | 12000/67349 [00:00<00:04, 12366.34 examples/s]Map:  21%|██        | 14000/67349 [00:01<00:04, 11431.04 examples/s]Map:  24%|██▍       | 16000/67349 [00:01<00:05, 8929.46 examples/s] Map:  27%|██▋       | 18000/67349 [00:01<00:04, 10051.07 examples/s]Map:  30%|██▉       | 20000/67349 [00:01<00:04, 10538.10 examples/s]Map:  33%|███▎      | 22000/67349 [00:01<00:04, 10970.01 examples/s]Map:  36%|███▌      | 24000/67349 [00:02<00:03, 12286.41 examples/s]Map:  39%|███▊      | 26000/67349 [00:02<00:05, 7922.00 examples/s] Map:  42%|████▏     | 28000/67349 [00:02<00:04, 8426.06 examples/s]Map:  45%|████▍     | 30000/67349 [00:02<00:04, 9289.12 examples/s]Map:  48%|████▊     | 32000/67349 [00:03<00:03, 10063.89 examples/s]Map:  50%|█████     | 34000/67349 [00:03<00:03, 10956.79 examples/s]Map:  53%|█████▎    | 36000/67349 [00:03<00:03, 8538.07 examples/s] Map:  56%|█████▋    | 38000/67349 [00:03<00:03, 9567.58 examples/s]Map:  59%|█████▉    | 40000/67349 [00:03<00:02, 10571.28 examples/s]Map:  62%|██████▏   | 42000/67349 [00:04<00:02, 11018.72 examples/s]Map:  65%|██████▌   | 44000/67349 [00:04<00:02, 11070.36 examples/s]Map:  68%|██████▊   | 46000/67349 [00:04<00:02, 10010.14 examples/s]Map:  71%|███████▏  | 48000/67349 [00:04<00:01, 10963.40 examples/s]Map:  74%|███████▍  | 50000/67349 [00:04<00:01, 11892.76 examples/s]Map:  77%|███████▋  | 52000/67349 [00:04<00:01, 11393.73 examples/s]Map:  80%|████████  | 54000/67349 [00:05<00:01, 10768.07 examples/s]Map:  83%|████████▎ | 56000/67349 [00:05<00:01, 7762.79 examples/s] Map:  86%|████████▌ | 58000/67349 [00:05<00:01, 8253.31 examples/s]Map:  89%|████████▉ | 60000/67349 [00:05<00:00, 9328.99 examples/s]Map:  94%|█████████▎| 63000/67349 [00:06<00:00, 11425.67 examples/s]Map:  97%|█████████▋| 65000/67349 [00:06<00:00, 8545.85 examples/s] Map:  99%|█████████▉| 67000/67349 [00:06<00:00, 9965.59 examples/s]                                                                   Found cached dataset glue (/root/.cache/huggingface/datasets/glue/sst2/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)
Map:   0%|          | 0/872 [00:00<?, ? examples/s]Map:  68%|██████▊   | 589/872 [00:00<00:00, 5840.19 examples/s]                                                               Example in validation set:
{'sentence': "it 's a charming and often affecting journey . ", 'label': 1, 'idx': 0, 'input_text': 'iedER statfigme von interroidater their bet ein}\\"> sub op donty try Pro tra sameep two nameoldlet simsp avbreblemey could cor accayscreurrsi constues}$View act bo ко som aboutland . it \'s a charming and often affecting journey .  . The sentiment is', 'target_text': 'positive'}
Map:   0%|          | 0/872 [00:00<?, ? examples/s]Map: 100%|██████████| 872/872 [00:00<00:00, 7278.14 examples/s]                                                               92
92
# of train data: 80
Example:
+------------------------------+------------------------------+--------+
| input_ids                    | attention_mask               | labels |
+------------------------------+------------------------------+--------+
| [1, 474, 287, 1001, 1002,... | [1, 1, 1, 1, 1, 1, 1, 1, ... | 8178   |
+------------------------------+------------------------------+--------+

# of dev data: 80
Example:
+------------------------------+------------------------------+--------+
| input_ids                    | attention_mask               | labels |
+------------------------------+------------------------------+--------+
| [1, 474, 287, 1001, 1002,... | [1, 1, 1, 1, 1, 1, 1, 1, ... | 6374   |
+------------------------------+------------------------------+--------+

# of test data: 872
Example:
+------------------------------+------------------------------+--------+
| input_ids                    | attention_mask               | labels |
+------------------------------+------------------------------+--------+
| [1, 474, 287, 1001, 1002,... | [1, 1, 1, 1, 1, 1, 1, 1, ... | 6374   |
+------------------------------+------------------------------+--------+
Global epoch 0...
Client 4 execute local training on 8 samples...
Local loss @ local epoch 0: 0.5840384364128113
Local loss @ local epoch 1: 1.4068647623062134
Local loss @ local epoch 2: 0.7344849109649658
Local loss @ local epoch 3: 0.5866348743438721
Local loss @ local epoch 4: 0.32996752858161926
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 256.91 seconds!
[tester] 
SST2Metric: acc=0.47591743119266056, hinge=2.0775542609188533, ce=0.9208731265789872
Local test acc @ epoch 0: 0.4759
Client 6 execute local training on 8 samples...
Local loss @ local epoch 0: 0.5573786497116089
Local loss @ local epoch 1: 0.6424747705459595
Local loss @ local epoch 2: 0.6805461049079895
Local loss @ local epoch 3: 0.6781517267227173
Local loss @ local epoch 4: 0.6743118762969971
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 258.77 seconds!
[tester] 
SST2Metric: acc=0.4908256880733945, hinge=2.014633487123962, ce=0.777940065215487
Local test acc @ epoch 0: 0.4908
Client 8 execute local training on 8 samples...
Local loss @ local epoch 0: 1.232194185256958
Local loss @ local epoch 1: 0.7077224850654602
Local loss @ local epoch 2: 0.833031177520752
Local loss @ local epoch 3: 0.7534984350204468
Local loss @ local epoch 4: 0.6857463121414185
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 259.09 seconds!
[tester] 
SST2Metric: acc=0.5068807339449541, hinge=1.981975619946051, ce=0.8746339829143034
Local test acc @ epoch 0: 0.5069
Client 2 execute local training on 8 samples...
Local loss @ local epoch 0: 0.8454112410545349
Local loss @ local epoch 1: 0.5135868191719055
Local loss @ local epoch 2: 0.5569329857826233
Local loss @ local epoch 3: 1.4661986827850342
Local loss @ local epoch 4: 0.6457611322402954
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 259.21 seconds!
[tester] 
SST2Metric: acc=0.5, hinge=2.0130575545337224, ce=0.9017825080167263
Local test acc @ epoch 0: 0.5
Client 5 execute local training on 8 samples...
Local loss @ local epoch 0: 1.1912611722946167
Local loss @ local epoch 1: 0.8117235898971558
Local loss @ local epoch 2: 0.6855291724205017
Local loss @ local epoch 3: 0.5351258516311646
Local loss @ local epoch 4: 0.42816197872161865
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 258.9 seconds!
[tester] 
SST2Metric: acc=0.5676605504587156, hinge=1.8723615736042687, ce=0.7752881222361818
Local test acc @ epoch 0: 0.5677
Client 7 execute local training on 8 samples...
Local loss @ local epoch 0: 0.8138406276702881
Local loss @ local epoch 1: 0.6995471119880676
Local loss @ local epoch 2: 0.7704792618751526
Local loss @ local epoch 3: 1.022219181060791
Local loss @ local epoch 4: 0.7015085816383362
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 258.75 seconds!
[tester] 
SST2Metric: acc=0.5091743119266054, hinge=2.166353554900633, ce=1.2642167079339333
Local test acc @ epoch 0: 0.5092
Client 3 execute local training on 8 samples...
Local loss @ local epoch 0: 0.5858731865882874
Local loss @ local epoch 1: 0.673988401889801
Local loss @ local epoch 2: 0.9628540873527527
Local loss @ local epoch 3: 0.6956009268760681
Local loss @ local epoch 4: 0.6570107340812683
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 258.68 seconds!
[tester] 
SST2Metric: acc=0.4908256880733945, hinge=2.2509007732802573, ce=1.3172993124078174
Local test acc @ epoch 0: 0.4908
Client 9 execute local training on 8 samples...
Local loss @ local epoch 0: 0.5936180353164673
Local loss @ local epoch 1: 0.5930911302566528
Local loss @ local epoch 2: 0.34197545051574707
Local loss @ local epoch 3: 0.4797326922416687
Local loss @ local epoch 4: 0.36325138807296753
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 258.48 seconds!
[tester] 
SST2Metric: acc=0.5997706422018348, hinge=1.637529627445641, ce=0.711688650857418
Local test acc @ epoch 0: 0.5998
Client 1 execute local training on 8 samples...
Local loss @ local epoch 0: 0.6113187074661255
Local loss @ local epoch 1: 0.6713151931762695
Local loss @ local epoch 2: 0.7022929787635803
Local loss @ local epoch 3: 0.8704107999801636
Local loss @ local epoch 4: 0.6797288060188293
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 251.41 seconds!
[tester] 
SST2Metric: acc=0.48394495412844035, hinge=2.010649874669696, ce=0.7365234412184549
Local test acc @ epoch 0: 0.4839
Client 0 execute local training on 8 samples...
Local loss @ local epoch 0: 0.8080668449401855
Local loss @ local epoch 1: 0.5112491250038147
Local loss @ local epoch 2: 0.521896243095398
Local loss @ local epoch 3: 1.3133639097213745
Local loss @ local epoch 4: 0.8294408917427063
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 259.25 seconds!
[tester] 
SST2Metric: acc=0.5240825688073395, hinge=2.013523413363947, ce=1.0462777695251166
Local test acc @ epoch 0: 0.5241
Global evaluate on test data...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 259.45 seconds!
[tester] 
SST2Metric: acc=0.6112385321100917, hinge=1.7979980750915108, ce=0.6543944240163225
Global test acc @ epoch 0: 0.6112
Global epoch 1...
Client 2 execute local training on 8 samples...
Local loss @ local epoch 0: 0.8702670931816101
Local loss @ local epoch 1: 0.5107914805412292
Local loss @ local epoch 2: 0.8255443572998047
Local loss @ local epoch 3: 1.4499391317367554
Local loss @ local epoch 4: 0.6213641166687012
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 258.18 seconds!
[tester] 
SST2Metric: acc=0.49311926605504586, hinge=2.0056771072772666, ce=0.7339535935209431
Local test acc @ epoch 1: 0.4931
Client 5 execute local training on 8 samples...
Local loss @ local epoch 0: 0.8284897208213806
Local loss @ local epoch 1: 0.6773428916931152
Local loss @ local epoch 2: 0.8496658205986023
Local loss @ local epoch 3: 1.3058905601501465
Local loss @ local epoch 4: 0.5307568311691284
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 259.09 seconds!
[tester] 
SST2Metric: acc=0.6146788990825688, hinge=1.8460479187309196, ce=0.6726098342226186
Local test acc @ epoch 1: 0.6147
Client 9 execute local training on 8 samples...
Local loss @ local epoch 0: 0.4810850918292999
Local loss @ local epoch 1: 0.649940013885498
Local loss @ local epoch 2: 0.5097766518592834
Local loss @ local epoch 3: 0.38470032811164856
Local loss @ local epoch 4: 0.464675635099411
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 258.94 seconds!
[tester] 
SST2Metric: acc=0.5424311926605505, hinge=1.7867873425877423, ce=0.7008870019825226
Local test acc @ epoch 1: 0.5424
Client 7 execute local training on 8 samples...
Local loss @ local epoch 0: 0.6187748908996582
Local loss @ local epoch 1: 0.6620007753372192
Local loss @ local epoch 2: 0.8446750640869141
Local loss @ local epoch 3: 0.8937156796455383
Local loss @ local epoch 4: 0.5518383979797363
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 259.38 seconds!
[tester] 
SST2Metric: acc=0.5103211009174312, hinge=1.7699394565109814, ce=0.881612535053437
Local test acc @ epoch 1: 0.5103
Client 8 execute local training on 8 samples...
Local loss @ local epoch 0: 0.8918977379798889
Local loss @ local epoch 1: 0.7507466673851013
Local loss @ local epoch 2: 1.2753649950027466
Local loss @ local epoch 3: 0.5497683882713318
Local loss @ local epoch 4: 0.45898470282554626
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 258.83 seconds!
[tester] 
SST2Metric: acc=0.5940366972477065, hinge=1.8079910349408421, ce=0.7247710501382111
Local test acc @ epoch 1: 0.594
Client 3 execute local training on 8 samples...
Local loss @ local epoch 0: 0.5060387849807739
Local loss @ local epoch 1: 0.37175458669662476
Local loss @ local epoch 2: 0.3383398652076721
Local loss @ local epoch 3: 0.818173885345459
Local loss @ local epoch 4: 0.7954950332641602
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 258.14 seconds!
[tester] 
SST2Metric: acc=0.5091743119266054, hinge=2.0026016191605036, ce=0.741488295684167
Local test acc @ epoch 1: 0.5092
Client 1 execute local training on 8 samples...
Local loss @ local epoch 0: 0.694409191608429
Local loss @ local epoch 1: 0.6997166872024536
Local loss @ local epoch 2: 0.7641708254814148
Local loss @ local epoch 3: 0.3974730670452118
Local loss @ local epoch 4: 0.27289825677871704
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 255.18 seconds!
[tester] 
SST2Metric: acc=0.7373853211009175, hinge=1.395284324610999, ce=0.5564838841967626
Local test acc @ epoch 1: 0.7374
Client 6 execute local training on 8 samples...
Local loss @ local epoch 0: 0.6026594638824463
Local loss @ local epoch 1: 0.36182188987731934
Local loss @ local epoch 2: 0.7265474200248718
Local loss @ local epoch 3: 1.1599743366241455
Local loss @ local epoch 4: 0.8688099384307861
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 258.1 seconds!
[tester] 
SST2Metric: acc=0.5504587155963303, hinge=1.8495197296142578, ce=0.7288991573753707
Local test acc @ epoch 1: 0.5505
Client 4 execute local training on 8 samples...
Local loss @ local epoch 0: 0.6747694611549377
Local loss @ local epoch 1: 0.5631793737411499
Local loss @ local epoch 2: 0.6254132390022278
Local loss @ local epoch 3: 0.4544183313846588
Local loss @ local epoch 4: 0.3259661793708801
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 259.05 seconds!
[tester] 
SST2Metric: acc=0.625, hinge=1.808837912498264, ce=0.6615649349645737
Local test acc @ epoch 1: 0.625
Client 0 execute local training on 8 samples...
Local loss @ local epoch 0: 0.6095655560493469
Local loss @ local epoch 1: 0.5665332078933716
Local loss @ local epoch 2: 0.5186278820037842
Local loss @ local epoch 3: 0.5195774435997009
Local loss @ local epoch 4: 0.4154294729232788
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 258.9 seconds!
[tester] 
SST2Metric: acc=0.5126146788990825, hinge=1.939761865166349, ce=1.0151716968335143
Local test acc @ epoch 1: 0.5126
Global evaluate on test data...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 258.85 seconds!
[tester] 
SST2Metric: acc=0.6502293577981652, hinge=1.6478270122764307, ce=0.6199606312524288
Global test acc @ epoch 1: 0.6502
Global epoch 2...
Client 2 execute local training on 8 samples...
Local loss @ local epoch 0: 0.7487847208976746
Local loss @ local epoch 1: 0.42002052068710327
Local loss @ local epoch 2: 0.53232741355896
Local loss @ local epoch 3: 0.6616554856300354
Local loss @ local epoch 4: 0.4183957278728485
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 258.93 seconds!
[tester] 
SST2Metric: acc=0.5802752293577982, hinge=1.7990559230156995, ce=0.7155194651643071
Local test acc @ epoch 2: 0.5803
Client 5 execute local training on 8 samples...
Local loss @ local epoch 0: 0.6713551878929138
Local loss @ local epoch 1: 1.0076088905334473
Local loss @ local epoch 2: 0.8864181041717529
Local loss @ local epoch 3: 0.7466088533401489
Local loss @ local epoch 4: 0.5650765299797058
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 259.18 seconds!
[tester] 
SST2Metric: acc=0.6100917431192661, hinge=1.7680713507013583, ce=0.6853399252125977
Local test acc @ epoch 2: 0.6101
Client 1 execute local training on 8 samples...
Local loss @ local epoch 0: 0.512662410736084
Local loss @ local epoch 1: 0.8397802114486694
Local loss @ local epoch 2: 0.6006019115447998
Local loss @ local epoch 3: 0.28948020935058594
Local loss @ local epoch 4: 0.37179625034332275
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 258.67 seconds!
[tester] 
SST2Metric: acc=0.555045871559633, hinge=1.7290409129420552, ce=0.7640390018804357
Local test acc @ epoch 2: 0.555
Client 9 execute local training on 8 samples...
Local loss @ local epoch 0: 0.43532565236091614
Local loss @ local epoch 1: 0.5129674077033997
Local loss @ local epoch 2: 0.3737156391143799
Local loss @ local epoch 3: 0.3799589276313782
Local loss @ local epoch 4: 0.24415265023708344
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 253.14 seconds!
[tester] 
SST2Metric: acc=0.7362385321100917, hinge=1.3959775873280447, ce=0.5348561017885121
Local test acc @ epoch 2: 0.7362
Client 0 execute local training on 8 samples...
Local loss @ local epoch 0: 0.6784160733222961
Local loss @ local epoch 1: 0.44751372933387756
Local loss @ local epoch 2: 0.6263602375984192
Local loss @ local epoch 3: 0.38942116498947144
Local loss @ local epoch 4: 0.3591243326663971
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 259.42 seconds!
[tester] 
SST2Metric: acc=0.7087155963302753, hinge=1.4426014636634688, ce=0.5816990656590243
Local test acc @ epoch 2: 0.7087
Client 4 execute local training on 8 samples...
Local loss @ local epoch 0: 0.5748425722122192
Local loss @ local epoch 1: 0.7065004706382751
Local loss @ local epoch 2: 0.2571878433227539
Local loss @ local epoch 3: 0.16962037980556488
Local loss @ local epoch 4: 0.08468569815158844
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 258.73 seconds!
[tester] 
SST2Metric: acc=0.5068807339449541, hinge=2.086446589286174, ce=1.1209966242313385
Local test acc @ epoch 2: 0.5069
Client 8 execute local training on 8 samples...
Local loss @ local epoch 0: 0.7081506252288818
Local loss @ local epoch 1: 0.6424769163131714
Local loss @ local epoch 2: 0.9256575107574463
Local loss @ local epoch 3: 0.6148704290390015
Local loss @ local epoch 4: 0.5202298760414124
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 258.74 seconds!
[tester] 
SST2Metric: acc=0.7155963302752294, hinge=1.5080785625571504, ce=0.556161371666357
Local test acc @ epoch 2: 0.7156
Client 3 execute local training on 8 samples...
Local loss @ local epoch 0: 0.4291144907474518
Local loss @ local epoch 1: 0.37543511390686035
Local loss @ local epoch 2: 0.6366219520568848
Local loss @ local epoch 3: 0.9861701130867004
Local loss @ local epoch 4: 0.7834367156028748
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 257.9 seconds!
[tester] 
SST2Metric: acc=0.4908256880733945, hinge=2.01656586970758, ce=0.7846097795788302
Local test acc @ epoch 2: 0.4908
Client 7 execute local training on 8 samples...
Local loss @ local epoch 0: 0.5665732026100159
Local loss @ local epoch 1: 0.3913012742996216
Local loss @ local epoch 2: 1.162544846534729
Local loss @ local epoch 3: 0.33094215393066406
Local loss @ local epoch 4: 0.285770058631897
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 258.98 seconds!
[tester] 
SST2Metric: acc=0.6559633027522935, hinge=1.5083821787746674, ce=0.6231385062047101
Local test acc @ epoch 2: 0.656
Client 6 execute local training on 8 samples...
Local loss @ local epoch 0: 0.692299485206604
Local loss @ local epoch 1: 0.7114572525024414
Local loss @ local epoch 2: 0.6659032702445984
Local loss @ local epoch 3: 0.5604165196418762
Local loss @ local epoch 4: 0.7535242438316345
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 259.41 seconds!
[tester] 
SST2Metric: acc=0.4908256880733945, hinge=2.2607787984226824, ce=1.3103975809495383
Local test acc @ epoch 2: 0.4908
Global evaluate on test data...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 258.94 seconds!
[tester] 
SST2Metric: acc=0.6330275229357798, hinge=1.6051186678606435, ce=0.649461149622541
Global test acc @ epoch 2: 0.633
Global epoch 3...
Client 5 execute local training on 8 samples...
Local loss @ local epoch 0: 0.40985843539237976
Local loss @ local epoch 1: 0.856707751750946
Local loss @ local epoch 2: 0.639426589012146
Local loss @ local epoch 3: 0.7917479872703552
Local loss @ local epoch 4: 1.0433495044708252
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 259.37 seconds!
[tester] 
SST2Metric: acc=0.6364678899082569, hinge=1.5060252424226988, ce=0.6450314504017524
Local test acc @ epoch 3: 0.6365
Client 7 execute local training on 8 samples...
Local loss @ local epoch 0: 0.5069910883903503
Local loss @ local epoch 1: 0.2879594564437866
Local loss @ local epoch 2: 0.44440874457359314
Local loss @ local epoch 3: 0.2862422466278076
Local loss @ local epoch 4: 0.16760781407356262
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 253.71 seconds!
[tester] 
SST2Metric: acc=0.6502293577981652, hinge=1.5136556852301326, ce=0.6579517735800612
Local test acc @ epoch 3: 0.6502
Client 3 execute local training on 8 samples...
Local loss @ local epoch 0: 0.4620988070964813
Local loss @ local epoch 1: 0.3836789131164551
Local loss @ local epoch 2: 0.37180548906326294
Local loss @ local epoch 3: 0.2252177596092224
Local loss @ local epoch 4: 0.49871405959129333
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 258.93 seconds!
[tester] 
SST2Metric: acc=0.6295871559633027, hinge=1.5800477742055141, ce=0.6484556835178935
Local test acc @ epoch 3: 0.6296
Client 0 execute local training on 8 samples...
Local loss @ local epoch 0: 0.39725208282470703
Local loss @ local epoch 1: 0.2245778739452362
Local loss @ local epoch 2: 0.36507564783096313
Local loss @ local epoch 3: 0.3275291919708252
Local loss @ local epoch 4: 0.7133079171180725
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 259.02 seconds!
[tester] 
SST2Metric: acc=0.661697247706422, hinge=1.4661699506120944, ce=0.6130567698850544
Local test acc @ epoch 3: 0.6617
Client 2 execute local training on 8 samples...
Local loss @ local epoch 0: 0.658438503742218
Local loss @ local epoch 1: 0.4712185561656952
Local loss @ local epoch 2: 0.9298404455184937
Local loss @ local epoch 3: 0.5736875534057617
Local loss @ local epoch 4: 0.6021051406860352
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 259.26 seconds!
[tester] 
SST2Metric: acc=0.48509174311926606, hinge=1.9862419574632557, ce=0.8353565056389625
Local test acc @ epoch 3: 0.4851
Client 8 execute local training on 8 samples...
Local loss @ local epoch 0: 0.7322295904159546
Local loss @ local epoch 1: 0.5348007082939148
Local loss @ local epoch 2: 0.4074636995792389
Local loss @ local epoch 3: 0.21549640595912933
Local loss @ local epoch 4: 0.2336604744195938
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 258.66 seconds!
[tester] 
SST2Metric: acc=0.6892201834862385, hinge=1.4116204586050927, ce=0.6454610927115887
Local test acc @ epoch 3: 0.6892
Client 6 execute local training on 8 samples...
Local loss @ local epoch 0: 0.32539045810699463
Local loss @ local epoch 1: 0.4071066379547119
Local loss @ local epoch 2: 0.9334661364555359
Local loss @ local epoch 3: 0.4274405837059021
Local loss @ local epoch 4: 0.5016538500785828
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 259.28 seconds!
[tester] 
SST2Metric: acc=0.6502293577981652, hinge=1.5098174684638277, ce=0.6471542866404997
Local test acc @ epoch 3: 0.6502
Client 4 execute local training on 8 samples...
Local loss @ local epoch 0: 0.3635718822479248
Local loss @ local epoch 1: 0.35240399837493896
Local loss @ local epoch 2: 0.6241118311882019
Local loss @ local epoch 3: 0.41380575299263
Local loss @ local epoch 4: 0.24519506096839905
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 259.35 seconds!
[tester] 
SST2Metric: acc=0.6594036697247706, hinge=1.5097150447171763, ce=0.6277653173022314
Local test acc @ epoch 3: 0.6594
Client 1 execute local training on 8 samples...
Local loss @ local epoch 0: 0.397002249956131
Local loss @ local epoch 1: 0.4821307063102722
Local loss @ local epoch 2: 0.8181653618812561
Local loss @ local epoch 3: 0.24525173008441925
Local loss @ local epoch 4: 0.1639893352985382
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 258.51 seconds!
[tester] 
SST2Metric: acc=0.7327981651376146, hinge=1.2721314003708166, ce=0.553720566726059
Local test acc @ epoch 3: 0.7328
Client 9 execute local training on 8 samples...
Local loss @ local epoch 0: 0.2797442376613617
Local loss @ local epoch 1: 0.30534741282463074
Local loss @ local epoch 2: 0.6339555978775024
Local loss @ local epoch 3: 0.8455413579940796
Local loss @ local epoch 4: 0.32739368081092834
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 257.64 seconds!
[tester] 
SST2Metric: acc=0.6123853211009175, hinge=1.594941079069715, ce=0.8120490915458137
Local test acc @ epoch 3: 0.6124
Global evaluate on test data...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 257.19 seconds!
[tester] 
SST2Metric: acc=0.7396788990825688, hinge=1.290201272712935, ce=0.5220987605392386
Global test acc @ epoch 3: 0.7397
Global epoch 4...
Client 6 execute local training on 8 samples...
Local loss @ local epoch 0: 0.3372911214828491
Local loss @ local epoch 1: 0.5914887189865112
Local loss @ local epoch 2: 0.2683601975440979
Local loss @ local epoch 3: 0.6896918416023254
Local loss @ local epoch 4: 0.32937902212142944
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 259.08 seconds!
[tester] 
SST2Metric: acc=0.6857798165137615, hinge=1.4686238454022538, ce=0.5851426206597494
Local test acc @ epoch 4: 0.6858
Client 7 execute local training on 8 samples...
Local loss @ local epoch 0: 0.3382459282875061
Local loss @ local epoch 1: 0.45254573225975037
Local loss @ local epoch 2: 0.1441093385219574
Local loss @ local epoch 3: 0.12792322039604187
Local loss @ local epoch 4: 0.07567083090543747
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 258.98 seconds!
[tester] 
SST2Metric: acc=0.7522935779816514, hinge=1.180187789261888, ce=0.5132793015843138
Local test acc @ epoch 4: 0.7523
Client 3 execute local training on 8 samples...
Local loss @ local epoch 0: 0.36047449707984924
Local loss @ local epoch 1: 0.3145514130592346
Local loss @ local epoch 2: 0.5143060088157654
Local loss @ local epoch 3: 0.2513780891895294
Local loss @ local epoch 4: 0.28083914518356323
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 259.1 seconds!
[tester] 
SST2Metric: acc=0.7855504587155964, hinge=1.2387780383092548, ce=0.4752200707929944
Local test acc @ epoch 4: 0.7856
Client 5 execute local training on 8 samples...
Local loss @ local epoch 0: 0.3623962998390198
Local loss @ local epoch 1: 0.5629849433898926
Local loss @ local epoch 2: 0.14824217557907104
Local loss @ local epoch 3: 0.06634391099214554
Local loss @ local epoch 4: 0.05742308124899864
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 258.51 seconds!
[tester] 
SST2Metric: acc=0.5905963302752294, hinge=1.6995721514071893, ce=0.8633156085916616
Local test acc @ epoch 4: 0.5906
Client 8 execute local training on 8 samples...
Local loss @ local epoch 0: 0.5811267495155334
Local loss @ local epoch 1: 0.32739880681037903
Local loss @ local epoch 2: 0.6686909198760986
Local loss @ local epoch 3: 1.1421380043029785
Local loss @ local epoch 4: 0.23408861458301544
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 258.99 seconds!
[tester] 
SST2Metric: acc=0.7889908256880734, hinge=1.1246534375422592, ce=0.46148583374985863
Local test acc @ epoch 4: 0.789
Client 0 execute local training on 8 samples...
Local loss @ local epoch 0: 0.26322048902511597
Local loss @ local epoch 1: 0.1866620033979416
Local loss @ local epoch 2: 0.08801150321960449
Local loss @ local epoch 3: 0.04263510927557945
Local loss @ local epoch 4: 0.04122556746006012
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 258.52 seconds!
[tester] 
SST2Metric: acc=0.7935779816513762, hinge=1.1001304712043989, ce=0.474774985685261
Local test acc @ epoch 4: 0.7936
Client 9 execute local training on 8 samples...
Local loss @ local epoch 0: 0.20788392424583435
Local loss @ local epoch 1: 0.2133837789297104
Local loss @ local epoch 2: 0.12990687787532806
Local loss @ local epoch 3: 0.0695110633969307
Local loss @ local epoch 4: 0.042766883969306946
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 258.58 seconds!
[tester] 
SST2Metric: acc=0.768348623853211, hinge=1.1203631894030701, ce=0.47919386228837
Local test acc @ epoch 4: 0.7683
Client 4 execute local training on 8 samples...
Local loss @ local epoch 0: 0.35374316573143005
Local loss @ local epoch 1: 0.08446516841650009
Local loss @ local epoch 2: 0.039418578147888184
Local loss @ local epoch 3: 0.024211443960666656
Local loss @ local epoch 4: 0.011421614326536655
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 253.73 seconds!
[tester] 
SST2Metric: acc=0.6708715596330275, hinge=1.449972621481353, ce=0.7646848293482711
Local test acc @ epoch 4: 0.6709
Client 2 execute local training on 8 samples...
Local loss @ local epoch 0: 0.1927780658006668
Local loss @ local epoch 1: 0.13635335862636566
Local loss @ local epoch 2: 0.10635482519865036
Local loss @ local epoch 3: 0.03899525851011276
Local loss @ local epoch 4: 0.03779064491391182
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 259.28 seconds!
[tester] 
SST2Metric: acc=0.6295871559633027, hinge=1.754547127479807, ce=1.0055023891116501
Local test acc @ epoch 4: 0.6296
Client 1 execute local training on 8 samples...
Local loss @ local epoch 0: 0.3862467408180237
Local loss @ local epoch 1: 0.3483836352825165
Local loss @ local epoch 2: 0.15488561987876892
Local loss @ local epoch 3: 0.058346010744571686
Local loss @ local epoch 4: 0.038224391639232635
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 258.84 seconds!
[tester] 
SST2Metric: acc=0.7717889908256881, hinge=1.0130437753342707, ce=0.48749156843084807
Local test acc @ epoch 4: 0.7718
Global evaluate on test data...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 258.94 seconds!
[tester] 
SST2Metric: acc=0.7041284403669725, hinge=1.2464887542188714, ce=0.5684551550844393
Global test acc @ epoch 4: 0.7041
Global epoch 5...
Client 7 execute local training on 8 samples...
Local loss @ local epoch 0: 0.20048148930072784
Local loss @ local epoch 1: 0.6498157978057861
Local loss @ local epoch 2: 0.7796522974967957
Local loss @ local epoch 3: 0.2758607268333435
Local loss @ local epoch 4: 0.24796566367149353
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 258.69 seconds!
[tester] 
SST2Metric: acc=0.7545871559633027, hinge=1.1781020503525341, ce=0.4885352665404661
Local test acc @ epoch 5: 0.7546
Client 2 execute local training on 8 samples...
Local loss @ local epoch 0: 0.3599417209625244
Local loss @ local epoch 1: 0.1130390614271164
Local loss @ local epoch 2: 0.0776849165558815
Local loss @ local epoch 3: 0.025647545233368874
Local loss @ local epoch 4: 0.013186860829591751
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 259.6 seconds!
[tester] 
SST2Metric: acc=0.5676605504587156, hinge=1.8918149288094372, ce=1.0545242246616324
Local test acc @ epoch 5: 0.5677
Client 6 execute local training on 8 samples...
Local loss @ local epoch 0: 0.17577742040157318
Local loss @ local epoch 1: 0.23552677035331726
Local loss @ local epoch 2: 0.11430471390485764
Local loss @ local epoch 3: 0.7155225276947021
Local loss @ local epoch 4: 0.05901937186717987
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 258.83 seconds!
[tester] 
SST2Metric: acc=0.658256880733945, hinge=1.4461483131034658, ce=0.730634295919893
Local test acc @ epoch 5: 0.6583
Client 4 execute local training on 8 samples...
Local loss @ local epoch 0: 0.10230528563261032
Local loss @ local epoch 1: 0.6765778064727783
Local loss @ local epoch 2: 0.5405821204185486
Local loss @ local epoch 3: 0.11023276299238205
Local loss @ local epoch 4: 0.06983771920204163
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 258.97 seconds!
[tester] 
SST2Metric: acc=0.6467889908256881, hinge=1.5195196098690733, ce=0.675734115986649
Local test acc @ epoch 5: 0.6468
Client 0 execute local training on 8 samples...
Local loss @ local epoch 0: 0.45365023612976074
Local loss @ local epoch 1: 0.19917914271354675
Local loss @ local epoch 2: 0.12102051079273224
Local loss @ local epoch 3: 0.04671837389469147
Local loss @ local epoch 4: 0.05178051441907883
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 258.34 seconds!
[tester] 
SST2Metric: acc=0.7155963302752294, hinge=1.2477427548771605, ce=0.6090244572575486
Local test acc @ epoch 5: 0.7156
Client 8 execute local training on 8 samples...
Local loss @ local epoch 0: 0.32042253017425537
Local loss @ local epoch 1: 0.36074337363243103
Local loss @ local epoch 2: 0.14597198367118835
Local loss @ local epoch 3: 0.4980875849723816
Local loss @ local epoch 4: 0.1139511913061142
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 248.08 seconds!
[tester] 
SST2Metric: acc=0.8405963302752294, hinge=0.8122442937498793, ce=0.3735818689979544
Local test acc @ epoch 5: 0.8406
Client 9 execute local training on 8 samples...
Local loss @ local epoch 0: 0.26456117630004883
Local loss @ local epoch 1: 0.2751145362854004
Local loss @ local epoch 2: 0.0847233459353447
Local loss @ local epoch 3: 0.06314745545387268
Local loss @ local epoch 4: 0.031078867614269257
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 258.49 seconds!
[tester] 
SST2Metric: acc=0.8165137614678899, hinge=0.8895239203894904, ce=0.4134281134796799
Local test acc @ epoch 5: 0.8165
Client 3 execute local training on 8 samples...
Local loss @ local epoch 0: 0.24777206778526306
Local loss @ local epoch 1: 0.1132897287607193
Local loss @ local epoch 2: 0.034211114048957825
Local loss @ local epoch 3: 0.07605399191379547
Local loss @ local epoch 4: 0.4352189302444458
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 259.2 seconds!
[tester] 
SST2Metric: acc=0.8245412844036697, hinge=0.8905326062386189, ce=0.4127850781340118
Local test acc @ epoch 5: 0.8245
Client 1 execute local training on 8 samples...
Local loss @ local epoch 0: 0.09200868010520935
Local loss @ local epoch 1: 0.1036401242017746
Local loss @ local epoch 2: 0.03913115710020065
Local loss @ local epoch 3: 0.09609590470790863
Local loss @ local epoch 4: 0.019746912643313408
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 258.17 seconds!
[tester] 
SST2Metric: acc=0.8038990825688074, hinge=0.9261877091105924, ce=0.46804869718371184
Local test acc @ epoch 5: 0.8039
Client 5 execute local training on 8 samples...
Local loss @ local epoch 0: 0.208572655916214
Local loss @ local epoch 1: 0.44313347339630127
Local loss @ local epoch 2: 0.5284782648086548
Local loss @ local epoch 3: 0.5720700025558472
Local loss @ local epoch 4: 1.0753309726715088
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 258.63 seconds!
[tester] 
SST2Metric: acc=0.4908256880733945, hinge=2.5682964029662108, ce=1.6005343576090052
Local test acc @ epoch 5: 0.4908
Global evaluate on test data...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 258.79 seconds!
[tester] 
SST2Metric: acc=0.8314220183486238, hinge=0.8150994512466115, ce=0.38488050789461226
Global test acc @ epoch 5: 0.8314
Global epoch 6...
Client 7 execute local training on 8 samples...
Local loss @ local epoch 0: 0.146943598985672
Local loss @ local epoch 1: 0.08131510019302368
Local loss @ local epoch 2: 0.16285675764083862
Local loss @ local epoch 3: 0.042100176215171814
Local loss @ local epoch 4: 0.015406524762511253
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 258.93 seconds!
[tester] 
SST2Metric: acc=0.8394495412844036, hinge=0.8391202844064171, ce=0.4304454658488068
Local test acc @ epoch 6: 0.8394
Client 2 execute local training on 8 samples...
Local loss @ local epoch 0: 0.12452200055122375
Local loss @ local epoch 1: 0.38224709033966064
Local loss @ local epoch 2: 0.5007271766662598
Local loss @ local epoch 3: 0.5196922421455383
Local loss @ local epoch 4: 0.09535225480794907
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 259.02 seconds!
[tester] 
SST2Metric: acc=0.6227064220183486, hinge=1.591423446705582, ce=0.8196620350583977
Local test acc @ epoch 6: 0.6227
Client 4 execute local training on 8 samples...
Local loss @ local epoch 0: 0.1064852699637413
Local loss @ local epoch 1: 0.04006173461675644
Local loss @ local epoch 2: 0.24743470549583435
Local loss @ local epoch 3: 1.1245143413543701
Local loss @ local epoch 4: 0.22365014255046844
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 249.01 seconds!
[tester] 
SST2Metric: acc=0.8096330275229358, hinge=0.9171739111252881, ce=0.43616857345497934
Local test acc @ epoch 6: 0.8096
Client 3 execute local training on 8 samples...
Local loss @ local epoch 0: 0.2520119547843933
Local loss @ local epoch 1: 0.18782192468643188
Local loss @ local epoch 2: 0.2950515151023865
Local loss @ local epoch 3: 0.09855010360479355
Local loss @ local epoch 4: 0.06168961897492409
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 258.69 seconds!
[tester] 
SST2Metric: acc=0.8440366972477065, hinge=0.8031629127099973, ce=0.3681360870052915
Local test acc @ epoch 6: 0.844
Client 6 execute local training on 8 samples...
Local loss @ local epoch 0: 0.11723814904689789
Local loss @ local epoch 1: 0.5871017575263977
Local loss @ local epoch 2: 0.6332455277442932
Local loss @ local epoch 3: 0.22108185291290283
Local loss @ local epoch 4: 0.07560592889785767
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 259.15 seconds!
[tester] 
SST2Metric: acc=0.7545871559633027, hinge=1.0949946234532453, ce=0.5250332401979954
Local test acc @ epoch 6: 0.7546
Client 1 execute local training on 8 samples...
Local loss @ local epoch 0: 0.06177987903356552
Local loss @ local epoch 1: 0.10722517967224121
Local loss @ local epoch 2: 0.1945636123418808
Local loss @ local epoch 3: 0.0661541074514389
Local loss @ local epoch 4: 0.030582325533032417
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 259.12 seconds!
[tester] 
SST2Metric: acc=0.8520642201834863, hinge=0.7430281547504828, ce=0.3705257450768707
Local test acc @ epoch 6: 0.8521
Client 9 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0977846086025238
Local loss @ local epoch 1: 0.14306917786598206
Local loss @ local epoch 2: 0.06633201986551285
Local loss @ local epoch 3: 0.2334006279706955
Local loss @ local epoch 4: 0.06927397847175598
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 258.42 seconds!
[tester] 
SST2Metric: acc=0.7798165137614679, hinge=1.0346438041247359, ce=0.4805902633986889
Local test acc @ epoch 6: 0.7798
Client 5 execute local training on 8 samples...
Local loss @ local epoch 0: 0.16824519634246826
Local loss @ local epoch 1: 0.061663974076509476
Local loss @ local epoch 2: 0.39770254492759705
Local loss @ local epoch 3: 0.11118761450052261
Local loss @ local epoch 4: 0.020574457943439484
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 259.11 seconds!
[tester] 
SST2Metric: acc=0.7924311926605505, hinge=0.9595742633036517, ce=0.46026355094797566
Local test acc @ epoch 6: 0.7924
Client 8 execute local training on 8 samples...
Local loss @ local epoch 0: 0.2862266004085541
Local loss @ local epoch 1: 0.4534798264503479
Local loss @ local epoch 2: 0.30604323744773865
Local loss @ local epoch 3: 0.12850061058998108
Local loss @ local epoch 4: 0.052636753767728806
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 259.24 seconds!
[tester] 
SST2Metric: acc=0.8841743119266054, hinge=0.6272876032995521, ce=0.3058525110261703
Local test acc @ epoch 6: 0.8842
Client 0 execute local training on 8 samples...
Local loss @ local epoch 0: 0.2184758484363556
Local loss @ local epoch 1: 0.08549082279205322
Local loss @ local epoch 2: 0.22759674489498138
Local loss @ local epoch 3: 0.09943533688783646
Local loss @ local epoch 4: 0.04396577179431915
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 259.25 seconds!
[tester] 
SST2Metric: acc=0.841743119266055, hinge=0.8119649621871633, ce=0.375930341456188
Local test acc @ epoch 6: 0.8417
Global evaluate on test data...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 259.26 seconds!
[tester] 
SST2Metric: acc=0.8302752293577982, hinge=0.8001745295360547, ce=0.3900712455938989
Global test acc @ epoch 6: 0.8303
Global epoch 7...
Client 7 execute local training on 8 samples...
Local loss @ local epoch 0: 0.052267055958509445
Local loss @ local epoch 1: 0.10105215013027191
Local loss @ local epoch 2: 0.14756520092487335
Local loss @ local epoch 3: 0.03136538714170456
Local loss @ local epoch 4: 0.06598580628633499
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 248.93 seconds!
[tester] 
SST2Metric: acc=0.7855504587155964, hinge=1.0983282756914787, ce=0.5788075013103289
Local test acc @ epoch 7: 0.7856
Client 1 execute local training on 8 samples...
Local loss @ local epoch 0: 0.05246337130665779
Local loss @ local epoch 1: 0.07240596413612366
Local loss @ local epoch 2: 0.09996245056390762
Local loss @ local epoch 3: 0.7589504718780518
Local loss @ local epoch 4: 1.626011610031128
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 259.16 seconds!
[tester] 
SST2Metric: acc=0.5126146788990825, hinge=1.9635890066076855, ce=0.8441793926812093
Local test acc @ epoch 7: 0.5126
Client 5 execute local training on 8 samples...
Local loss @ local epoch 0: 0.06984004378318787
Local loss @ local epoch 1: 0.011631639674305916
Local loss @ local epoch 2: 0.05640765279531479
Local loss @ local epoch 3: 0.14474807679653168
Local loss @ local epoch 4: 0.10251812636852264
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 258.77 seconds!
[tester] 
SST2Metric: acc=0.8440366972477065, hinge=0.7897346563842318, ce=0.3865837433606113
Local test acc @ epoch 7: 0.844
Client 3 execute local training on 8 samples...
Local loss @ local epoch 0: 0.12275828421115875
Local loss @ local epoch 1: 0.11774718016386032
Local loss @ local epoch 2: 0.06860074400901794
Local loss @ local epoch 3: 0.03303205966949463
Local loss @ local epoch 4: 0.020666390657424927
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 258.71 seconds!
[tester] 
SST2Metric: acc=0.8440366972477065, hinge=0.7590521176200394, ce=0.37802226462085314
Local test acc @ epoch 7: 0.844
Client 2 execute local training on 8 samples...
Local loss @ local epoch 0: 0.04961930215358734
Local loss @ local epoch 1: 0.4834553003311157
Local loss @ local epoch 2: 0.2394912838935852
Local loss @ local epoch 3: 0.10286261141300201
Local loss @ local epoch 4: 0.042007528245449066
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 258.77 seconds!
[tester] 
SST2Metric: acc=0.7603211009174312, hinge=1.0952399977303426, ce=0.537880364839637
Local test acc @ epoch 7: 0.7603
Client 4 execute local training on 8 samples...
Local loss @ local epoch 0: 0.030856404453516006
Local loss @ local epoch 1: 0.006703148130327463
Local loss @ local epoch 2: 0.007174770347774029
Local loss @ local epoch 3: 0.0023522034753113985
Local loss @ local epoch 4: 0.0031351838260889053
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 258.94 seconds!
[tester] 
SST2Metric: acc=0.6353211009174312, hinge=2.047615374447009, ce=1.3071946440597808
Local test acc @ epoch 7: 0.6353
Client 9 execute local training on 8 samples...
Local loss @ local epoch 0: 0.11390551179647446
Local loss @ local epoch 1: 0.26613959670066833
Local loss @ local epoch 2: 0.10790494829416275
Local loss @ local epoch 3: 0.0377463772892952
Local loss @ local epoch 4: 0.6934934258460999
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 259.32 seconds!
[tester] 
SST2Metric: acc=0.8738532110091743, hinge=0.6488128669491602, ce=0.31365821290111867
Local test acc @ epoch 7: 0.8739
Client 0 execute local training on 8 samples...
Local loss @ local epoch 0: 0.11982164531946182
Local loss @ local epoch 1: 0.20123928785324097
Local loss @ local epoch 2: 0.38019952178001404
Local loss @ local epoch 3: 0.15006159245967865
Local loss @ local epoch 4: 0.03970889002084732
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 258.66 seconds!
[tester] 
SST2Metric: acc=0.8474770642201835, hinge=0.7178034138515454, ce=0.3478214369249453
Local test acc @ epoch 7: 0.8475
Client 8 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0725184753537178
Local loss @ local epoch 1: 0.4803515076637268
Local loss @ local epoch 2: 0.17114444077014923
Local loss @ local epoch 3: 0.07472744584083557
Local loss @ local epoch 4: 0.0428059920668602
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 258.06 seconds!
[tester] 
SST2Metric: acc=0.8635321100917431, hinge=0.6718629666971504, ce=0.3252863762643906
Local test acc @ epoch 7: 0.8635
Client 6 execute local training on 8 samples...
Local loss @ local epoch 0: 0.08342203497886658
Local loss @ local epoch 1: 0.9154800176620483
Local loss @ local epoch 2: 0.2208867073059082
Local loss @ local epoch 3: 0.13925428688526154
Local loss @ local epoch 4: 0.06578364968299866
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 258.59 seconds!
[tester] 
SST2Metric: acc=0.7878440366972477, hinge=0.9288895615470518, ce=0.4475200793987841
Local test acc @ epoch 7: 0.7878
Global evaluate on test data...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 259.01 seconds!
[tester] 
SST2Metric: acc=0.8704128440366973, hinge=0.639708231621926, ce=0.3329708974535039
Global test acc @ epoch 7: 0.8704
Global epoch 8...
Client 7 execute local training on 8 samples...
Local loss @ local epoch 0: 0.021519947797060013
Local loss @ local epoch 1: 0.028439372777938843
Local loss @ local epoch 2: 0.44223296642303467
Local loss @ local epoch 3: 0.019835470244288445
Local loss @ local epoch 4: 0.013913890346884727
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 259.09 seconds!
[tester] 
SST2Metric: acc=0.8428899082568807, hinge=0.7757758752468529, ce=0.3839598640355222
Local test acc @ epoch 8: 0.8429
Client 9 execute local training on 8 samples...
Local loss @ local epoch 0: 0.055307041853666306
Local loss @ local epoch 1: 0.1453990340232849
Local loss @ local epoch 2: 0.33007943630218506
Local loss @ local epoch 3: 0.08985680341720581
Local loss @ local epoch 4: 0.010754194110631943
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 259.17 seconds!
[tester] 
SST2Metric: acc=0.856651376146789, hinge=0.6621109508319732, ce=0.3439307779056627
Local test acc @ epoch 8: 0.8567
Client 1 execute local training on 8 samples...
Local loss @ local epoch 0: 0.013744574971497059
Local loss @ local epoch 1: 0.011784306727349758
Local loss @ local epoch 2: 0.015965716913342476
Local loss @ local epoch 3: 0.06953029334545135
Local loss @ local epoch 4: 0.009752736426889896
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 258.6 seconds!
[tester] 
SST2Metric: acc=0.8268348623853211, hinge=1.0109369678234836, ce=0.6417304811424601
Local test acc @ epoch 8: 0.8268
Client 0 execute local training on 8 samples...
Local loss @ local epoch 0: 0.11604385077953339
Local loss @ local epoch 1: 0.44560322165489197
Local loss @ local epoch 2: 0.014549920335412025
Local loss @ local epoch 3: 0.03515719249844551
Local loss @ local epoch 4: 0.006532843224704266
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 259.09 seconds!
[tester] 
SST2Metric: acc=0.8463302752293578, hinge=0.7824737828805906, ce=0.417480035761901
Local test acc @ epoch 8: 0.8463
Client 5 execute local training on 8 samples...
Local loss @ local epoch 0: 0.012834606692194939
Local loss @ local epoch 1: 0.003669554367661476
Local loss @ local epoch 2: 0.44020846486091614
Local loss @ local epoch 3: 0.7140061855316162
Local loss @ local epoch 4: 0.14313824474811554
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 258.94 seconds!
[tester] 
SST2Metric: acc=0.8440366972477065, hinge=0.8500908224954518, ce=0.46662525008987943
Local test acc @ epoch 8: 0.844
Client 6 execute local training on 8 samples...
Local loss @ local epoch 0: 0.09539070725440979
Local loss @ local epoch 1: 0.3144460618495941
Local loss @ local epoch 2: 0.3408440351486206
Local loss @ local epoch 3: 0.46335041522979736
Local loss @ local epoch 4: 0.3591698408126831
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 259.04 seconds!
[tester] 
SST2Metric: acc=0.7924311926605505, hinge=0.9735128742839219, ce=0.4148078070731338
Local test acc @ epoch 8: 0.7924
Client 8 execute local training on 8 samples...
Local loss @ local epoch 0: 0.026510249823331833
Local loss @ local epoch 1: 0.0316225029528141
Local loss @ local epoch 2: 0.5341017842292786
Local loss @ local epoch 3: 0.02703520469367504
Local loss @ local epoch 4: 0.02734924480319023
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 256.23 seconds!
[tester] 
SST2Metric: acc=0.8692660550458715, hinge=0.6089715980881945, ce=0.3079052650190275
Local test acc @ epoch 8: 0.8693
Client 4 execute local training on 8 samples...
Local loss @ local epoch 0: 0.012401807121932507
Local loss @ local epoch 1: 0.15046104788780212
Local loss @ local epoch 2: 0.03608064725995064
Local loss @ local epoch 3: 1.396868109703064
Local loss @ local epoch 4: 1.8674904108047485
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 259.13 seconds!
[tester] 
SST2Metric: acc=0.5963302752293578, hinge=1.775599674347344, ce=0.6829821675195606
Local test acc @ epoch 8: 0.5963
Client 3 execute local training on 8 samples...
Local loss @ local epoch 0: 0.03057117387652397
Local loss @ local epoch 1: 0.02157863974571228
Local loss @ local epoch 2: 0.003088329453021288
Local loss @ local epoch 3: 0.006066903471946716
Local loss @ local epoch 4: 1.1434096097946167
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 259.45 seconds!
[tester] 
SST2Metric: acc=0.8635321100917431, hinge=0.6638420977450292, ce=0.3704453168795743
Local test acc @ epoch 8: 0.8635
Client 2 execute local training on 8 samples...
Local loss @ local epoch 0: 0.05189725384116173
Local loss @ local epoch 1: 0.014206914231181145
Local loss @ local epoch 2: 0.19485610723495483
Local loss @ local epoch 3: 0.1949486881494522
Local loss @ local epoch 4: 0.003907432314008474
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 258.87 seconds!
[tester] 
SST2Metric: acc=0.7075688073394495, hinge=1.4193288120654746, ce=0.7982026567563004
Local test acc @ epoch 8: 0.7076
Global evaluate on test data...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 258.62 seconds!
[tester] 
SST2Metric: acc=0.8658256880733946, hinge=0.7260230195905091, ce=0.4283797864635569
Global test acc @ epoch 8: 0.8658
Global epoch 9...
Client 8 execute local training on 8 samples...
Local loss @ local epoch 0: 0.030864767730236053
Local loss @ local epoch 1: 0.0800824835896492
Local loss @ local epoch 2: 0.024615582078695297
Local loss @ local epoch 3: 1.0562840700149536
Local loss @ local epoch 4: 0.020001862198114395
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 258.82 seconds!
[tester] 
SST2Metric: acc=0.819954128440367, hinge=0.9083049345180529, ce=0.5012710490221277
Local test acc @ epoch 9: 0.82
Client 5 execute local training on 8 samples...
Local loss @ local epoch 0: 0.01983119733631611
Local loss @ local epoch 1: 0.1279803067445755
Local loss @ local epoch 2: 0.013972249813377857
Local loss @ local epoch 3: 0.046396203339099884
Local loss @ local epoch 4: 0.012514953501522541
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 258.43 seconds!
[tester] 
SST2Metric: acc=0.8532110091743119, hinge=0.7512406155877157, ce=0.4002440690549962
Local test acc @ epoch 9: 0.8532
Client 1 execute local training on 8 samples...
Local loss @ local epoch 0: 0.018449321389198303
Local loss @ local epoch 1: 0.010054145939648151
Local loss @ local epoch 2: 0.011029738001525402
Local loss @ local epoch 3: 0.0008500807452946901
Local loss @ local epoch 4: 0.0006931236712262034
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 258.72 seconds!
[tester] 
SST2Metric: acc=0.8853211009174312, hinge=0.6368506174842152, ce=0.3851063372567296
Local test acc @ epoch 9: 0.8853
Client 4 execute local training on 8 samples...
Local loss @ local epoch 0: 0.010232123546302319
Local loss @ local epoch 1: 0.4785669445991516
Local loss @ local epoch 2: 0.011110439896583557
Local loss @ local epoch 3: 0.006490165833383799
Local loss @ local epoch 4: 0.010045165196061134
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 258.93 seconds!
[tester] 
SST2Metric: acc=0.8188073394495413, hinge=0.9624952724493971, ce=0.5518818252566142
Local test acc @ epoch 9: 0.8188
Client 0 execute local training on 8 samples...
Local loss @ local epoch 0: 0.5368848443031311
Local loss @ local epoch 1: 0.03209618479013443
Local loss @ local epoch 2: 0.009324206970632076
Local loss @ local epoch 3: 0.010145970620214939
Local loss @ local epoch 4: 0.008572995662689209
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 254.52 seconds!
[tester] 
SST2Metric: acc=0.8394495412844036, hinge=0.7321611439142752, ce=0.38336993184863427
Local test acc @ epoch 9: 0.8394
Client 7 execute local training on 8 samples...
Local loss @ local epoch 0: 0.004098069854080677
Local loss @ local epoch 1: 0.006647695787250996
Local loss @ local epoch 2: 0.8398040533065796
Local loss @ local epoch 3: 0.3847348392009735
Local loss @ local epoch 4: 0.7572223544120789
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 259.29 seconds!
[tester] 
SST2Metric: acc=0.8314220183486238, hinge=0.8429249793017676, ce=0.4195608355856817
Local test acc @ epoch 9: 0.8314
Client 9 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0546652153134346
Local loss @ local epoch 1: 0.02816079929471016
Local loss @ local epoch 2: 0.29357656836509705
Local loss @ local epoch 3: 0.1510007083415985
Local loss @ local epoch 4: 0.03214055672287941
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 259.17 seconds!
[tester] 
SST2Metric: acc=0.7901376146788991, hinge=0.9952107924933827, ce=0.5221661065935815
Local test acc @ epoch 9: 0.7901
Client 2 execute local training on 8 samples...
Local loss @ local epoch 0: 0.004483446013182402
Local loss @ local epoch 1: 0.055381279438734055
Local loss @ local epoch 2: 0.04188898578286171
Local loss @ local epoch 3: 0.0013254617806524038
Local loss @ local epoch 4: 0.002344172913581133
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 259.14 seconds!
[tester] 
SST2Metric: acc=0.7924311926605505, hinge=1.0608134431029679, ce=0.6263579861218229
Local test acc @ epoch 9: 0.7924
Client 6 execute local training on 8 samples...
Local loss @ local epoch 0: 0.20135726034641266
Local loss @ local epoch 1: 0.7534909248352051
Local loss @ local epoch 2: 0.06583009660243988
Local loss @ local epoch 3: 0.0735403448343277
Local loss @ local epoch 4: 0.05778443068265915
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 259.06 seconds!
[tester] 
SST2Metric: acc=0.75, hinge=1.1475291518716637, ce=0.5701623779781367
Local test acc @ epoch 9: 0.75
Client 3 execute local training on 8 samples...
Local loss @ local epoch 0: 0.07017400115728378
Local loss @ local epoch 1: 0.008473552763462067
Local loss @ local epoch 2: 0.002277567982673645
Local loss @ local epoch 3: 0.01811967045068741
Local loss @ local epoch 4: 0.47298842668533325
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 258.66 seconds!
[tester] 
SST2Metric: acc=0.5802752293577982, hinge=1.6415310942251748, ce=0.8192746807402427
Local test acc @ epoch 9: 0.5803
Global evaluate on test data...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 258.65 seconds!
[tester] 
SST2Metric: acc=0.8761467889908257, hinge=0.620540607002897, ce=0.34447962525001635
Global test acc @ epoch 9: 0.8761
Global epoch 10...
Client 5 execute local training on 8 samples...
Local loss @ local epoch 0: 0.023184388875961304
Local loss @ local epoch 1: 0.15149912238121033
Local loss @ local epoch 2: 0.7458042502403259
Local loss @ local epoch 3: 2.337631940841675
Local loss @ local epoch 4: 0.8683140277862549
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 258.16 seconds!
[tester] 
SST2Metric: acc=0.4908256880733945, hinge=2.0142603598603417, ce=0.7689316816833041
Local test acc @ epoch 10: 0.4908
Client 6 execute local training on 8 samples...
Local loss @ local epoch 0: 0.008604450151324272
Local loss @ local epoch 1: 0.0023809275589883327
Local loss @ local epoch 2: 0.00516893994063139
Local loss @ local epoch 3: 0.004234245512634516
Local loss @ local epoch 4: 0.00020555655646603554
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 257.02 seconds!
[tester] 
SST2Metric: acc=0.7706422018348624, hinge=1.3060151345412665, ce=0.8275051546483856
Local test acc @ epoch 10: 0.7706
Client 9 execute local training on 8 samples...
Local loss @ local epoch 0: 0.06465829908847809
Local loss @ local epoch 1: 0.10038640350103378
Local loss @ local epoch 2: 0.0461113378405571
Local loss @ local epoch 3: 0.006712810602039099
Local loss @ local epoch 4: 0.009750157594680786
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 256.76 seconds!
[tester] 
SST2Metric: acc=0.8876146788990825, hinge=0.5593090418281905, ce=0.3028367583409225
Local test acc @ epoch 10: 0.8876
Client 2 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0033536364790052176
Local loss @ local epoch 1: 0.00019695443916134536
Local loss @ local epoch 2: 0.0003900527663063258
Local loss @ local epoch 3: 0.00024354932247661054
Local loss @ local epoch 4: 3.4077485906891525e-05
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 259.68 seconds!
[tester] 
SST2Metric: acc=0.7694954128440367, hinge=1.5273953028228304, ce=1.0561416020300218
Local test acc @ epoch 10: 0.7695
Client 8 execute local training on 8 samples...
Local loss @ local epoch 0: 0.059987977147102356
Local loss @ local epoch 1: 0.027019808068871498
Local loss @ local epoch 2: 0.1155976727604866
Local loss @ local epoch 3: 0.3708871901035309
Local loss @ local epoch 4: 0.22171002626419067
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 259.04 seconds!
[tester] 
SST2Metric: acc=0.8360091743119266, hinge=0.773749817265283, ce=0.4275808626905494
Local test acc @ epoch 10: 0.836
Client 4 execute local training on 8 samples...
Local loss @ local epoch 0: 0.004074206575751305
Local loss @ local epoch 1: 0.002974415896460414
Local loss @ local epoch 2: 0.0014672669349238276
Local loss @ local epoch 3: 0.0038825846277177334
Local loss @ local epoch 4: 6.916608253959566e-05
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 258.92 seconds!
[tester] 
SST2Metric: acc=0.8153669724770642, hinge=1.1482395987718477, ce=0.7789094332910523
Local test acc @ epoch 10: 0.8154
Client 1 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0015382996061816812
Local loss @ local epoch 1: 0.0010107612470164895
Local loss @ local epoch 2: 0.000771484337747097
Local loss @ local epoch 3: 0.00027368610608391464
Local loss @ local epoch 4: 2.3170847271103412e-05
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 258.91 seconds!
[tester] 
SST2Metric: acc=0.7855504587155964, hinge=1.4633119173279596, ce=1.030252947136541
Local test acc @ epoch 10: 0.7856
Client 7 execute local training on 8 samples...
Local loss @ local epoch 0: 0.05576268956065178
Local loss @ local epoch 1: 0.04199405014514923
Local loss @ local epoch 2: 0.05974801257252693
Local loss @ local epoch 3: 0.0029952109325677156
Local loss @ local epoch 4: 0.0004845538060180843
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 259.19 seconds!
[tester] 
SST2Metric: acc=0.8463302752293578, hinge=0.8854754176161704, ce=0.5607918140739032
Local test acc @ epoch 10: 0.8463
Client 0 execute local training on 8 samples...
Local loss @ local epoch 0: 0.010412892326712608
Local loss @ local epoch 1: 0.14753393828868866
Local loss @ local epoch 2: 0.06124294176697731
Local loss @ local epoch 3: 0.0004054318997077644
Local loss @ local epoch 4: 0.0014092823257669806
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 258.82 seconds!
[tester] 
SST2Metric: acc=0.8612385321100917, hinge=0.7240658686795366, ce=0.4105810540057548
Local test acc @ epoch 10: 0.8612
Client 3 execute local training on 8 samples...
Local loss @ local epoch 0: 0.053083356469869614
Local loss @ local epoch 1: 0.002748247468844056
Local loss @ local epoch 2: 0.002958442782983184
Local loss @ local epoch 3: 0.0017648525536060333
Local loss @ local epoch 4: 0.0011118771508336067
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 258.89 seconds!
[tester] 
SST2Metric: acc=0.9105504587155964, hinge=0.5497377497067145, ce=0.3434308098122871
Local test acc @ epoch 10: 0.9106
Global evaluate on test data...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 254.93 seconds!
[tester] 
SST2Metric: acc=0.8887614678899083, hinge=0.6755600071281468, ce=0.4352600114538074
Global test acc @ epoch 10: 0.8888
Global epoch 11...
Client 5 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0030376503709703684
Local loss @ local epoch 1: 0.32806873321533203
Local loss @ local epoch 2: 0.6166143417358398
Local loss @ local epoch 3: 0.015440409071743488
Local loss @ local epoch 4: 0.004024037159979343
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 258.7 seconds!
[tester] 
SST2Metric: acc=0.8738532110091743, hinge=0.6730863112922109, ce=0.41777187764138374
Local test acc @ epoch 11: 0.8739
Client 3 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0030447514727711678
Local loss @ local epoch 1: 1.4910637140274048
Local loss @ local epoch 2: 0.030978960916399956
Local loss @ local epoch 3: 0.005852469243109226
Local loss @ local epoch 4: 0.008043687790632248
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 259.39 seconds!
[tester] 
SST2Metric: acc=0.8394495412844036, hinge=0.7918707482311704, ce=0.44797611218415745
Local test acc @ epoch 11: 0.8394
Client 4 execute local training on 8 samples...
Local loss @ local epoch 0: 0.001408079988323152
Local loss @ local epoch 1: 2.1139185428619385
Local loss @ local epoch 2: 0.0005429502343758941
Local loss @ local epoch 3: 0.007071369327604771
Local loss @ local epoch 4: 0.019899630919098854
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 259.0 seconds!
[tester] 
SST2Metric: acc=0.8291284403669725, hinge=0.88728534751529, ce=0.5276528449746174
Local test acc @ epoch 11: 0.8291
Client 0 execute local training on 8 samples...
Local loss @ local epoch 0: 0.08042630553245544
Local loss @ local epoch 1: 0.20925913751125336
Local loss @ local epoch 2: 0.006929384544491768
Local loss @ local epoch 3: 0.007860800251364708
Local loss @ local epoch 4: 0.007991519756615162
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 259.08 seconds!
[tester] 
SST2Metric: acc=0.8543577981651376, hinge=0.6760408453164845, ce=0.36342702438593455
Local test acc @ epoch 11: 0.8544
Client 9 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0012967297807335854
Local loss @ local epoch 1: 0.022133639082312584
Local loss @ local epoch 2: 0.00025504990480840206
Local loss @ local epoch 3: 0.14720094203948975
Local loss @ local epoch 4: 0.0005338407354429364
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 259.43 seconds!
[tester] 
SST2Metric: acc=0.7488532110091743, hinge=1.635137228927481, ce=1.1126798600580015
Local test acc @ epoch 11: 0.7489
Client 6 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0012154504656791687
Local loss @ local epoch 1: 1.325864553451538
Local loss @ local epoch 2: 0.0007266573375090957
Local loss @ local epoch 3: 0.00306832161732018
Local loss @ local epoch 4: 0.006866490002721548
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 258.93 seconds!
[tester] 
SST2Metric: acc=0.8715596330275229, hinge=0.6550645817310439, ce=0.3727562688632843
Local test acc @ epoch 11: 0.8716
Client 2 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0020839297212660313
Local loss @ local epoch 1: 0.9544159173965454
Local loss @ local epoch 2: 0.0059851231053471565
Local loss @ local epoch 3: 0.00552013237029314
Local loss @ local epoch 4: 0.006199799943715334
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 258.83 seconds!
[tester] 
SST2Metric: acc=0.8910550458715596, hinge=0.5369138336236324, ce=0.29569739186196425
Local test acc @ epoch 11: 0.8911
Client 7 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0009863452287390828
Local loss @ local epoch 1: 0.0007799455197528005
Local loss @ local epoch 2: 4.760560841532424e-05
Local loss @ local epoch 3: 0.0004397509328555316
Local loss @ local epoch 4: 5.813881944050081e-05
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 258.21 seconds!
[tester] 
SST2Metric: acc=0.8784403669724771, hinge=0.914787652158956, ce=0.6548765046822939
Local test acc @ epoch 11: 0.8784
Client 8 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0009801057167351246
Local loss @ local epoch 1: 0.001665754709392786
Local loss @ local epoch 2: 0.37777823209762573
Local loss @ local epoch 3: 0.004435819573700428
Local loss @ local epoch 4: 0.09216190874576569
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 256.18 seconds!
[tester] 
SST2Metric: acc=0.8600917431192661, hinge=0.9107418209314346, ce=0.6114476744193421
Local test acc @ epoch 11: 0.8601
Client 1 execute local training on 8 samples...
Local loss @ local epoch 0: 0.00013454159488901496
Local loss @ local epoch 1: 0.0022501619532704353
Local loss @ local epoch 2: 0.0010768420761451125
Local loss @ local epoch 3: 0.00011013864423148334
Local loss @ local epoch 4: 0.00012931576929986477
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 258.86 seconds!
[tester] 
SST2Metric: acc=0.838302752293578, hinge=0.9937032492609199, ce=0.6564214212423161
Local test acc @ epoch 11: 0.8383
Global evaluate on test data...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 259.13 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=0.6452608297177411, ce=0.4167290506083433
Global test acc @ epoch 11: 0.8933
Global epoch 12...
Client 5 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0016313293017446995
Local loss @ local epoch 1: 0.001265164464712143
Local loss @ local epoch 2: 0.0020095030777156353
Local loss @ local epoch 3: 1.950504520209506e-05
Local loss @ local epoch 4: 0.00014337225002236664
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 258.84 seconds!
[tester] 
SST2Metric: acc=0.8635321100917431, hinge=0.9085261365142437, ce=0.6310805581904437
Local test acc @ epoch 12: 0.8635
Client 8 execute local training on 8 samples...
Local loss @ local epoch 0: 0.006161440629512072
Local loss @ local epoch 1: 0.017266947776079178
Local loss @ local epoch 2: 0.0399169847369194
Local loss @ local epoch 3: 0.00036938668927177787
Local loss @ local epoch 4: 0.00040148815605789423
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 258.81 seconds!
[tester] 
SST2Metric: acc=0.908256880733945, hinge=0.5661308527539629, ce=0.3456623343865114
Local test acc @ epoch 12: 0.9083
Client 2 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0015513598918914795
Local loss @ local epoch 1: 2.6761985282064416e-05
Local loss @ local epoch 2: 1.794077797967475e-05
Local loss @ local epoch 3: 1.5482102753594518e-05
Local loss @ local epoch 4: 1.4796665709582157e-05
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 257.39 seconds!
[tester] 
SST2Metric: acc=0.7534403669724771, hinge=1.7013464028682184, ce=1.1933790558434856
Local test acc @ epoch 12: 0.7534
Client 0 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0006468783831223845
Local loss @ local epoch 1: 0.0002275507285958156
Local loss @ local epoch 2: 5.5412649089703336e-05
Local loss @ local epoch 3: 3.938210284104571e-05
Local loss @ local epoch 4: 4.769622319145128e-05
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 258.93 seconds!
[tester] 
SST2Metric: acc=0.8864678899082569, hinge=0.7963490816978139, ce=0.5632191069748299
Local test acc @ epoch 12: 0.8865
Client 1 execute local training on 8 samples...
Local loss @ local epoch 0: 0.00025343234301544726
Local loss @ local epoch 1: 0.00021897742408327758
Local loss @ local epoch 2: 1.388771488564089e-05
Local loss @ local epoch 3: 0.00016488398250658065
Local loss @ local epoch 4: 3.546468633430777e-06
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 258.54 seconds!
[tester] 
SST2Metric: acc=0.8428899082568807, hinge=1.2148997958647, ce=0.8919352916622169
Local test acc @ epoch 12: 0.8429
Client 3 execute local training on 8 samples...
Local loss @ local epoch 0: 0.005343521945178509
Local loss @ local epoch 1: 0.016052432358264923
Local loss @ local epoch 2: 0.3047216832637787
Local loss @ local epoch 3: 0.10433797538280487
Local loss @ local epoch 4: 0.0004177464870736003
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 257.14 seconds!
[tester] 
SST2Metric: acc=0.9048165137614679, hinge=0.5184600539163712, ce=0.3173606275696171
Local test acc @ epoch 12: 0.9048
Client 6 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0003653062740340829
Local loss @ local epoch 1: 2.6076226276927628e-05
Local loss @ local epoch 2: 3.814683850578149e-06
Local loss @ local epoch 3: 3.7401789541036123e-06
Local loss @ local epoch 4: 6.657986523350701e-05
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 258.69 seconds!
[tester] 
SST2Metric: acc=0.5974770642201835, hinge=3.1956315472585346, ce=2.3833104570678203
Local test acc @ epoch 12: 0.5975
Client 9 execute local training on 8 samples...
Local loss @ local epoch 0: 0.00028760795248672366
Local loss @ local epoch 1: 4.6042590838624164e-05
Local loss @ local epoch 2: 4.887559043709189e-06
Local loss @ local epoch 3: 3.233541974623222e-06
Local loss @ local epoch 4: 1.7583349745109444e-06
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 259.11 seconds!
[tester] 
SST2Metric: acc=0.8841743119266054, hinge=1.1039342833768337, ce=0.8767549560870225
Local test acc @ epoch 12: 0.8842
Client 7 execute local training on 8 samples...
Local loss @ local epoch 0: 0.000906142988242209
Local loss @ local epoch 1: 0.00027402161504141986
Local loss @ local epoch 2: 0.883389413356781
Local loss @ local epoch 3: 0.003477397607639432
Local loss @ local epoch 4: 0.05838259309530258
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 258.56 seconds!
[tester] 
SST2Metric: acc=0.8348623853211009, hinge=1.0285106707330143, ce=0.6868824195138119
Local test acc @ epoch 12: 0.8349
Client 4 execute local training on 8 samples...
Local loss @ local epoch 0: 0.00025494155124761164
Local loss @ local epoch 1: 0.03138808161020279
Local loss @ local epoch 2: 7.031115092104301e-05
Local loss @ local epoch 3: 0.004663496743887663
Local loss @ local epoch 4: 0.0006494785193353891
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 258.49 seconds!
[tester] 
SST2Metric: acc=0.7775229357798165, hinge=1.3658039520639893, ce=0.9128768054878984
Local test acc @ epoch 12: 0.7775
Global evaluate on test data...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 258.57 seconds!
[tester] 
SST2Metric: acc=0.8910550458715596, hinge=0.9038451297567525, ce=0.680312735679396
Global test acc @ epoch 12: 0.8911
Global epoch 13...
Client 6 execute local training on 8 samples...
Local loss @ local epoch 0: 9.611158930056263e-06
Local loss @ local epoch 1: 1.3724753856658936
Local loss @ local epoch 2: 0.01899091899394989
Local loss @ local epoch 3: 0.021246500313282013
Local loss @ local epoch 4: 0.0021540538873523474
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 258.79 seconds!
[tester] 
SST2Metric: acc=0.8038990825688074, hinge=1.1641843287496392, ce=0.7472839513142089
Local test acc @ epoch 13: 0.8039
Client 0 execute local training on 8 samples...
Local loss @ local epoch 0: 1.4766557796974666e-05
Local loss @ local epoch 1: 0.0002963428560178727
Local loss @ local epoch 2: 0.06012692302465439
Local loss @ local epoch 3: 0.004989936947822571
Local loss @ local epoch 4: 0.012009331956505775
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 258.63 seconds!
[tester] 
SST2Metric: acc=0.8543577981651376, hinge=0.843026018607507, ce=0.5446566811948588
Local test acc @ epoch 13: 0.8544
Client 9 execute local training on 8 samples...
Local loss @ local epoch 0: 2.0950272300979123e-05
Local loss @ local epoch 1: 0.0005160482251085341
Local loss @ local epoch 2: 1.0654202924342826e-05
Local loss @ local epoch 3: 0.00015137875743675977
Local loss @ local epoch 4: 5.588976637227461e-05
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 258.01 seconds!
[tester] 
SST2Metric: acc=0.8715596330275229, hinge=1.0057368687260042, ce=0.7502286027574716
Local test acc @ epoch 13: 0.8716
Client 5 execute local training on 8 samples...
Local loss @ local epoch 0: 2.2022473785909824e-05
Local loss @ local epoch 1: 0.11982014030218124
Local loss @ local epoch 2: 0.0020252710673958063
Local loss @ local epoch 3: 0.041170645505189896
Local loss @ local epoch 4: 0.009709453210234642
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 254.51 seconds!
[tester] 
SST2Metric: acc=0.8635321100917431, hinge=0.7785635523566412, ce=0.49201073944611357
Local test acc @ epoch 13: 0.8635
Client 8 execute local training on 8 samples...
Local loss @ local epoch 0: 3.8783411582699046e-05
Local loss @ local epoch 1: 0.00044363466440699995
Local loss @ local epoch 2: 0.17174987494945526
Local loss @ local epoch 3: 0.00016860365576576442
Local loss @ local epoch 4: 0.0009426132310181856
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 258.65 seconds!
[tester] 
SST2Metric: acc=0.9151376146788991, hinge=0.5287393491475953, ce=0.35216231621297744
Local test acc @ epoch 13: 0.9151
Client 3 execute local training on 8 samples...
Local loss @ local epoch 0: 0.00021671967988368124
Local loss @ local epoch 1: 0.00032626307802274823
Local loss @ local epoch 2: 2.191876046708785e-05
Local loss @ local epoch 3: 0.000777444860432297
Local loss @ local epoch 4: 4.514397144317627
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 257.04 seconds!
[tester] 
SST2Metric: acc=0.4908256880733945, hinge=3.0369348345546547, ce=2.038848775242447
Local test acc @ epoch 13: 0.4908
Client 1 execute local training on 8 samples...
Local loss @ local epoch 0: 9.789976502361242e-06
Local loss @ local epoch 1: 5.617666829493828e-06
Local loss @ local epoch 2: 4.336198799137492e-06
Local loss @ local epoch 3: 5.215403575675737e-07
Local loss @ local epoch 4: 1.4156086081129615e-06
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 256.86 seconds!
[tester] 
SST2Metric: acc=0.8772935779816514, hinge=1.2851417097476645, ce=1.0360455519588254
Local test acc @ epoch 13: 0.8773
Client 4 execute local training on 8 samples...
Local loss @ local epoch 0: 5.261325713945553e-05
Local loss @ local epoch 1: 0.3443237841129303
Local loss @ local epoch 2: 0.00013073062291368842
Local loss @ local epoch 3: 0.2363000214099884
Local loss @ local epoch 4: 1.0993198156356812
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 257.63 seconds!
[tester] 
SST2Metric: acc=0.7568807339449541, hinge=1.0925518333091648, ce=0.5173052439312322
Local test acc @ epoch 13: 0.7569
Client 2 execute local training on 8 samples...
Local loss @ local epoch 0: 4.067999270773726e-06
Local loss @ local epoch 1: 4.3063982957392e-06
Local loss @ local epoch 2: 0.03453140705823898
Local loss @ local epoch 3: 2.8312138056207914e-06
Local loss @ local epoch 4: 8.985300155472942e-06
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 257.71 seconds!
[tester] 
SST2Metric: acc=0.7717889908256881, hinge=1.5953716314440474, ce=1.1078745694114767
Local test acc @ epoch 13: 0.7718
Client 7 execute local training on 8 samples...
Local loss @ local epoch 0: 4.798146619577892e-06
Local loss @ local epoch 1: 4.872628323937533e-06
Local loss @ local epoch 2: 0.001097565283998847
Local loss @ local epoch 3: 0.0005649719969369471
Local loss @ local epoch 4: 2.2299890518188477
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 257.79 seconds!
[tester] 
SST2Metric: acc=0.4908256880733945, hinge=5.17675544581282, ce=4.158720807197991
Local test acc @ epoch 13: 0.4908
Global evaluate on test data...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 257.76 seconds!
[tester] 
SST2Metric: acc=0.9208715596330275, hinge=0.7712729943728228, ce=0.6096427261641799
Global test acc @ epoch 13: 0.9209
Global epoch 14...
Client 9 execute local training on 8 samples...
Local loss @ local epoch 0: 1.996751507249428e-06
Local loss @ local epoch 1: 7.301561026906711e-07
Local loss @ local epoch 2: 1.3652582168579102
Local loss @ local epoch 3: 0.002861885353922844
Local loss @ local epoch 4: 0.1086612194776535
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 254.65 seconds!
[tester] 
SST2Metric: acc=0.8337155963302753, hinge=1.2702571833079015, ce=0.9212873701259564
Local test acc @ epoch 14: 0.8337
Client 5 execute local training on 8 samples...
Local loss @ local epoch 0: 3.8444827623607125e-06
Local loss @ local epoch 1: 8.121047358145006e-06
Local loss @ local epoch 2: 4.753403118229471e-06
Local loss @ local epoch 3: 6.988504537730478e-06
Local loss @ local epoch 4: 4.023310680167924e-07
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 253.53 seconds!
[tester] 
SST2Metric: acc=0.8428899082568807, hinge=2.0661807619377015, ce=1.751830724217502
Local test acc @ epoch 14: 0.8429
Client 6 execute local training on 8 samples...
Local loss @ local epoch 0: 2.7865053198183887e-06
Local loss @ local epoch 1: 3.4272656534994894e-07
Local loss @ local epoch 2: 2.0861622829215776e-07
Local loss @ local epoch 3: 4.917376941193652e-07
Local loss @ local epoch 4: 1.6067163944244385
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 257.89 seconds!
[tester] 
SST2Metric: acc=0.841743119266055, hinge=1.4975740212366122, ce=1.1739194833888744
Local test acc @ epoch 14: 0.8417
Client 3 execute local training on 8 samples...
Local loss @ local epoch 0: 2.3021260858513415e-05
Local loss @ local epoch 1: 8.80645529832691e-06
Local loss @ local epoch 2: 3.3898893889272586e-05
Local loss @ local epoch 3: 2.0592702639987692e-05
Local loss @ local epoch 4: 2.3245765987667255e-06
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 259.03 seconds!
[tester] 
SST2Metric: acc=0.8899082568807339, hinge=1.0008906506070303, ce=0.779459001132895
Local test acc @ epoch 14: 0.8899
Client 8 execute local training on 8 samples...
Local loss @ local epoch 0: 2.1904579625697806e-06
Local loss @ local epoch 1: 7.599539003422251e-06
Local loss @ local epoch 2: 2.4735859369684476e-06
Local loss @ local epoch 3: 3.9636929614061955e-06
Local loss @ local epoch 4: 1.0430811414607888e-07
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 258.25 seconds!
[tester] 
SST2Metric: acc=0.9071100917431193, hinge=1.0210808929773645, ce=0.8340743497395441
Local test acc @ epoch 14: 0.9071
Client 2 execute local training on 8 samples...
Local loss @ local epoch 0: 5.662437843056978e-07
Local loss @ local epoch 1: 4.470348002882929e-08
Local loss @ local epoch 2: 1.4901160305669237e-08
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 2.9802320611338473e-08
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 258.17 seconds!
[tester] 
SST2Metric: acc=0.8658256880733946, hinge=1.5318219758775256, ce=1.2614511964671067
Local test acc @ epoch 14: 0.8658
Client 1 execute local training on 8 samples...
Local loss @ local epoch 0: 6.109473247306596e-07
Local loss @ local epoch 1: 5.9604641222676946e-08
Local loss @ local epoch 2: 4.470348002882929e-08
Local loss @ local epoch 3: 7.450579886381092e-08
Local loss @ local epoch 4: 2.9802320611338473e-08
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 259.32 seconds!
[tester] 
SST2Metric: acc=0.9002293577981652, hinge=1.2794936910408352, ce=1.0793996415668752
Local test acc @ epoch 14: 0.9002
Client 4 execute local training on 8 samples...
Local loss @ local epoch 0: 4.738533334602835e-06
Local loss @ local epoch 1: 9.536730658510351e-07
Local loss @ local epoch 2: 3.7252860352054995e-07
Local loss @ local epoch 3: 1.937150670983101e-07
Local loss @ local epoch 4: 1.7881392011531716e-07
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 258.99 seconds!
[tester] 
SST2Metric: acc=0.8681192660550459, hinge=1.4049163798400022, ce=1.137935817362277
Local test acc @ epoch 14: 0.8681
Client 7 execute local training on 8 samples...
Local loss @ local epoch 0: 2.965319936265587e-06
Local loss @ local epoch 1: 6.365729495882988e-05
Local loss @ local epoch 2: 0.015235253609716892
Local loss @ local epoch 3: 6.914033292559907e-06
Local loss @ local epoch 4: 3.0947143386583775e-05
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 258.83 seconds!
[tester] 
SST2Metric: acc=0.8979357798165137, hinge=0.9399713701611265, ce=0.7366935057883843
Local test acc @ epoch 14: 0.8979
Client 0 execute local training on 8 samples...
Local loss @ local epoch 0: 2.458672952343477e-06
Local loss @ local epoch 1: 1.7434326764487196e-06
Local loss @ local epoch 2: 4.917380920232972e-07
Local loss @ local epoch 3: 4.3213330513935944e-07
Local loss @ local epoch 4: 2.2351733264258655e-07
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 254.05 seconds!
[tester] 
SST2Metric: acc=0.908256880733945, hinge=0.9364900830962243, ce=0.7516729385642641
Local test acc @ epoch 14: 0.9083
Global evaluate on test data...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 259.24 seconds!
[tester] 
SST2Metric: acc=0.9243119266055045, hinge=0.9721705548533606, ce=0.8191818457769462
Global test acc @ epoch 14: 0.9243
Global epoch 15...
Client 2 execute local training on 8 samples...
Local loss @ local epoch 0: 1.1920927533992653e-07
Local loss @ local epoch 1: 2.6822064569387294e-07
Local loss @ local epoch 2: 6.258483153942507e-07
Local loss @ local epoch 3: 2.1515164375305176
Local loss @ local epoch 4: 7.077863756421721e-06
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 258.78 seconds!
[tester] 
SST2Metric: acc=0.8818807339449541, hinge=1.054493579432505, ce=0.8064613270475084
Local test acc @ epoch 15: 0.8819
Client 7 execute local training on 8 samples...
Local loss @ local epoch 0: 1.341104365337742e-07
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 258.76 seconds!
[tester] 
SST2Metric: acc=0.9151376146788991, hinge=1.2251465908431132, ce=1.0522809131461155
Local test acc @ epoch 15: 0.9151
Client 5 execute local training on 8 samples...
Local loss @ local epoch 0: 5.960463766996327e-08
Local loss @ local epoch 1: 7.450579886381092e-08
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 258.72 seconds!
[tester] 
SST2Metric: acc=0.8944954128440367, hinge=1.671055608933125, ce=1.4609157480520805
Local test acc @ epoch 15: 0.8945
Client 3 execute local training on 8 samples...
Local loss @ local epoch 0: 6.109466994530521e-07
Local loss @ local epoch 1: 2.8311949336057296e-06
Local loss @ local epoch 2: 5.960459930065554e-07
Local loss @ local epoch 3: 1.0132773695659125e-06
Local loss @ local epoch 4: 2.2351736106429598e-07
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 258.93 seconds!
[tester] 
SST2Metric: acc=0.9025229357798165, hinge=1.2860489628457148, ce=1.0864496787060318
Local test acc @ epoch 15: 0.9025
Client 1 execute local training on 8 samples...
Local loss @ local epoch 0: 4.470348002882929e-08
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 258.72 seconds!
[tester] 
SST2Metric: acc=0.9208715596330275, hinge=1.2292917994184231, ce=1.0692079394049803
Local test acc @ epoch 15: 0.9209
Client 8 execute local training on 8 samples...
Local loss @ local epoch 0: 1.3411042232291948e-07
Local loss @ local epoch 1: 2.2351736106429598e-07
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 7.450579886381092e-08
Local loss @ local epoch 4: 1.4901160305669237e-08
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 258.98 seconds!
[tester] 
SST2Metric: acc=0.8830275229357798, hinge=1.6428428098149257, ce=1.409456106689818
Local test acc @ epoch 15: 0.883
Client 0 execute local training on 8 samples...
Local loss @ local epoch 0: 5.9604641222676946e-08
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 256.84 seconds!
[tester] 
SST2Metric: acc=0.9197247706422018, hinge=1.2920998761413294, ce=1.1298348725749519
Local test acc @ epoch 15: 0.9197
Client 9 execute local training on 8 samples...
Local loss @ local epoch 0: 1.937150670983101e-07
Local loss @ local epoch 1: 4.470348002882929e-08
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 256.15 seconds!
[tester] 
SST2Metric: acc=0.8658256880733946, hinge=2.1624024072918324, ce=1.8946369797985956
Local test acc @ epoch 15: 0.8658
Client 6 execute local training on 8 samples...
Local loss @ local epoch 0: 1.4901158351676713e-07
Local loss @ local epoch 1: 8.225173587561585e-06
Local loss @ local epoch 2: 3.4291832447052
Local loss @ local epoch 3: 9.26837219594745e-06
Local loss @ local epoch 4: 0.0005246712826192379
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 258.83 seconds!
[tester] 
SST2Metric: acc=0.7224770642201835, hinge=1.7562609804332803, ce=1.166545563240198
Local test acc @ epoch 15: 0.7225
Client 4 execute local training on 8 samples...
Local loss @ local epoch 0: 1.4901159772762185e-07
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 259.13 seconds!
[tester] 
SST2Metric: acc=0.8944954128440367, hinge=1.4785810698063002, ce=1.2632568701744074
Local test acc @ epoch 15: 0.8945
Global evaluate on test data...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 258.95 seconds!
[tester] 
SST2Metric: acc=0.8944954128440367, hinge=1.589396420832074, ce=1.3782273443896305
Global test acc @ epoch 15: 0.8945
Global epoch 16...
Client 6 execute local training on 8 samples...
Local loss @ local epoch 0: 2.9802318834981634e-08
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 259.19 seconds!
[tester] 
SST2Metric: acc=0.8979357798165137, hinge=1.6408804299634532, ce=1.4349106991510945
Local test acc @ epoch 16: 0.8979
Client 3 execute local training on 8 samples...
Local loss @ local epoch 0: 4.321331630308123e-07
Local loss @ local epoch 1: 2.9802318834981634e-08
Local loss @ local epoch 2: 1.9371505288745539e-07
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 258.96 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=1.5068543941602794, ce=1.295709253630542
Local test acc @ epoch 16: 0.8933
Client 7 execute local training on 8 samples...
Local loss @ local epoch 0: 5.9604641222676946e-08
Local loss @ local epoch 1: 1.4901160305669237e-08
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 259.05 seconds!
[tester] 
SST2Metric: acc=0.9151376146788991, hinge=1.4440310120309165, ce=1.2714520865242176
Local test acc @ epoch 16: 0.9151
Client 8 execute local training on 8 samples...
Local loss @ local epoch 0: 1.192092469182171e-07
Local loss @ local epoch 1: 1.4901160305669237e-08
Local loss @ local epoch 2: 0.009600983001291752
Local loss @ local epoch 3: 1.4901158351676713e-07
Local loss @ local epoch 4: 0.12623289227485657
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 258.36 seconds!
[tester] 
SST2Metric: acc=0.9128440366972477, hinge=0.7356470745364461, ce=0.5547478457621197
Local test acc @ epoch 16: 0.9128
Client 4 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 256.05 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=1.6843097487447458, ce=1.4681437974078169
Local test acc @ epoch 16: 0.8933
Client 9 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 258.58 seconds!
[tester] 
SST2Metric: acc=0.8979357798165137, hinge=1.6037764132296273, ce=1.3969896834599478
Local test acc @ epoch 16: 0.8979
Client 2 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 258.96 seconds!
[tester] 
SST2Metric: acc=0.8944954128440367, hinge=1.6060093627610337, ce=1.393783164498334
Local test acc @ epoch 16: 0.8945
Client 1 execute local training on 8 samples...
Local loss @ local epoch 0: 4.470348002882929e-08
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 259.06 seconds!
[tester] 
SST2Metric: acc=0.9105504587155964, hinge=1.5473256015449488, ce=1.3687027041180502
Local test acc @ epoch 16: 0.9106
Client 0 execute local training on 8 samples...
Local loss @ local epoch 0: 2.9802320611338473e-08
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 258.52 seconds!
[tester] 
SST2Metric: acc=0.8967889908256881, hinge=1.5407117156807435, ce=1.3356401073529223
Local test acc @ epoch 16: 0.8968
Client 5 execute local training on 8 samples...
Local loss @ local epoch 0: 5.9604641222676946e-08
Local loss @ local epoch 1: 1.4901160305669237e-08
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 259.34 seconds!
[tester] 
SST2Metric: acc=0.9025229357798165, hinge=1.635127406601512, ce=1.438741731802299
Local test acc @ epoch 16: 0.9025
Global evaluate on test data...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 258.82 seconds!
[tester] 
SST2Metric: acc=0.9025229357798165, hinge=1.5240789091368334, ce=1.3300751517380005
Global test acc @ epoch 16: 0.9025
Global epoch 17...
Client 8 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 259.09 seconds!
[tester] 
SST2Metric: acc=0.9025229357798165, hinge=1.5834898533077415, ce=1.3903089868905005
Local test acc @ epoch 17: 0.9025
Client 0 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 256.46 seconds!
[tester] 
SST2Metric: acc=0.9013761467889908, hinge=1.6112970783622986, ce=1.4123965115636805
Local test acc @ epoch 17: 0.9014
Client 4 execute local training on 8 samples...
Local loss @ local epoch 0: 1.4901160305669237e-08
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 258.19 seconds!
[tester] 
SST2Metric: acc=0.8887614678899083, hinge=2.0162831302903115, ce=1.7900046169075756
Local test acc @ epoch 17: 0.8888
Client 3 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 259.01 seconds!
[tester] 
SST2Metric: acc=0.8956422018348624, hinge=1.6587449722333785, ce=1.4520050124264512
Local test acc @ epoch 17: 0.8956
Client 2 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 259.13 seconds!
[tester] 
SST2Metric: acc=0.9036697247706422, hinge=1.6402560434210192, ce=1.4462916939083226
Local test acc @ epoch 17: 0.9037
Client 7 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 258.93 seconds!
[tester] 
SST2Metric: acc=0.9048165137614679, hinge=1.62328602113855, ce=1.4322377955146268
Local test acc @ epoch 17: 0.9048
Client 6 execute local training on 8 samples...
Local loss @ local epoch 0: 1.4901160305669237e-08
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 258.82 seconds!
[tester] 
SST2Metric: acc=0.8876146788990825, hinge=1.949974465260812, ce=1.7214641248775069
Local test acc @ epoch 17: 0.8876
Client 1 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 258.75 seconds!
[tester] 
SST2Metric: acc=0.9025229357798165, hinge=1.6321058139341686, ce=1.4373949514037005
Local test acc @ epoch 17: 0.9025
Client 5 execute local training on 8 samples...
Local loss @ local epoch 0: 1.4901160305669237e-08
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 259.0 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=1.9236118779816758, ce=1.708291231048366
Local test acc @ epoch 17: 0.8933
Client 9 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 259.09 seconds!
[tester] 
SST2Metric: acc=0.9036697247706422, hinge=1.6988383971769876, ce=1.5077176927621134
Local test acc @ epoch 17: 0.9037
Global evaluate on test data...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 257.58 seconds!
[tester] 
SST2Metric: acc=0.8990825688073395, hinge=1.738641596168553, ce=1.53687166177139
Global test acc @ epoch 17: 0.8991
Global epoch 18...
Client 2 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 256.16 seconds!
[tester] 
SST2Metric: acc=0.8990825688073395, hinge=1.771037160256587, ce=1.5692982830108904
Local test acc @ epoch 18: 0.8991
Client 6 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 259.17 seconds!
[tester] 
SST2Metric: acc=0.9002293577981652, hinge=1.7914468635112868, ce=1.5893662563589646
Local test acc @ epoch 18: 0.9002
Client 3 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 259.18 seconds!
[tester] 
SST2Metric: acc=0.8990825688073395, hinge=1.7530808913598366, ce=1.5512491313452221
Local test acc @ epoch 18: 0.8991
Client 0 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 259.07 seconds!
[tester] 
SST2Metric: acc=0.8990825688073395, hinge=1.7572162236095568, ce=1.5552065001523072
Local test acc @ epoch 18: 0.8991
Client 1 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 258.86 seconds!
[tester] 
SST2Metric: acc=0.8990825688073395, hinge=1.7566353809942894, ce=1.555087071541311
Local test acc @ epoch 18: 0.8991
Client 8 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 258.92 seconds!
[tester] 
SST2Metric: acc=0.8990825688073395, hinge=1.7495577420116564, ce=1.5482532997846765
Local test acc @ epoch 18: 0.8991
Client 4 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 258.38 seconds!
[tester] 
SST2Metric: acc=0.8990825688073395, hinge=1.8141555151808153, ce=1.6112680056102395
Local test acc @ epoch 18: 0.8991
Client 9 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 256.75 seconds!
[tester] 
SST2Metric: acc=0.8990825688073395, hinge=1.8365888910009227, ce=1.632944057105295
Local test acc @ epoch 18: 0.8991
Client 7 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 257.33 seconds!
[tester] 
SST2Metric: acc=0.8990825688073395, hinge=1.7461109926941198, ce=1.5453229487797078
Local test acc @ epoch 18: 0.8991
Client 5 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 259.49 seconds!
[tester] 
SST2Metric: acc=0.8990825688073395, hinge=1.782407438262887, ce=1.5808337518048359
Local test acc @ epoch 18: 0.8991
Global evaluate on test data...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 259.69 seconds!
[tester] 
SST2Metric: acc=0.8990825688073395, hinge=1.7772366628734344, ce=1.575176318332096
Global test acc @ epoch 18: 0.8991
Global epoch 19...
Client 2 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 259.33 seconds!
[tester] 
SST2Metric: acc=0.8990825688073395, hinge=1.803370130991717, ce=1.6012578495329421
Local test acc @ epoch 19: 0.8991
Client 5 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 259.31 seconds!
[tester] 
SST2Metric: acc=0.8990825688073395, hinge=1.810484547680671, ce=1.6085822089899395
Local test acc @ epoch 19: 0.8991
Client 7 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 259.16 seconds!
[tester] 
SST2Metric: acc=0.8990825688073395, hinge=1.781713215856377, ce=1.580224048395676
Local test acc @ epoch 19: 0.8991
Client 4 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 258.95 seconds!
[tester] 
SST2Metric: acc=0.8990825688073395, hinge=1.8358699462009131, ce=1.6328380632373336
Local test acc @ epoch 19: 0.8991
Client 3 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 259.29 seconds!
[tester] 
SST2Metric: acc=0.8990825688073395, hinge=1.7860120917678972, ce=1.5838652823570007
Local test acc @ epoch 19: 0.8991
Client 0 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 257.28 seconds!
[tester] 
SST2Metric: acc=0.8990825688073395, hinge=1.7919282637058047, ce=1.5897280770015951
Local test acc @ epoch 19: 0.8991
Client 6 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 256.29 seconds!
[tester] 
SST2Metric: acc=0.8990825688073395, hinge=1.818493810405425, ce=1.616141767127281
Local test acc @ epoch 19: 0.8991
Client 1 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 259.65 seconds!
[tester] 
SST2Metric: acc=0.8990825688073395, hinge=1.7904968122276692, ce=1.5885463748148752
Local test acc @ epoch 19: 0.8991
Client 8 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 259.13 seconds!
[tester] 
SST2Metric: acc=0.8990825688073395, hinge=1.7863304363478214, ce=1.58451289490592
Local test acc @ epoch 19: 0.8991
Client 9 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 259.22 seconds!
[tester] 
SST2Metric: acc=0.8979357798165137, hinge=1.8590736565513348, ce=1.6555169399940428
Local test acc @ epoch 19: 0.8979
Global evaluate on test data...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 259.03 seconds!
[tester] 
SST2Metric: acc=0.8990825688073395, hinge=1.8072266827482697, ce=1.6050014363201395
Global test acc @ epoch 19: 0.8991
Global epoch 20...
Client 7 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 258.95 seconds!
[tester] 
SST2Metric: acc=0.8990825688073395, hinge=1.8100468669462642, ce=1.6081477548255088
Local test acc @ epoch 20: 0.8991
Client 4 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 258.67 seconds!
[tester] 
SST2Metric: acc=0.8990825688073395, hinge=1.8560346873528366, ce=1.65245794141993
Local test acc @ epoch 20: 0.8991
Client 3 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 259.17 seconds!
[tester] 
SST2Metric: acc=0.8990825688073395, hinge=1.8128133175569936, ce=1.6104332321173342
Local test acc @ epoch 20: 0.8991
Client 1 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 259.1 seconds!
[tester] 
SST2Metric: acc=0.8990825688073395, hinge=1.8177051347330075, ce=1.6155257470439142
Local test acc @ epoch 20: 0.8991
Client 2 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 255.89 seconds!
[tester] 
SST2Metric: acc=0.8979357798165137, hinge=1.8294804325891196, ce=1.627127427473681
Local test acc @ epoch 20: 0.8979
Client 8 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 259.49 seconds!
[tester] 
SST2Metric: acc=0.8990825688073395, hinge=1.8150102211794723, ce=1.6129274556992625
Local test acc @ epoch 20: 0.8991
Client 9 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 258.92 seconds!
[tester] 
SST2Metric: acc=0.8979357798165137, hinge=1.8779150006967946, ce=1.674369117919169
Local test acc @ epoch 20: 0.8979
Client 0 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 259.5 seconds!
[tester] 
SST2Metric: acc=0.8979357798165137, hinge=1.819537296207673, ce=1.6172587062338186
Local test acc @ epoch 20: 0.8979
Client 6 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 258.52 seconds!
[tester] 
SST2Metric: acc=0.8990825688073395, hinge=1.8414092539647304, ce=1.6388294526412932
Local test acc @ epoch 20: 0.8991
Client 5 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 258.93 seconds!
[tester] 
SST2Metric: acc=0.8990825688073395, hinge=1.834474084710856, ce=1.6322469928750332
Local test acc @ epoch 20: 0.8991
Global evaluate on test data...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 259.25 seconds!
[tester] 
SST2Metric: acc=0.8979357798165137, hinge=1.8321819029269961, ce=1.6295607103897027
Global test acc @ epoch 20: 0.8979
Global epoch 21...
Client 0 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 257.3 seconds!
[tester] 
SST2Metric: acc=0.8990825688073395, hinge=1.843074680468358, ce=1.6402121179595184
Local test acc @ epoch 21: 0.8991
Client 6 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 257.44 seconds!
[tester] 
SST2Metric: acc=0.8990825688073395, hinge=1.8618765566327156, ce=1.658501648126968
Local test acc @ epoch 21: 0.8991
Client 4 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 259.58 seconds!
[tester] 
SST2Metric: acc=0.8990825688073395, hinge=1.8742684961185543, ce=1.6701818394290437
Local test acc @ epoch 21: 0.8991
Client 1 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 259.49 seconds!
[tester] 
SST2Metric: acc=0.8979357798165137, hinge=1.8409925991788916, ce=1.638293241234322
Local test acc @ epoch 21: 0.8979
Client 5 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 259.01 seconds!
[tester] 
SST2Metric: acc=0.8990825688073395, hinge=1.8555899868044285, ce=1.6526906573559175
Local test acc @ epoch 21: 0.8991
Client 7 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 259.2 seconds!
[tester] 
SST2Metric: acc=0.8990825688073395, hinge=1.8340631634543796, ce=1.6315579429381575
Local test acc @ epoch 21: 0.8991
Client 9 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 258.78 seconds!
[tester] 
SST2Metric: acc=0.8979357798165137, hinge=1.8946132695456164, ce=1.6908485023100952
Local test acc @ epoch 21: 0.8979
Client 3 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 258.96 seconds!
[tester] 
SST2Metric: acc=0.8990825688073395, hinge=1.8360699248423271, ce=1.6330337063501283
Local test acc @ epoch 21: 0.8991
Client 2 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 257.24 seconds!
[tester] 
SST2Metric: acc=0.8990825688073395, hinge=1.8519924493557816, ce=1.6489948149136693
Local test acc @ epoch 21: 0.8991
Client 8 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 259.02 seconds!
[tester] 
SST2Metric: acc=0.8979357798165137, hinge=1.8392775976056353, ce=1.6365310790322618
Local test acc @ epoch 21: 0.8979
Global evaluate on test data...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 256.12 seconds!
[tester] 
SST2Metric: acc=0.8990825688073395, hinge=1.8537303689149542, ce=1.6505450759747384
Global test acc @ epoch 21: 0.8991
Global epoch 22...
Client 2 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 258.92 seconds!
[tester] 
SST2Metric: acc=0.8990825688073395, hinge=1.871416266084811, ce=1.6679573156916825
Local test acc @ epoch 22: 0.8991
Client 5 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 259.21 seconds!
[tester] 
SST2Metric: acc=0.8990825688073395, hinge=1.8741567868978606, ce=1.6707675410581317
Local test acc @ epoch 22: 0.8991
Client 7 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 259.36 seconds!
[tester] 
SST2Metric: acc=0.8979357798165137, hinge=1.8549341805484316, ce=1.651729317514612
Local test acc @ epoch 22: 0.8979
Client 3 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 258.91 seconds!
[tester] 
SST2Metric: acc=0.8979357798165137, hinge=1.85638327896595, ce=1.6527152610424798
Local test acc @ epoch 22: 0.8979
Client 1 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 259.57 seconds!
[tester] 
SST2Metric: acc=0.8979357798165137, hinge=1.8613380029660846, ce=1.6580509157680117
Local test acc @ epoch 22: 0.8979
Client 0 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 259.04 seconds!
[tester] 
SST2Metric: acc=0.8990825688073395, hinge=1.863443836706494, ce=1.6601027296672264
Local test acc @ epoch 22: 0.8991
Client 9 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 257.24 seconds!
[tester] 
SST2Metric: acc=0.8979357798165137, hinge=1.9100596122785445, ce=1.7056205827919115
Local test acc @ epoch 22: 0.8979
Client 6 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 258.94 seconds!
[tester] 
SST2Metric: acc=0.8990825688073395, hinge=1.880026444109208, ce=1.6760215942265897
Local test acc @ epoch 22: 0.8991
Client 4 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 258.57 seconds!
[tester] 
SST2Metric: acc=0.8990825688073395, hinge=1.890777808537177, ce=1.6862887291061053
Local test acc @ epoch 22: 0.8991
Client 8 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 259.06 seconds!
[tester] 
SST2Metric: acc=0.8990825688073395, hinge=1.8600953226789423, ce=1.6567707343908655
Local test acc @ epoch 22: 0.8991
Global evaluate on test data...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 258.6 seconds!
[tester] 
SST2Metric: acc=0.8990825688073395, hinge=1.8725353182729232, ce=1.668943078174457
Global test acc @ epoch 22: 0.8991
Global epoch 23...
Client 1 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 258.79 seconds!
[tester] 
SST2Metric: acc=0.8990825688073395, hinge=1.8792574602529544, ce=1.6755465399442298
Local test acc @ epoch 23: 0.8991
Client 0 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 258.98 seconds!
[tester] 
SST2Metric: acc=0.8990825688073395, hinge=1.8812988877569863, ce=1.677629376282928
Local test acc @ epoch 23: 0.8991
Client 2 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 258.46 seconds!
[tester] 
SST2Metric: acc=0.8990825688073395, hinge=1.8886208472722168, ce=1.6848511735351535
Local test acc @ epoch 23: 0.8991
Client 8 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 257.89 seconds!
[tester] 
SST2Metric: acc=0.8990825688073395, hinge=1.8782748346208433, ce=1.6745479082616324
Local test acc @ epoch 23: 0.8991
Client 6 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 259.26 seconds!
[tester] 
SST2Metric: acc=0.8990825688073395, hinge=1.8962593436787982, ce=1.6917698262890033
Local test acc @ epoch 23: 0.8991
Client 4 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 255.45 seconds!
[tester] 
SST2Metric: acc=0.8979357798165137, hinge=1.9057892452959622, ce=1.7009960179558947
Local test acc @ epoch 23: 0.8979
Client 9 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 259.14 seconds!
[tester] 
SST2Metric: acc=0.8979357798165137, hinge=1.924116579627772, ce=1.7190870780241734
Local test acc @ epoch 23: 0.8979
Client 7 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 259.46 seconds!
[tester] 
SST2Metric: acc=0.8979357798165137, hinge=1.8733118441673593, ce=1.6695951469702255
Local test acc @ epoch 23: 0.8979
Client 3 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 259.0 seconds!
[tester] 
SST2Metric: acc=0.8979357798165137, hinge=1.8744457447474157, ce=1.6702866765511775
Local test acc @ epoch 23: 0.8979
Client 5 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 259.08 seconds!
[tester] 
SST2Metric: acc=0.8990825688073395, hinge=1.8907617640878083, ce=1.6870350318327092
Local test acc @ epoch 23: 0.8991
Global evaluate on test data...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 259.28 seconds!
[tester] 
SST2Metric: acc=0.8990825688073395, hinge=1.8892570781871814, ce=1.6853918332397406
Global test acc @ epoch 23: 0.8991
Global epoch 24...
Client 3 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 258.91 seconds!
[tester] 
SST2Metric: acc=0.8979357798165137, hinge=1.8906435058751236, ce=1.6861198322159485
Local test acc @ epoch 24: 0.8979
Client 0 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 256.86 seconds!
[tester] 
SST2Metric: acc=0.8979357798165137, hinge=1.8972704648424725, ce=1.6933994209000145
Local test acc @ epoch 24: 0.8979
Client 7 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 258.64 seconds!
[tester] 
SST2Metric: acc=0.8979357798165137, hinge=1.8896645261060208, ce=1.6855925743111027
Local test acc @ epoch 24: 0.8979
Client 1 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 256.78 seconds!
[tester] 
SST2Metric: acc=0.8990825688073395, hinge=1.8953348965546406, ce=1.691336412368069
Local test acc @ epoch 24: 0.8991
Client 6 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 259.05 seconds!
[tester] 
SST2Metric: acc=0.8979357798165137, hinge=1.9109996893537153, ce=1.7061562011689497
Local test acc @ epoch 24: 0.8979
Client 2 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 258.55 seconds!
[tester] 
SST2Metric: acc=0.8979357798165137, hinge=1.9040720296015434, ce=1.700116729918027
Local test acc @ epoch 24: 0.8979
Client 9 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 259.46 seconds!
[tester] 
SST2Metric: acc=0.8979357798165137, hinge=1.9370460225901474, ce=1.7315156165583059
Local test acc @ epoch 24: 0.8979
Client 8 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 258.96 seconds!
[tester] 
SST2Metric: acc=0.8990825688073395, hinge=1.8945343380947726, ce=1.6905468927743286
Local test acc @ epoch 24: 0.8991
Client 4 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 258.97 seconds!
[tester] 
SST2Metric: acc=0.8979357798165137, hinge=1.919766225809351, ce=1.7145895152787076
Local test acc @ epoch 24: 0.8979
Client 5 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 258.94 seconds!
[tester] 
SST2Metric: acc=0.8979357798165137, hinge=1.9057254279972216, ce=1.7017887691377969
Local test acc @ epoch 24: 0.8979
Global evaluate on test data...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 259.01 seconds!
[tester] 
SST2Metric: acc=0.8979357798165137, hinge=1.904324058272423, ce=1.7002989257367638
Global test acc @ epoch 24: 0.8979
Global epoch 25...
Client 0 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 258.94 seconds!
[tester] 
SST2Metric: acc=0.8979357798165137, hinge=1.9117377326575988, ce=1.7077691415224803
Local test acc @ epoch 25: 0.8979
Client 5 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 259.29 seconds!
[tester] 
SST2Metric: acc=0.8979357798165137, hinge=1.919360335540334, ce=1.715317444439014
Local test acc @ epoch 25: 0.8979
Client 8 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 259.07 seconds!
[tester] 
SST2Metric: acc=0.8979357798165137, hinge=1.9091812377675959, ce=1.7050466547089909
Local test acc @ epoch 25: 0.8979
Client 4 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 259.05 seconds!
[tester] 
SST2Metric: acc=0.8979357798165137, hinge=1.9328488937211692, ce=1.7271832791562856
Local test acc @ epoch 25: 0.8979
Client 3 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 258.37 seconds!
[tester] 
SST2Metric: acc=0.8967889908256881, hinge=1.90542575088116, ce=1.700648990327772
Local test acc @ epoch 25: 0.8968
Client 1 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 259.06 seconds!
[tester] 
SST2Metric: acc=0.8979357798165137, hinge=1.90991711233734, ce=1.7057448276415734
Local test acc @ epoch 25: 0.8979
Client 2 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 258.47 seconds!
[tester] 
SST2Metric: acc=0.8979357798165137, hinge=1.9181684256967055, ce=1.7140644145249908
Local test acc @ epoch 25: 0.8979
Client 7 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 256.71 seconds!
[tester] 
SST2Metric: acc=0.8979357798165137, hinge=1.9045166795680282, ce=1.7002142577168555
Local test acc @ epoch 25: 0.8979
Client 9 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 258.77 seconds!
[tester] 
SST2Metric: acc=0.8979357798165137, hinge=1.9491500154547734, ce=1.7430879299941269
Local test acc @ epoch 25: 0.8979
Client 6 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 258.77 seconds!
[tester] 
SST2Metric: acc=0.8979357798165137, hinge=1.9247745097777165, ce=1.7194446447812926
Local test acc @ epoch 25: 0.8979
Global evaluate on test data...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 259.31 seconds!
[tester] 
SST2Metric: acc=0.8979357798165137, hinge=1.9181337264973088, ce=1.7139798963457635
Global test acc @ epoch 25: 0.8979
Global epoch 26...
Client 4 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 258.9 seconds!
[tester] 
SST2Metric: acc=0.8967889908256881, hinge=1.945038084453399, ce=1.738968000588446
Local test acc @ epoch 26: 0.8968
Client 5 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 259.26 seconds!
[tester] 
SST2Metric: acc=0.8979357798165137, hinge=1.9322055293879379, ce=1.7278888415748925
Local test acc @ epoch 26: 0.8979
Client 9 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 258.83 seconds!
[tester] 
SST2Metric: acc=0.8979357798165137, hinge=1.960418277787506, ce=1.7539327462876912
Local test acc @ epoch 26: 0.8979
Client 1 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 259.05 seconds!
[tester] 
SST2Metric: acc=0.8979357798165137, hinge=1.9234366952826123, ce=1.7190180880570032
Local test acc @ epoch 26: 0.8979
Client 8 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 259.51 seconds!
[tester] 
SST2Metric: acc=0.8979357798165137, hinge=1.9227430352924066, ce=1.71841442250301
Local test acc @ epoch 26: 0.8979
Client 0 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 256.29 seconds!
[tester] 
SST2Metric: acc=0.8979357798165137, hinge=1.9255351372268221, ce=1.7210134910179085
Local test acc @ epoch 26: 0.8979
Client 2 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 258.98 seconds!
[tester] 
SST2Metric: acc=0.8979357798165137, hinge=1.931415864087026, ce=1.7269472395569225
Local test acc @ epoch 26: 0.8979
Client 7 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 256.98 seconds!
[tester] 
SST2Metric: acc=0.8979357798165137, hinge=1.9180894625296288, ce=1.7136577643008901
Local test acc @ epoch 26: 0.8979
Client 6 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 258.65 seconds!
[tester] 
SST2Metric: acc=0.8967889908256881, hinge=1.9376016912657186, ce=1.7317685333625203
Local test acc @ epoch 26: 0.8968
Client 3 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 258.84 seconds!
[tester] 
SST2Metric: acc=0.8967889908256881, hinge=1.9190053980831707, ce=1.7140680852433283
Local test acc @ epoch 26: 0.8968
Global evaluate on test data...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 259.23 seconds!
[tester] 
SST2Metric: acc=0.8979357798165137, hinge=1.9312732904602627, ce=1.7266404547363374
Global test acc @ epoch 26: 0.8979
Global epoch 27...
Client 0 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 259.01 seconds!
[tester] 
SST2Metric: acc=0.8979357798165137, hinge=1.9384118334962688, ce=1.733263589622855
Local test acc @ epoch 27: 0.8979
Client 5 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 258.13 seconds!
[tester] 
SST2Metric: acc=0.8979357798165137, hinge=1.9445137497755365, ce=1.7396136264466087
Local test acc @ epoch 27: 0.8979
Client 9 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 256.33 seconds!
[tester] 
SST2Metric: acc=0.8967889908256881, hinge=1.9713495559648637, ce=1.7641749690963553
Local test acc @ epoch 27: 0.8968
Client 1 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 258.05 seconds!
[tester] 
SST2Metric: acc=0.8979357798165137, hinge=1.9361134983530832, ce=1.7313390636304518
Local test acc @ epoch 27: 0.8979
Client 3 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 255.06 seconds!
[tester] 
SST2Metric: acc=0.8967889908256881, hinge=1.9316840037840222, ce=1.726508091438876
Local test acc @ epoch 27: 0.8968
Client 2 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 257.98 seconds!
[tester] 
SST2Metric: acc=0.8979357798165137, hinge=1.943880620079303, ce=1.738925504140266
Local test acc @ epoch 27: 0.8979
Client 6 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 257.82 seconds!
[tester] 
SST2Metric: acc=0.8967889908256881, hinge=1.9495336532319358, ce=1.743292544154162
Local test acc @ epoch 27: 0.8968
Client 8 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 256.6 seconds!
[tester] 
SST2Metric: acc=0.8979357798165137, hinge=1.9354688030317289, ce=1.730797145944409
Local test acc @ epoch 27: 0.8979
Client 4 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 257.39 seconds!
[tester] 
SST2Metric: acc=0.8967889908256881, hinge=1.9564778983866402, ce=1.7500761831476568
Local test acc @ epoch 27: 0.8968
Client 7 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 259.3 seconds!
[tester] 
SST2Metric: acc=0.8979357798165137, hinge=1.9309040709919887, ce=1.7261321830968481
Local test acc @ epoch 27: 0.8979
Global evaluate on test data...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 258.82 seconds!
[tester] 
SST2Metric: acc=0.8979357798165137, hinge=1.9436809719975936, ce=1.7384458761341308
Global test acc @ epoch 27: 0.8979
Global epoch 28...
Client 4 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 257.04 seconds!
[tester] 
SST2Metric: acc=0.8967889908256881, hinge=1.9671779901062676, ce=1.7605150804459317
Local test acc @ epoch 28: 0.8968
Client 1 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 258.63 seconds!
[tester] 
SST2Metric: acc=0.8979357798165137, hinge=1.9478995378957975, ce=1.7428469081456515
Local test acc @ epoch 28: 0.8979
Client 9 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 255.68 seconds!
[tester] 
SST2Metric: acc=0.8956422018348624, hinge=1.9817163241565774, ce=1.7738909740178193
Local test acc @ epoch 28: 0.8956
Client 3 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 258.87 seconds!
[tester] 
SST2Metric: acc=0.8967889908256881, hinge=1.9437033181343604, ce=1.738159219240393
Local test acc @ epoch 28: 0.8968
Client 8 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 258.38 seconds!
[tester] 
SST2Metric: acc=0.8979357798165137, hinge=1.947370884615347, ce=1.7423367052906105
Local test acc @ epoch 28: 0.8979
Client 7 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 259.45 seconds!
[tester] 
SST2Metric: acc=0.8979357798165137, hinge=1.9428570713471929, ce=1.73775368241856
Local test acc @ epoch 28: 0.8979
Client 6 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 258.93 seconds!
[tester] 
SST2Metric: acc=0.8967889908256881, hinge=1.9606972909575209, ce=1.7541295758599227
Local test acc @ epoch 28: 0.8968
Client 2 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 259.14 seconds!
[tester] 
SST2Metric: acc=0.8979357798165137, hinge=1.9556815849805096, ce=1.7501511911619276
Local test acc @ epoch 28: 0.8979
Client 5 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 258.61 seconds!
[tester] 
SST2Metric: acc=0.8979357798165137, hinge=1.9560904489197861, ce=1.750636930556669
Local test acc @ epoch 28: 0.8979
Client 0 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 259.04 seconds!
[tester] 
SST2Metric: acc=0.8979357798165137, hinge=1.950436805366376, ce=1.7447625133722888
Local test acc @ epoch 28: 0.8979
Global evaluate on test data...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 255.64 seconds!
[tester] 
SST2Metric: acc=0.8979357798165137, hinge=1.9552516986470703, ce=1.749514226398801
Global test acc @ epoch 28: 0.8979
Global epoch 29...
Client 0 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 258.87 seconds!
[tester] 
SST2Metric: acc=0.8979357798165137, hinge=1.9616098693751414, ce=1.755504605832779
Local test acc @ epoch 29: 0.8979
Client 2 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 258.83 seconds!
[tester] 
SST2Metric: acc=0.8979357798165137, hinge=1.96669982729155, ce=1.7606975568481915
Local test acc @ epoch 29: 0.8979
Client 9 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 259.57 seconds!
[tester] 
SST2Metric: acc=0.8956422018348624, hinge=1.9915084703526367, ce=1.783144840626831
Local test acc @ epoch 29: 0.8956
Client 1 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 259.24 seconds!
[tester] 
SST2Metric: acc=0.8979357798165137, hinge=1.959165026015098, ce=1.753679999214766
Local test acc @ epoch 29: 0.8979
Client 6 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 258.41 seconds!
[tester] 
SST2Metric: acc=0.8967889908256881, hinge=1.971186029801675, ce=1.7643677244645037
Local test acc @ epoch 29: 0.8968
Client 8 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 259.1 seconds!
[tester] 
SST2Metric: acc=0.8979357798165137, hinge=1.9587266665259633, ce=1.7531831520931693
Local test acc @ epoch 29: 0.8979
Client 4 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 256.43 seconds!
[tester] 
SST2Metric: acc=0.8967889908256881, hinge=1.9773120278612188, ce=1.770452918013409
Local test acc @ epoch 29: 0.8968
Client 5 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 258.7 seconds!
[tester] 
SST2Metric: acc=0.8979357798165137, hinge=1.9669237292687827, ce=1.7610111380833773
Local test acc @ epoch 29: 0.8979
Client 7 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 256.67 seconds!
[tester] 
SST2Metric: acc=0.8979357798165137, hinge=1.9540628476427235, ce=1.7486970637478096
Local test acc @ epoch 29: 0.8979
Client 3 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 258.96 seconds!
[tester] 
SST2Metric: acc=0.8967889908256881, hinge=1.9549638432100278, ce=1.749117847530385
Local test acc @ epoch 29: 0.8968
Global evaluate on test data...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 258.48 seconds!
[tester] 
SST2Metric: acc=0.8979357798165137, hinge=1.9661045151019314, ce=1.7599488511877137
Global test acc @ epoch 29: 0.8979
Global epoch 30...
Client 4 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 258.48 seconds!
[tester] 
SST2Metric: acc=0.8967889908256881, hinge=1.9869946115607515, ce=1.7798815432004502
Local test acc @ epoch 30: 0.8968
Client 5 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 258.45 seconds!
[tester] 
SST2Metric: acc=0.8979357798165137, hinge=1.9771045159309282, ce=1.7708217627273695
Local test acc @ epoch 30: 0.8979
Client 0 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 258.12 seconds!
[tester] 
SST2Metric: acc=0.8979357798165137, hinge=1.9721345991716472, ce=1.7656799693351926
Local test acc @ epoch 30: 0.8979
Client 2 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 257.47 seconds!
[tester] 
SST2Metric: acc=0.8979357798165137, hinge=1.9770894363659237, ce=1.7707064891173219
Local test acc @ epoch 30: 0.8979
Client 3 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 259.03 seconds!
[tester] 
SST2Metric: acc=0.8967889908256881, hinge=1.9655242264270782, ce=1.7594339863631951
Local test acc @ epoch 30: 0.8968
Client 7 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 258.53 seconds!
[tester] 
SST2Metric: acc=0.8979357798165137, hinge=1.9645672694805565, ce=1.7589997651924028
Local test acc @ epoch 30: 0.8979
Client 1 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 255.57 seconds!
[tester] 
SST2Metric: acc=0.8979357798165137, hinge=1.969839192448406, ce=1.7639005939342194
Local test acc @ epoch 30: 0.8979
Client 8 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 259.22 seconds!
[tester] 
SST2Metric: acc=0.8979357798165137, hinge=1.9693703331531736, ce=1.7634127123950076
Local test acc @ epoch 30: 0.8979
Client 6 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 259.67 seconds!
[tester] 
SST2Metric: acc=0.8967889908256881, hinge=1.9810879043209444, ce=1.774089516123104
Local test acc @ epoch 30: 0.8968
Client 9 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 259.06 seconds!
[tester] 
SST2Metric: acc=0.8956422018348624, hinge=2.0007983588297433, ce=1.7920142872806073
Local test acc @ epoch 30: 0.8956
Global evaluate on test data...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 258.98 seconds!
[tester] 
SST2Metric: acc=0.8979357798165137, hinge=1.976314796083564, ce=1.7698255573323751
Global test acc @ epoch 30: 0.8979
Global epoch 31...
Client 8 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 259.33 seconds!
[tester] 
SST2Metric: acc=0.8979357798165137, hinge=1.9793846508778563, ce=1.7730957363910123
Local test acc @ epoch 31: 0.8979
Client 0 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 257.61 seconds!
[tester] 
SST2Metric: acc=0.8979357798165137, hinge=1.9820484800623097, ce=1.7753224666512761
Local test acc @ epoch 31: 0.8979
Client 6 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 259.23 seconds!
[tester] 
SST2Metric: acc=0.8967889908256881, hinge=1.9908249300280842, ce=1.7833400592179223
Local test acc @ epoch 31: 0.8968
Client 3 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 256.03 seconds!
[tester] 
SST2Metric: acc=0.8967889908256881, hinge=1.9754855239063227, ce=1.7692056784193706
Local test acc @ epoch 31: 0.8968
Client 5 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 259.23 seconds!
[tester] 
SST2Metric: acc=0.8979357798165137, hinge=1.9867424608097164, ce=1.7801760802738564
Local test acc @ epoch 31: 0.8979
Client 2 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 259.51 seconds!
[tester] 
SST2Metric: acc=0.8979357798165137, hinge=1.9868739019019888, ce=1.780201188914747
Local test acc @ epoch 31: 0.8979
Client 4 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 259.41 seconds!
[tester] 
SST2Metric: acc=0.8956422018348624, hinge=1.9966085946888006, ce=1.78892736848663
Local test acc @ epoch 31: 0.8956
Client 9 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 259.55 seconds!
[tester] 
SST2Metric: acc=0.8956422018348624, hinge=2.010012602313943, ce=1.800546482065365
Local test acc @ epoch 31: 0.8956
Client 1 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 259.41 seconds!
[tester] 
SST2Metric: acc=0.8979357798165137, hinge=1.9799107259840047, ce=1.7736023785273953
Local test acc @ epoch 31: 0.8979
Client 7 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 258.33 seconds!
[tester] 
SST2Metric: acc=0.8979357798165137, hinge=1.9745267663800388, ce=1.768776137253701
Local test acc @ epoch 31: 0.8979
Global evaluate on test data...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 259.1 seconds!
[tester] 
SST2Metric: acc=0.8979357798165137, hinge=1.9859650629922885, ce=1.7792212678530126
Global test acc @ epoch 31: 0.8979
Global epoch 32...
Client 9 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 259.28 seconds!
[tester] 
SST2Metric: acc=0.8956422018348624, hinge=2.0195580700395306, ce=1.8088079139521844
Local test acc @ epoch 32: 0.8956
Client 8 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 255.98 seconds!
[tester] 
SST2Metric: acc=0.8979357798165137, hinge=1.9889434119430156, ce=1.7823424218809514
Local test acc @ epoch 32: 0.8979
Client 1 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 259.14 seconds!
[tester] 
SST2Metric: acc=0.8979357798165137, hinge=1.989410936969136, ce=1.7828181713064306
Local test acc @ epoch 32: 0.8979
Client 4 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 259.42 seconds!
[tester] 
SST2Metric: acc=0.8956422018348624, hinge=2.0058661319247078, ce=1.7976221706296893
Local test acc @ epoch 32: 0.8956
Client 5 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 259.02 seconds!
[tester] 
SST2Metric: acc=0.8979357798165137, hinge=1.9961033798412446, ce=1.7891196153669668
Local test acc @ epoch 32: 0.8979
Client 6 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 259.09 seconds!
[tester] 
SST2Metric: acc=0.8956422018348624, hinge=2.000146253940162, ce=1.792219845556095
Local test acc @ epoch 32: 0.8956
Client 0 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 259.42 seconds!
[tester] 
SST2Metric: acc=0.8979357798165137, hinge=1.991453475908402, ce=1.7845286219191794
Local test acc @ epoch 32: 0.8979
Client 2 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 257.54 seconds!
[tester] 
SST2Metric: acc=0.8967889908256881, hinge=1.9962796521022779, ce=1.7892740232889381
Local test acc @ epoch 32: 0.8968
Client 3 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 259.45 seconds!
[tester] 
SST2Metric: acc=0.8967889908256881, hinge=1.9849397728749372, ce=1.7785246669235397
Local test acc @ epoch 32: 0.8968
Client 7 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 256.52 seconds!
[tester] 
SST2Metric: acc=0.8979357798165137, hinge=1.9841636566940797, ce=1.7780737075216786
Local test acc @ epoch 32: 0.8979
Global evaluate on test data...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 259.34 seconds!
[tester] 
SST2Metric: acc=0.8979357798165137, hinge=1.9953694753690596, ce=1.7882043948398798
Global test acc @ epoch 32: 0.8979
Global epoch 33...
Client 9 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 258.89 seconds!
[tester] 
SST2Metric: acc=0.8944954128440367, hinge=2.028674404145381, ce=1.8168802301909714
Local test acc @ epoch 33: 0.8945
Client 4 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 258.62 seconds!
[tester] 
SST2Metric: acc=0.8956422018348624, hinge=2.0156145300887047, ce=1.8060469485476964
Local test acc @ epoch 33: 0.8956
Client 6 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 258.71 seconds!
[tester] 
SST2Metric: acc=0.8956422018348624, hinge=2.009323267078181, ce=1.8007619500949512
Local test acc @ epoch 33: 0.8956
Client 7 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 258.8 seconds!
[tester] 
SST2Metric: acc=0.8979357798165137, hinge=1.9932945936644844, ce=1.7869502915235926
Local test acc @ epoch 33: 0.8979
Client 8 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 259.74 seconds!
[tester] 
SST2Metric: acc=0.8967889908256881, hinge=1.9983817779142923, ce=1.7911923894560167
Local test acc @ epoch 33: 0.8968
Client 2 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 258.79 seconds!
[tester] 
SST2Metric: acc=0.8967889908256881, hinge=2.0055161629794935, ce=1.7979651279103919
Local test acc @ epoch 33: 0.8968
Client 3 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 259.27 seconds!
[tester] 
SST2Metric: acc=0.8967889908256881, hinge=1.9938940006658572, ce=1.7874059829712339
Local test acc @ epoch 33: 0.8968
Client 1 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 258.63 seconds!
[tester] 
SST2Metric: acc=0.8967889908256881, hinge=1.9985696849746442, ce=1.79166530768622
Local test acc @ epoch 33: 0.8968
Client 5 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 259.2 seconds!
[tester] 
SST2Metric: acc=0.8967889908256881, hinge=2.0052105013383636, ce=1.7977113999201466
Local test acc @ epoch 33: 0.8968
Client 0 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 259.17 seconds!
[tester] 
SST2Metric: acc=0.8967889908256881, hinge=2.0007986684731387, ce=1.7933331379471635
Local test acc @ epoch 33: 0.8968
Global evaluate on test data...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 259.14 seconds!
[tester] 
SST2Metric: acc=0.8967889908256881, hinge=2.0044948432697067, ce=1.7968335516975977
Global test acc @ epoch 33: 0.8968
Global epoch 34...
Client 1 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 258.79 seconds!
[tester] 
SST2Metric: acc=0.8967889908256881, hinge=2.0075767529666972, ce=1.8001531719892607
Local test acc @ epoch 34: 0.8968
Client 3 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 259.02 seconds!
[tester] 
SST2Metric: acc=0.8967889908256881, hinge=2.0023981578853154, ce=1.7959143840124885
Local test acc @ epoch 34: 0.8968
Client 8 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 256.87 seconds!
[tester] 
SST2Metric: acc=0.8967889908256881, hinge=2.00755268882174, ce=1.7997069918037576
Local test acc @ epoch 34: 0.8968
Client 4 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 259.26 seconds!
[tester] 
SST2Metric: acc=0.8956422018348624, hinge=2.0250737124353373, ce=1.8142591263676877
Local test acc @ epoch 34: 0.8956
Client 9 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 258.57 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.0373908888309376, ce=1.824780493521151
Local test acc @ epoch 34: 0.8933
Client 2 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 258.94 seconds!
[tester] 
SST2Metric: acc=0.8967889908256881, hinge=2.0146434516261476, ce=1.806353348994986
Local test acc @ epoch 34: 0.8968
Client 6 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 259.1 seconds!
[tester] 
SST2Metric: acc=0.8956422018348624, hinge=2.018970443704806, ce=1.80905800169483
Local test acc @ epoch 34: 0.8956
Client 0 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 259.14 seconds!
[tester] 
SST2Metric: acc=0.8967889908256881, hinge=2.0099375745572083, ce=1.8018124565245348
Local test acc @ epoch 34: 0.8968
Client 5 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 259.39 seconds!
[tester] 
SST2Metric: acc=0.8967889908256881, hinge=2.014534249491648, ce=1.8060166738746175
Local test acc @ epoch 34: 0.8968
Client 7 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 259.19 seconds!
[tester] 
SST2Metric: acc=0.8967889908256881, hinge=2.0024257615345333, ce=1.7955033200095305
Local test acc @ epoch 34: 0.8968
Global evaluate on test data...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 259.02 seconds!
[tester] 
SST2Metric: acc=0.8967889908256881, hinge=2.0138388180404627, ce=1.8051737938969072
Global test acc @ epoch 34: 0.8968
Global epoch 35...
Client 2 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 257.02 seconds!
[tester] 
SST2Metric: acc=0.8956422018348624, hinge=2.0241011835019522, ce=1.814496710795543
Local test acc @ epoch 35: 0.8956
Client 3 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 258.86 seconds!
[tester] 
SST2Metric: acc=0.8967889908256881, hinge=2.011192978795515, ce=1.804141029025601
Local test acc @ epoch 35: 0.8968
Client 8 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 256.67 seconds!
[tester] 
SST2Metric: acc=0.8967889908256881, hinge=2.017181595120955, ce=1.8079769734227793
Local test acc @ epoch 35: 0.8968
Client 5 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 258.86 seconds!
[tester] 
SST2Metric: acc=0.8967889908256881, hinge=2.023882497477969, ce=1.8140882909799965
Local test acc @ epoch 35: 0.8968
Client 0 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 259.38 seconds!
[tester] 
SST2Metric: acc=0.8956422018348624, hinge=2.019490588969047, ce=1.810023810373915
Local test acc @ epoch 35: 0.8956
Client 6 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 258.92 seconds!
[tester] 
SST2Metric: acc=0.8956422018348624, hinge=2.028222880368933, ce=1.8171463803416044
Local test acc @ epoch 35: 0.8956
Client 4 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 258.78 seconds!
[tester] 
SST2Metric: acc=0.8944954128440367, hinge=2.0341123775878085, ce=1.8223042699170253
Local test acc @ epoch 35: 0.8945
Client 9 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 259.15 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.04572624514956, ce=1.8324941781289634
Local test acc @ epoch 35: 0.8933
Client 7 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 257.51 seconds!
[tester] 
SST2Metric: acc=0.8967889908256881, hinge=2.011664993035684, ce=1.8037510988943535
Local test acc @ epoch 35: 0.8968
Client 1 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 259.16 seconds!
[tester] 
SST2Metric: acc=0.8967889908256881, hinge=2.01710090861408, ce=1.8084094477950547
Local test acc @ epoch 35: 0.8968
Global evaluate on test data...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 259.57 seconds!
[tester] 
SST2Metric: acc=0.8967889908256881, hinge=2.023182197174895, ce=1.8132877599977555
Global test acc @ epoch 35: 0.8968
Global epoch 36...
Client 7 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 256.97 seconds!
[tester] 
SST2Metric: acc=0.8967889908256881, hinge=2.021000380773063, ce=1.8117761961851107
Local test acc @ epoch 36: 0.8968
Client 0 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 259.5 seconds!
[tester] 
SST2Metric: acc=0.8956422018348624, hinge=2.028683943349287, ce=1.818049621465193
Local test acc @ epoch 36: 0.8956
Client 2 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 258.94 seconds!
[tester] 
SST2Metric: acc=0.8956422018348624, hinge=2.033216258117912, ce=1.8224627650909515
Local test acc @ epoch 36: 0.8956
Client 9 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 259.06 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.0536969239832064, ce=1.8399892997438707
Local test acc @ epoch 36: 0.8933
Client 4 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 259.1 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.042748347881737, ce=1.8301686952356693
Local test acc @ epoch 36: 0.8933
Client 6 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 258.59 seconds!
[tester] 
SST2Metric: acc=0.8944954128440367, hinge=2.0370558613757477, ce=1.8250648686855415
Local test acc @ epoch 36: 0.8945
Client 3 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 256.93 seconds!
[tester] 
SST2Metric: acc=0.8967889908256881, hinge=2.0203952340904725, ce=1.8121497525226995
Local test acc @ epoch 36: 0.8968
Client 5 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 258.77 seconds!
[tester] 
SST2Metric: acc=0.8944954128440367, hinge=2.0328859200171374, ce=1.8220095206536362
Local test acc @ epoch 36: 0.8945
Client 8 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 255.52 seconds!
[tester] 
SST2Metric: acc=0.8967889908256881, hinge=2.026476897230936, ce=1.8160593680957962
Local test acc @ epoch 36: 0.8968
Client 1 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 258.69 seconds!
[tester] 
SST2Metric: acc=0.8967889908256881, hinge=2.026327521839273, ce=1.8164446189553611
Local test acc @ epoch 36: 0.8968
Global evaluate on test data...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 258.91 seconds!
[tester] 
SST2Metric: acc=0.8944954128440367, hinge=2.0321996356915992, ce=1.8212353962384455
Global test acc @ epoch 36: 0.8945
Global epoch 37...
Client 4 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 258.99 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.051007064657474, ce=1.8378300902418758
Local test acc @ epoch 37: 0.8933
Client 5 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 258.88 seconds!
[tester] 
SST2Metric: acc=0.8944954128440367, hinge=2.041528389267965, ce=1.8297546092315384
Local test acc @ epoch 37: 0.8945
Client 9 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 259.16 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.061349747377798, ce=1.847268120633225
Local test acc @ epoch 37: 0.8933
Client 6 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 258.97 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.045502581590906, ce=1.8328031500294393
Local test acc @ epoch 37: 0.8933
Client 0 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 258.53 seconds!
[tester] 
SST2Metric: acc=0.8944954128440367, hinge=2.0374854570681897, ce=1.8259312630416817
Local test acc @ epoch 37: 0.8945
Client 3 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 258.05 seconds!
[tester] 
SST2Metric: acc=0.8967889908256881, hinge=2.0292199243099316, ce=1.8199685470938374
Local test acc @ epoch 37: 0.8968
Client 2 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 255.68 seconds!
[tester] 
SST2Metric: acc=0.8944954128440367, hinge=2.041939667332063, ce=1.8302881698127302
Local test acc @ epoch 37: 0.8945
Client 7 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 258.59 seconds!
[tester] 
SST2Metric: acc=0.8967889908256881, hinge=2.0300495091381423, ce=1.8196645520366823
Local test acc @ epoch 37: 0.8968
Client 8 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 258.02 seconds!
[tester] 
SST2Metric: acc=0.8956422018348624, hinge=2.035333027260019, ce=1.8239623054278034
Local test acc @ epoch 37: 0.8956
Client 1 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 258.77 seconds!
[tester] 
SST2Metric: acc=0.8956422018348624, hinge=2.0352584667708897, ce=1.8243213714600328
Local test acc @ epoch 37: 0.8956
Global evaluate on test data...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 258.59 seconds!
[tester] 
SST2Metric: acc=0.8944954128440367, hinge=2.040820978786967, ce=1.8290254147347138
Global test acc @ epoch 37: 0.8945
Global epoch 38...
Client 0 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 258.74 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.0458695544015377, ce=1.8336138648416855
Local test acc @ epoch 38: 0.8933
Client 3 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 258.17 seconds!
[tester] 
SST2Metric: acc=0.8956422018348624, hinge=2.0376749522642257, ce=1.8276339409870042
Local test acc @ epoch 38: 0.8956
Client 1 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 259.19 seconds!
[tester] 
SST2Metric: acc=0.8956422018348624, hinge=2.0437882384029002, ce=1.832030167932016
Local test acc @ epoch 38: 0.8956
Client 6 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 258.5 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.053590594901951, ce=1.8403410957973896
Local test acc @ epoch 38: 0.8933
Client 2 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 258.43 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.050292855555858, ce=1.837921638412387
Local test acc @ epoch 38: 0.8933
Client 8 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 258.51 seconds!
[tester] 
SST2Metric: acc=0.8944954128440367, hinge=2.043790182252543, ce=1.8316892203687323
Local test acc @ epoch 38: 0.8945
Client 7 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 258.92 seconds!
[tester] 
SST2Metric: acc=0.8956422018348624, hinge=2.038679792793519, ce=1.827382764508628
Local test acc @ epoch 38: 0.8956
Client 9 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 259.21 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.0687005398196914, ce=1.8543178312004305
Local test acc @ epoch 38: 0.8933
Client 5 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 259.25 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.049823240129226, ce=1.837342031916544
Local test acc @ epoch 38: 0.8933
Client 4 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 259.11 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.0589424638299767, ce=1.8452952596277568
Local test acc @ epoch 38: 0.8933
Global evaluate on test data...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 257.38 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.049061130766475, ce=1.83662715567023
Global test acc @ epoch 38: 0.8933
Global epoch 39...
Client 6 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 258.98 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.0613309284415813, ce=1.8476485023612252
Local test acc @ epoch 39: 0.8933
Client 7 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 256.25 seconds!
[tester] 
SST2Metric: acc=0.8944954128440367, hinge=2.0469362176612975, ce=1.834934565397642
Local test acc @ epoch 39: 0.8945
Client 2 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 259.14 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.058318386110691, ce=1.8453637957773714
Local test acc @ epoch 39: 0.8933
Client 4 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 259.23 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.066533814056204, ce=1.8525090203149686
Local test acc @ epoch 39: 0.8933
Client 0 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 258.86 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.05393878172297, ce=1.8411185185739214
Local test acc @ epoch 39: 0.8933
Client 5 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 259.31 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.0577464073622993, ce=1.8447115041539408
Local test acc @ epoch 39: 0.8933
Client 9 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 259.32 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.0757575874481726, ce=1.8611304199063026
Local test acc @ epoch 39: 0.8933
Client 1 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 256.9 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.0519666372362626, ce=1.8395756362530722
Local test acc @ epoch 39: 0.8933
Client 3 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 259.18 seconds!
[tester] 
SST2Metric: acc=0.8956422018348624, hinge=2.046260492243898, ce=1.835148727956308
Local test acc @ epoch 39: 0.8956
Client 8 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 259.44 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.051901878840333, ce=1.8392381047414084
Local test acc @ epoch 39: 0.8933
Global evaluate on test data...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 256.01 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.0569637618753887, ce=1.8440340152415813
Global test acc @ epoch 39: 0.8933
Global epoch 40...
Client 0 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 259.02 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.061687011510954, ce=1.8483925641054535
Local test acc @ epoch 40: 0.8933
Client 6 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 259.33 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.0687653284280674, ce=1.854734640041308
Local test acc @ epoch 40: 0.8933
Client 5 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 259.19 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.065338691987029, ce=1.8518609613760013
Local test acc @ epoch 40: 0.8933
Client 3 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 258.82 seconds!
[tester] 
SST2Metric: acc=0.8956422018348624, hinge=2.054481822279615, ce=1.8424657313842598
Local test acc @ epoch 40: 0.8956
Client 7 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 259.26 seconds!
[tester] 
SST2Metric: acc=0.8944954128440367, hinge=2.0548525406133145, ce=1.8423046222138846
Local test acc @ epoch 40: 0.8945
Client 9 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 257.75 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.0825491697963225, ce=1.8677183739955983
Local test acc @ epoch 40: 0.8933
Client 2 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 259.18 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.066020201925838, ce=1.852605585959619
Local test acc @ epoch 40: 0.8933
Client 1 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 255.75 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.059791644231989, ce=1.8469058851782707
Local test acc @ epoch 40: 0.8933
Client 8 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 259.06 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.059662744813009, ce=1.846563001025648
Local test acc @ epoch 40: 0.8933
Client 4 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 259.5 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.073833743640042, ce=1.8595023631995728
Local test acc @ epoch 40: 0.8933
Global evaluate on test data...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 259.39 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.064579643924302, ce=1.8512108960799352
Global test acc @ epoch 40: 0.8933
Global epoch 41...
Client 8 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 258.97 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.067089334266995, ce=1.8536473897701438
Local test acc @ epoch 41: 0.8933
Client 6 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 259.14 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.075908544960372, ce=1.8615934287411566
Local test acc @ epoch 41: 0.8933
Client 5 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 258.47 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.072633734536827, ce=1.858797910600319
Local test acc @ epoch 41: 0.8933
Client 3 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 259.22 seconds!
[tester] 
SST2Metric: acc=0.8944954128440367, hinge=2.0623752798782577, ce=1.8495867748348727
Local test acc @ epoch 41: 0.8945
Client 0 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 259.06 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.069125098372818, ce=1.8554527242918888
Local test acc @ epoch 41: 0.8933
Client 1 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 257.14 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.0672893198805116, ce=1.8540101424981874
Local test acc @ epoch 41: 0.8933
Client 7 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 259.0 seconds!
[tester] 
SST2Metric: acc=0.8944954128440367, hinge=2.0626774143188373, ce=1.8494631403081812
Local test acc @ epoch 41: 0.8945
Client 4 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 259.37 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.0808390409027764, ce=1.8662538641848325
Local test acc @ epoch 41: 0.8933
Client 2 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 259.2 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.073385321355741, ce=1.859603031939857
Local test acc @ epoch 41: 0.8933
Client 9 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 258.63 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.0890838446420266, ce=1.8740863165821797
Local test acc @ epoch 41: 0.8922
Global evaluate on test data...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 259.68 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.071895500388714, ce=1.858170254211615
Global test acc @ epoch 41: 0.8933
Global epoch 42...
Client 1 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 257.15 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.074486076284986, ce=1.8608920084883154
Local test acc @ epoch 42: 0.8933
Client 7 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 258.61 seconds!
[tester] 
SST2Metric: acc=0.8944954128440367, hinge=2.0702024482258965, ce=1.856402464390975
Local test acc @ epoch 42: 0.8945
Client 6 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 258.97 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.08277037657729, ce=1.868222374899981
Local test acc @ epoch 42: 0.8933
Client 8 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 259.13 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.074306296372632, ce=1.8605251771415559
Local test acc @ epoch 42: 0.8933
Client 0 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 259.1 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.0762602388858795, ce=1.862281811328885
Local test acc @ epoch 42: 0.8933
Client 5 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 259.38 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.0796294921855316, ce=1.8655049689791223
Local test acc @ epoch 42: 0.8933
Client 2 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 258.76 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.080466764782547, ce=1.8663849797444716
Local test acc @ epoch 42: 0.8933
Client 9 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 259.15 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.095380983494837, ce=1.8802471818486066
Local test acc @ epoch 42: 0.8922
Client 3 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 257.9 seconds!
[tester] 
SST2Metric: acc=0.8944954128440367, hinge=2.069971287195836, ce=1.8565077610431013
Local test acc @ epoch 42: 0.8945
Client 4 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 259.07 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.087586403850022, ce=1.8727937336340676
Local test acc @ epoch 42: 0.8922
Global evaluate on test data...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 258.88 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.0789186801385444, ce=1.8649038862817937
Global test acc @ epoch 42: 0.8933
Global epoch 43...
Client 5 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 258.39 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.086401385041552, ce=1.8720023071635479
Local test acc @ epoch 43: 0.8933
Client 2 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 259.1 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.0872663316923545, ce=1.8729400818450206
Local test acc @ epoch 43: 0.8933
Client 8 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 259.11 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.0814256017361212, ce=1.867174428822203
Local test acc @ epoch 43: 0.8933
Client 1 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 258.81 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.0816626253478026, ce=1.8675683517912802
Local test acc @ epoch 43: 0.8933
Client 6 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 259.24 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.089375260773055, ce=1.8746377465321202
Local test acc @ epoch 43: 0.8922
Client 4 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 259.09 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.094077609547781, ce=1.8791162322074155
Local test acc @ epoch 43: 0.8922
Client 3 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 257.35 seconds!
[tester] 
SST2Metric: acc=0.8944954128440367, hinge=2.0772651689588475, ce=1.86321162552926
Local test acc @ epoch 43: 0.8945
Client 0 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 259.02 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.0832006652420816, ce=1.8688974752381318
Local test acc @ epoch 43: 0.8933
Client 9 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 259.07 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.1014528051702253, ce=1.8862104390626222
Local test acc @ epoch 43: 0.8922
Client 7 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 255.5 seconds!
[tester] 
SST2Metric: acc=0.8944954128440367, hinge=2.0774568927670836, ce=1.8631436241317028
Local test acc @ epoch 43: 0.8945
Global evaluate on test data...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 259.17 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.085806594802699, ce=1.8714248196187053
Global test acc @ epoch 43: 0.8933
Global epoch 44...
Client 2 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 259.1 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.094065807007868, ce=1.879291124865388
Local test acc @ epoch 44: 0.8922
Client 3 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 258.99 seconds!
[tester] 
SST2Metric: acc=0.8944954128440367, hinge=2.0842839154081605, ce=1.8697105940073568
Local test acc @ epoch 44: 0.8945
Client 8 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 259.2 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.0883135780555393, ce=1.8736185298805874
Local test acc @ epoch 44: 0.8922
Client 9 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 259.03 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.1073132374691306, ce=1.8919868198583845
Local test acc @ epoch 44: 0.8922
Client 7 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 258.5 seconds!
[tester] 
SST2Metric: acc=0.8944954128440367, hinge=2.0844354842780928, ce=1.8696650457982602
Local test acc @ epoch 44: 0.8945
Client 6 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 259.06 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.0957338710170275, ce=1.8808452851134672
Local test acc @ epoch 44: 0.8922
Client 4 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 256.79 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.100337970694271, ce=1.8852405695000056
Local test acc @ epoch 44: 0.8922
Client 1 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 258.9 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.0885866624773097, ce=1.8740336085077478
Local test acc @ epoch 44: 0.8933
Client 5 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 258.76 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.0931501238171113, ce=1.8782996282323703
Local test acc @ epoch 44: 0.8922
Client 0 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 258.91 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.090054243256193, ce=1.8752993003922858
Local test acc @ epoch 44: 0.8922
Global evaluate on test data...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 258.67 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.092562607668955, ce=1.87773566527755
Global test acc @ epoch 44: 0.8922
Global epoch 45...
Client 5 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 258.94 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.0996477300420815, ce=1.8843883030597288
Local test acc @ epoch 45: 0.8922
Client 6 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 257.0 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.1020371879732935, ce=1.8868557771428627
Local test acc @ epoch 45: 0.8922
Client 2 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 259.1 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.100620837523303, ce=1.8854406235137193
Local test acc @ epoch 45: 0.8922
Client 9 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 258.51 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.1131181969828563, ce=1.8975859709821565
Local test acc @ epoch 45: 0.8922
Client 8 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 256.17 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.0949710386061886, ce=1.8798628896622316
Local test acc @ epoch 45: 0.8922
Client 7 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 258.69 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.091171389462751, ce=1.875996350731967
Local test acc @ epoch 45: 0.8933
Client 3 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 258.66 seconds!
[tester] 
SST2Metric: acc=0.8944954128440367, hinge=2.091061623681576, ce=1.8760310531884177
Local test acc @ epoch 45: 0.8945
Client 1 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 258.84 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.095286313273491, ce=1.8802937809169191
Local test acc @ epoch 45: 0.8922
Client 4 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 259.19 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.1064226539856796, ce=1.891171820536179
Local test acc @ epoch 45: 0.8922
Client 0 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 259.1 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.096657062889239, ce=1.8814922535252887
Local test acc @ epoch 45: 0.8922
Global evaluate on test data...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 259.14 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.099078980744432, ce=1.8838504261778564
Global test acc @ epoch 45: 0.8922
Global epoch 46...
Client 5 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 259.38 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.1059266927592253, ce=1.8902952784070717
Local test acc @ epoch 46: 0.8922
Client 8 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 259.54 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.1013747911661045, ce=1.8858979553893676
Local test acc @ epoch 46: 0.8922
Client 2 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 257.28 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.106944628264926, ce=1.8913970394741182
Local test acc @ epoch 46: 0.8922
Client 9 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 259.1 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.11886666332363, ce=1.9030203815243292
Local test acc @ epoch 46: 0.8922
Client 1 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 259.44 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.101738676292087, ce=1.886355154018738
Local test acc @ epoch 46: 0.8922
Client 7 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 259.06 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.097667033382512, ce=1.882132251809736
Local test acc @ epoch 46: 0.8933
Client 0 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 259.09 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.103024992510813, ce=1.8874919508569064
Local test acc @ epoch 46: 0.8922
Client 6 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 258.45 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.108222277886277, ce=1.8926877247399991
Local test acc @ epoch 46: 0.8922
Client 4 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 259.22 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.1125151643512448, ce=1.896919459489553
Local test acc @ epoch 46: 0.8922
Client 3 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 259.35 seconds!
[tester] 
SST2Metric: acc=0.8944954128440367, hinge=2.0975924507467023, ce=1.882159041480679
Local test acc @ epoch 46: 0.8945
Global evaluate on test data...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 258.04 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.1053722820697574, ce=1.8897765425876925
Global test acc @ epoch 46: 0.8922
Global epoch 47...
Client 6 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 259.33 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.1141907915883107, ce=1.898331736794398
Local test acc @ epoch 47: 0.8922
Client 7 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 259.13 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.103928349433689, ce=1.8880753158023995
Local test acc @ epoch 47: 0.8933
Client 8 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 259.09 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.1075653294357686, ce=1.8917564525379107
Local test acc @ epoch 47: 0.8922
Client 4 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 258.99 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.1184019155185156, ce=1.9024893546460253
Local test acc @ epoch 47: 0.8922
Client 3 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 259.29 seconds!
[tester] 
SST2Metric: acc=0.893348623853211, hinge=2.10388670082486, ce=1.8881021877400082
Local test acc @ epoch 47: 0.8933
Client 5 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 258.24 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.1119907903015065, ce=1.8960267919985776
Local test acc @ epoch 47: 0.8922
Client 9 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 258.87 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.124431563079904, ce=1.9082951173130456
Local test acc @ epoch 47: 0.8922
Client 1 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 259.36 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.1079760606682627, ce=1.892237915192692
Local test acc @ epoch 47: 0.8922
Client 2 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 255.67 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.113051587288533, ce=1.8971720691884935
Local test acc @ epoch 47: 0.8922
Client 0 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 259.13 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.1091765993778857, ce=1.8933095842219734
Local test acc @ epoch 47: 0.8922
Global evaluate on test data...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 259.18 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.111447754666346, ce=1.8955258732662563
Global test acc @ epoch 47: 0.8922
Global epoch 48...
Client 7 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 258.7 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.1099832796175546, ce=1.8938494607218808
Local test acc @ epoch 48: 0.8922
Client 3 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 259.14 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.1099650843427815, ce=1.8938751314463955
Local test acc @ epoch 48: 0.8922
Client 6 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 258.71 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.1199627634855585, ce=1.903808004049212
Local test acc @ epoch 48: 0.8922
Client 2 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 257.13 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.118942929243823, ce=1.9027640188916257
Local test acc @ epoch 48: 0.8922
Client 4 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 259.26 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.124097706117761, ce=1.9078947986903938
Local test acc @ epoch 48: 0.8922
Client 5 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 258.99 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.117847798614327, ce=1.9015847053674038
Local test acc @ epoch 48: 0.8922
Client 1 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 259.17 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.113996590769619, ce=1.8979440925650768
Local test acc @ epoch 48: 0.8922
Client 8 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 259.23 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.113548057478502, ce=1.8974448936844381
Local test acc @ epoch 48: 0.8922
Client 0 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 258.87 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.1151195518467403, ce=1.8989536496442163
Local test acc @ epoch 48: 0.8922
Client 9 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 259.05 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.129821348217649, ce=1.9134205485748372
Local test acc @ epoch 48: 0.8922
Global evaluate on test data...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 258.9 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.117320063737554, ce=1.9011028610866443
Global test acc @ epoch 48: 0.8922
Global epoch 49...
Client 4 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 256.91 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.1296089061903296, ce=1.9131436182011898
Local test acc @ epoch 49: 0.8922
Client 0 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 259.54 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.120857671176622, ce=1.9044255417011515
Local test acc @ epoch 49: 0.8922
Client 5 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 259.36 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.1235109162986827, ce=1.9069817593962115
Local test acc @ epoch 49: 0.8922
Client 9 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 256.15 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.1350398097836645, ce=1.91840068424768
Local test acc @ epoch 49: 0.8922
Client 6 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 258.94 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.125558067078984, ce=1.9091371715590018
Local test acc @ epoch 49: 0.8922
Client 7 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 259.53 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.11584996657634, ce=1.8994704991641165
Local test acc @ epoch 49: 0.8922
Client 1 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 259.25 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.119816753022168, ce=1.9034842566358376
Local test acc @ epoch 49: 0.8922
Client 8 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 259.09 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.119326622250977, ce=1.9029631716665918
Local test acc @ epoch 49: 0.8922
Client 3 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 258.86 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.1158507709656287, ce=1.8994965477847447
Local test acc @ epoch 49: 0.8922
Client 2 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 257.79 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.1246478371663926, ce=1.9082033674979975
Local test acc @ epoch 49: 0.8922
Global evaluate on test data...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 259.3 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.1229993926549175, ce=1.9065209034453783
Global test acc @ epoch 49: 0.8922
Global epoch 50...
Client 8 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 258.85 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.1249151768487526, ce=1.9083241542714153
Local test acc @ epoch 50: 0.8922
Client 2 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 256.73 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.130158835868223, ce=1.9134769338106319
Local test acc @ epoch 50: 0.8922
Client 0 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 259.28 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.126414135508581, ce=1.9097464010777057
Local test acc @ epoch 50: 0.8922
Client 7 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 259.0 seconds!
[tester] 
SST2Metric: acc=0.8910550458715596, hinge=2.1215096200824877, ce=1.9049183969802435
Local test acc @ epoch 50: 0.8911
Client 3 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 259.24 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.1215308758370375, ce=1.9049510552999327
Local test acc @ epoch 50: 0.8922
Client 1 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 259.1 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.125441524140332, ce=1.908863993851399
Local test acc @ epoch 50: 0.8922
Client 5 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 256.64 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.1290012521481296, ce=1.9122379754862076
Local test acc @ epoch 50: 0.8922
Client 9 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 259.16 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.1401142185434288, ce=1.9232615459467617
Local test acc @ epoch 50: 0.8922
Client 4 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 259.2 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.1349644730670736, ce=1.9182634419670554
Local test acc @ epoch 50: 0.8922
Client 6 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 258.88 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.1309714888760802, ce=1.914311865166787
Local test acc @ epoch 50: 0.8922
Global evaluate on test data...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 259.27 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.1284966616455567, ce=1.9117878629989198
Global test acc @ epoch 50: 0.8922
Global epoch 51...
Client 5 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 259.27 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.1343099462603212, ce=1.9173436426295933
Local test acc @ epoch 51: 0.8922
Client 2 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 259.34 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.1355082061859445, ce=1.9186179152521874
Local test acc @ epoch 51: 0.8922
Client 3 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 258.7 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.1270352626612428, ce=1.9102613928040966
Local test acc @ epoch 51: 0.8922
Client 1 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 259.09 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.130883222599642, ce=1.9140927829946817
Local test acc @ epoch 51: 0.8922
Client 7 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 256.59 seconds!
[tester] 
SST2Metric: acc=0.8910550458715596, hinge=2.1269971826754577, ce=1.9102224709715527
Local test acc @ epoch 51: 0.8911
Client 9 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 259.05 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.1450250550694423, ce=1.9279869377733327
Local test acc @ epoch 51: 0.8922
Client 6 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 258.99 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.1362159213888536, ce=1.9193446833860766
Local test acc @ epoch 51: 0.8922
Client 8 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 259.07 seconds!
[tester] 
SST2Metric: acc=0.8910550458715596, hinge=2.130324300549446, ce=1.9135379632477574
Local test acc @ epoch 51: 0.8911
Client 4 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 258.86 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.1401373757681714, ce=1.9232280395309427
Local test acc @ epoch 51: 0.8922
Client 0 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 259.17 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.1317993571998874, ce=1.9149261680378251
Local test acc @ epoch 51: 0.8922
Global evaluate on test data...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 258.9 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.133816122735312, ce=1.9169086278690306
Global test acc @ epoch 51: 0.8922
Global epoch 52...
Client 1 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 258.94 seconds!
[tester] 
SST2Metric: acc=0.8910550458715596, hinge=2.1361543041303617, ce=1.9191810860518634
Local test acc @ epoch 52: 0.8911
Client 4 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 256.36 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.1451580365863414, ce=1.9280656396218343
Local test acc @ epoch 52: 0.8922
Client 5 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 258.67 seconds!
[tester] 
SST2Metric: acc=0.8910550458715596, hinge=2.1394565996773744, ce=1.922316667312217
Local test acc @ epoch 52: 0.8911
Client 3 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 258.91 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.132364029184394, ce=1.915430359967448
Local test acc @ epoch 52: 0.8922
Client 9 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 258.53 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.1498004293769872, ce=1.9326032431341578
Local test acc @ epoch 52: 0.8922
Client 7 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 256.65 seconds!
[tester] 
SST2Metric: acc=0.8910550458715596, hinge=2.132310761771071, ce=1.915382042833709
Local test acc @ epoch 52: 0.8911
Client 2 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 258.55 seconds!
[tester] 
SST2Metric: acc=0.8910550458715596, hinge=2.1406850177760517, ce=1.9236172076976628
Local test acc @ epoch 52: 0.8911
Client 8 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 258.63 seconds!
[tester] 
SST2Metric: acc=0.8910550458715596, hinge=2.135566051400036, ce=1.9186133334436641
Local test acc @ epoch 52: 0.8911
Client 6 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 258.48 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.141301371362231, ce=1.924246234261349
Local test acc @ epoch 52: 0.8922
Client 0 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 258.83 seconds!
[tester] 
SST2Metric: acc=0.8910550458715596, hinge=2.1370106898589967, ce=1.9199619150787617
Local test acc @ epoch 52: 0.8911
Global evaluate on test data...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 256.89 seconds!
[tester] 
SST2Metric: acc=0.8910550458715596, hinge=2.13897544542037, ce=1.9218962625677465
Global test acc @ epoch 52: 0.8911
Global epoch 53...
Client 2 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 259.19 seconds!
[tester] 
SST2Metric: acc=0.8910550458715596, hinge=2.1456976725694235, ce=1.9284810181829204
Local test acc @ epoch 53: 0.8911
Client 5 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 259.14 seconds!
[tester] 
SST2Metric: acc=0.8910550458715596, hinge=2.1444453005123574, ce=1.9271607929144317
Local test acc @ epoch 53: 0.8911
Client 3 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 256.28 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.1375239647309714, ce=1.9204570844976216
Local test acc @ epoch 53: 0.8922
Client 7 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 258.83 seconds!
[tester] 
SST2Metric: acc=0.8910550458715596, hinge=2.137457589490698, ce=1.9203997395305306
Local test acc @ epoch 53: 0.8911
Client 6 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 258.7 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.146231821644197, ce=1.929019017215156
Local test acc @ epoch 53: 0.8922
Client 1 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 259.23 seconds!
[tester] 
SST2Metric: acc=0.8910550458715596, hinge=2.1412661059187093, ce=1.9241384672262867
Local test acc @ epoch 53: 0.8911
Client 8 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 258.37 seconds!
[tester] 
SST2Metric: acc=0.8910550458715596, hinge=2.140642009197025, ce=1.9235508378305395
Local test acc @ epoch 53: 0.8911
Client 0 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 256.56 seconds!
[tester] 
SST2Metric: acc=0.8910550458715596, hinge=2.1420573173859796, ce=1.9248587444895453
Local test acc @ epoch 53: 0.8911
Client 9 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 258.75 seconds!
[tester] 
SST2Metric: acc=0.8910550458715596, hinge=2.1544568740993464, ce=1.9371100070039413
Local test acc @ epoch 53: 0.8911
Client 4 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 258.51 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.1500421278520463, ce=1.932794165795802
Local test acc @ epoch 53: 0.8922
Global evaluate on test data...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 258.54 seconds!
[tester] 
SST2Metric: acc=0.8910550458715596, hinge=2.1439709754985405, ce=1.9267507402630735
Global test acc @ epoch 53: 0.8911
Global epoch 54...
Client 9 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 254.92 seconds!
[tester] 
SST2Metric: acc=0.8910550458715596, hinge=2.1592189292295263, ce=1.9415087688423347
Local test acc @ epoch 54: 0.8911
Client 1 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 258.78 seconds!
[tester] 
SST2Metric: acc=0.8910550458715596, hinge=2.146219466394241, ce=1.9289647837650223
Local test acc @ epoch 54: 0.8911
Client 6 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 258.76 seconds!
[tester] 
SST2Metric: acc=0.8910550458715596, hinge=2.1510178847323864, ce=1.933675747638924
Local test acc @ epoch 54: 0.8911
Client 4 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 258.24 seconds!
[tester] 
SST2Metric: acc=0.8910550458715596, hinge=2.154850944603255, ce=1.9373932284811748
Local test acc @ epoch 54: 0.8911
Client 7 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 258.55 seconds!
[tester] 
SST2Metric: acc=0.8910550458715596, hinge=2.1424527032933103, ce=1.925290711864305
Local test acc @ epoch 54: 0.8911
Client 3 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluate data in 257.81 seconds!
[tester] 
SST2Metric: acc=0.8922018348623854, hinge=2.1425241303006444, ce=1.9253502903039867
Local test acc @ epoch 54: 0.8922
Client 0 execute local training on 8 samples...
Local loss @ local epoch 0: 0.0
Local loss @ local epoch 1: 0.0
Local loss @ local epoch 2: 0.0
Local loss @ local epoch 3: 0.0
Local loss @ local epoch 4: 0.0
/root/anaconda3/envs/bbt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
