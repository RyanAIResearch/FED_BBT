/root/anaconda3/envs/bbt/lib/python3.8/site-packages/cma/s.py:13: UserWarning: Could not import matplotlib.pyplot, therefore ``cma.plot()`` etc. is not available
  _warnings.warn('Could not import matplotlib.pyplot, therefore'
Some weights of RobertaForMaskedLM were not initialized from the model checkpoint at roberta-large and are newly initialized: ['lm_head.decoder.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Read cache from caches/data_roberta-large_agnews_50_42.pt.
# of train data: 2000
Example:
+------------------------+------------------------+----------+--------+
| input_ids              | attention_mask         | mask_pos | labels |
+------------------------+------------------------+----------+--------+
| [0, 1000, 1001, 100... | [1, 1, 1, 1, 1, 1, ... | 52       | 18562  |
+------------------------+------------------------+----------+--------+

# of dev data: 2000
Example:
+------------------------+------------------------+----------+--------+
| input_ids              | attention_mask         | mask_pos | labels |
+------------------------+------------------------+----------+--------+
| [0, 1000, 1001, 100... | [1, 1, 1, 1, 1, 1, ... | 52       | 10554  |
+------------------------+------------------------+----------+--------+

# of test data: 7600
Example:
+------------------------+------------------------+----------+--------+
| input_ids              | attention_mask         | mask_pos | labels |
+------------------------+------------------------+----------+--------+
| [0, 1000, 1001, 100... | [1, 1, 1, 1, 1, 1, ... | 52       | 18562  |
+------------------------+------------------------+----------+--------+
[Embedding] mu: -0.018104109913110733 | std: 0.13582643866539001 [RandProj]  mu: 0.0 | std: 0.006074342999950358
Population Size: 10
Serial Evaluation.
********* Evaluated on dev set *********
Dev loss: 0.6906. Dev perf: 0.747. Best dev perf: 0.747
********* Done *********
********* Evaluated on dev set *********
Dev loss: 0.7023. Dev perf: 0.7415. Best dev perf: 0.747
********* Done *********
********* Evaluated on dev set *********
Dev loss: 0.6814. Dev perf: 0.744. Best dev perf: 0.747
********* Done *********
********* Evaluated on dev set *********
Dev loss: 0.683. Dev perf: 0.7445. Best dev perf: 0.747
********* Done *********
[# API Calls 50] loss: 0.672. Current perf: 0.756. Best perf so far: 0.761
********* Evaluated on dev set *********
Dev loss: 0.6705. Dev perf: 0.7505. Best dev perf: 0.7505
********* Done *********
********* Evaluated on dev set *********
Dev loss: 0.6697. Dev perf: 0.753. Best dev perf: 0.753
********* Done *********
********* Evaluated on dev set *********
Dev loss: 0.6658. Dev perf: 0.7535. Best dev perf: 0.7535
********* Done *********
********* Evaluated on dev set *********
Dev loss: 0.6599. Dev perf: 0.7535. Best dev perf: 0.7535
********* Done *********
********* Evaluated on dev set *********
Dev loss: 0.6564. Dev perf: 0.755. Best dev perf: 0.755
********* Done *********
Evaluate on test data...
Evaluate data in 90.04 seconds!
[tester] 
AGNewsMetric: acc=0.7755263157894737, hinge=1.7747621189920526, ce=6.613426941319516
Test acc @ 9 test: 0.7755
Check sigma @ 9 sigma: 0.9390731235293382
Traceback (most recent call last):
  File "bbt.py", line 739, in <module>
    fitnesses = [model_forward_api.eval(x) for x in solutions]
  File "bbt.py", line 739, in <listcomp>
    fitnesses = [model_forward_api.eval(x) for x in solutions]
  File "bbt.py", line 461, in eval
    logits = self.model(
  File "/root/anaconda3/envs/bbt/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/workspace/Black-Box-Tuning/models/modeling_roberta.py", line 960, in forward
    outputs = self.roberta(
  File "/root/anaconda3/envs/bbt/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/workspace/Black-Box-Tuning/models/modeling_roberta.py", line 725, in forward
    encoder_outputs = self.encoder(
  File "/root/anaconda3/envs/bbt/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/workspace/Black-Box-Tuning/models/modeling_roberta.py", line 444, in forward
    layer_outputs = layer_module(
  File "/root/anaconda3/envs/bbt/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/workspace/Black-Box-Tuning/models/modeling_roberta.py", line 368, in forward
    self_attention_outputs = self.attention(
  File "/root/anaconda3/envs/bbt/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/workspace/Black-Box-Tuning/models/modeling_roberta.py", line 300, in forward
    self_outputs = self.self(
  File "/root/anaconda3/envs/bbt/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/workspace/Black-Box-Tuning/models/modeling_roberta.py", line 234, in forward
    attention_probs = self.dropout(attention_probs)
  File "/root/anaconda3/envs/bbt/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/anaconda3/envs/bbt/lib/python3.8/site-packages/torch/nn/modules/dropout.py", line 59, in forward
    return F.dropout(input, self.p, self.training, self.inplace)
  File "/root/anaconda3/envs/bbt/lib/python3.8/site-packages/torch/nn/functional.py", line 1252, in dropout
    return _VF.dropout_(input, p, training) if inplace else _VF.dropout(input, p, training)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 17.03 GiB (GPU 0; 79.19 GiB total capacity; 59.88 GiB already allocated; 550.81 MiB free; 77.47 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
